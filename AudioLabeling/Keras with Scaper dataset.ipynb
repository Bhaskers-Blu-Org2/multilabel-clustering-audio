{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dropout, Activation, Flatten, Reshape\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Convolution2D as Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_folder = \"C:\\\\Users\\\\t-anmend\\\\Documents\\\\train_1\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_folder = \"/home/t-anmend/train1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(features_folder,'train1_features.txt'),'rb') as f:\n",
    "    features  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(features_folder,'train1_stats_features.txt'),'rb') as f:\n",
    "    features_stats  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[153,   8, 164, ...,   0,   5, 255],\n",
       "        [154,  10, 163, ...,   0,  39, 255],\n",
       "        [153,   7, 164, ...,   0,   9, 255],\n",
       "        ...,\n",
       "        [165,  43, 168, ..., 155, 177, 255],\n",
       "        [162,  39, 188, ..., 107, 200, 255],\n",
       "        [164,  38, 176, ..., 209, 153, 255]], dtype=uint8),\n",
       " ['drilling', 'dog_bark', 'air_conditioner', 'children_playing', 'jackhammer'],\n",
       " 'C:\\\\Users\\\\t-anmend\\\\Documents\\\\train_1\\\\uniform\\\\soundscape_train_1_uniform0.wav']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features 19x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features = np.arange(len(features))\n",
    "    \n",
    "train_ind = np.random.choice(index_features, int(0.7 * len(index_features)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for index in range(len(features)):\n",
    "    if index in train_ind:\n",
    "        X_train.append(features[index][0])\n",
    "        y_train.append(features[index][1])\n",
    "    else:\n",
    "        X_test.append(features[index][0])\n",
    "        y_test.append(features[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 19, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 19, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = (2**(math.ceil(math.log(X_train.shape[1], 2))) - X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros_up = int(np.ceil(num_zeros / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros_down = num_zeros - num_zeros_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = np.zeros((X_train.shape[0], X_train.shape[1] + num_zeros, X_train.shape[2]))\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_padded[i] = np.vstack((np.zeros((num_zeros_up, X_train.shape[2])), X_train[i], np.zeros((num_zeros_down, X_train.shape[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_padded = np.zeros((X_test.shape[0], X_test.shape[1] + num_zeros, X_test.shape[2]))\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_padded[i] = np.vstack((np.zeros((num_zeros_up, X_test.shape[2])), X_test[i], np.zeros((num_zeros_down, X_train.shape[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 32, 128)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 32, 128)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa1f25d4f98>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcc6dde6240>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features = np.arange(len(features))\n",
    "    \n",
    "train_ind = np.random.choice(index_features, int(0.7 * len(index_features)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for index in range(len(features)):\n",
    "    if index in train_ind:\n",
    "        X_train.append(features[index][0])\n",
    "        y_train.append(features[index][1])\n",
    "    else:\n",
    "        X_test.append(features[index][0])\n",
    "        y_test.append(features[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 19, 128)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 19, 128)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdbf4192080>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 15:14:11.888473 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0722 15:14:11.917692 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0722 15:14:11.928736 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 1000  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "#input_img = Input(shape=(np.prod(X_train.shape[1:]),))\n",
    "input_img = Input(shape=(19,128,1))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "x = (Flatten(name='flatten'))(input_img)\n",
    "x = Dense(np.prod(X_train.shape[1:]), activation='relu')(x)\n",
    "x = Dense(encoding_dim, activation='relu')(x)\n",
    "#x = Dense(512, activation='relu')(x)\n",
    "\n",
    "#encoded = Dense(256, activation='relu')(x)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "encoded = Dense(512, activation='relu')(x)\n",
    "x = Dense(1000, activation='relu')(encoded)\n",
    "\n",
    "x = Dense(np.prod(X_train.shape[1:]), activation='relu')(x)\n",
    "decoded = (Reshape((19,128,1)))(x)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(256,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 15:14:33.905975 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 2432)\n",
      "(900, 2432)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0],np.prod(X_train.shape[1:])))\n",
    "X_test = X_test.reshape((X_test.shape[0], np.prod(X_test.shape[1:])))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.min(axis=0).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 2432)\n",
      "(900, 2432)\n"
     ]
    }
   ],
   "source": [
    "X_train = (X_train) / 255\n",
    "X_test = (X_test) / 255\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train.mean(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0) + 1e-6)\n",
    "X_test = (X_test - X_test.mean(axis=0)) / (X_test.max(axis=0) - X_test.min(axis=0) + 1e-6)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 19, 128, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 19, 128, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\t-anmend\\\\Documents\\\\train_1\\\\models\\\\best_model_CNN_20190718_133049.ckpt'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.today()\n",
    "\n",
    "t = today.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_folder = f\"C:\\\\Users\\\\t-anmend\\\\Documents\\\\train_1\\\\models\\\\best_model_CNN_{t}.ckpt\"\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.today()\n",
    "\n",
    "t = today.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_folder = f\"/home/t-anmend/train1/models/best_model_CNN_{t}.ckpt\"\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(model_folder, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=f\"/home/t-anmend/train1/tensor_board/{t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 15:16:25.439339 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0722 15:16:25.570036 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 15:16:28.589735 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0722 15:16:28.594825 140518029633280 deprecation_wrapper.py:119] From /home/t-anmend/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "2100/2100 [==============================] - 2s 855us/step - loss: 72519.0307 - val_loss: 8567.1358\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8567.13580, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 2/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 7772.7871 - val_loss: 6515.7006\n",
      "\n",
      "Epoch 00002: val_loss improved from 8567.13580 to 6515.70056, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 3/3000\n",
      "2100/2100 [==============================] - 1s 427us/step - loss: 6182.2602 - val_loss: 5880.3561\n",
      "\n",
      "Epoch 00003: val_loss improved from 6515.70056 to 5880.35609, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 4/3000\n",
      "2100/2100 [==============================] - 2s 764us/step - loss: 5762.9578 - val_loss: 5661.3367\n",
      "\n",
      "Epoch 00004: val_loss improved from 5880.35609 to 5661.33674, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 5/3000\n",
      "2100/2100 [==============================] - 2s 738us/step - loss: 5611.4998 - val_loss: 5571.8860\n",
      "\n",
      "Epoch 00005: val_loss improved from 5661.33674 to 5571.88595, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 6/3000\n",
      "2100/2100 [==============================] - 1s 647us/step - loss: 5540.0302 - val_loss: 5495.5529\n",
      "\n",
      "Epoch 00006: val_loss improved from 5571.88595 to 5495.55286, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 7/3000\n",
      "2100/2100 [==============================] - 2s 767us/step - loss: 5471.5360 - val_loss: 5440.2138\n",
      "\n",
      "Epoch 00007: val_loss improved from 5495.55286 to 5440.21379, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 8/3000\n",
      "2100/2100 [==============================] - 1s 541us/step - loss: 5430.1430 - val_loss: 5411.5538\n",
      "\n",
      "Epoch 00008: val_loss improved from 5440.21379 to 5411.55378, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 9/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 5398.2274 - val_loss: 5393.4925\n",
      "\n",
      "Epoch 00009: val_loss improved from 5411.55378 to 5393.49248, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 10/3000\n",
      "2100/2100 [==============================] - 1s 674us/step - loss: 5390.4215 - val_loss: 5385.3249\n",
      "\n",
      "Epoch 00010: val_loss improved from 5393.49248 to 5385.32489, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 11/3000\n",
      "2100/2100 [==============================] - 1s 598us/step - loss: 5377.8282 - val_loss: 5366.2002\n",
      "\n",
      "Epoch 00011: val_loss improved from 5385.32489 to 5366.20019, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 12/3000\n",
      "2100/2100 [==============================] - 1s 584us/step - loss: 5363.2515 - val_loss: 5347.9203\n",
      "\n",
      "Epoch 00012: val_loss improved from 5366.20019 to 5347.92029, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 13/3000\n",
      "2100/2100 [==============================] - 1s 579us/step - loss: 5329.9934 - val_loss: 5271.4100\n",
      "\n",
      "Epoch 00013: val_loss improved from 5347.92029 to 5271.40999, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 14/3000\n",
      "2100/2100 [==============================] - 1s 497us/step - loss: 5244.3594 - val_loss: 5176.5004\n",
      "\n",
      "Epoch 00014: val_loss improved from 5271.40999 to 5176.50040, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 15/3000\n",
      "2100/2100 [==============================] - 1s 671us/step - loss: 5134.4878 - val_loss: 5091.1397\n",
      "\n",
      "Epoch 00015: val_loss improved from 5176.50040 to 5091.13966, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 16/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 5084.9494 - val_loss: 5040.6036\n",
      "\n",
      "Epoch 00016: val_loss improved from 5091.13966 to 5040.60362, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 17/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 5041.8314 - val_loss: 5009.7210\n",
      "\n",
      "Epoch 00017: val_loss improved from 5040.60362 to 5009.72099, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 18/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 4996.3183 - val_loss: 4922.0066\n",
      "\n",
      "Epoch 00018: val_loss improved from 5009.72099 to 4922.00664, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 19/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 4908.9843 - val_loss: 4821.2804\n",
      "\n",
      "Epoch 00019: val_loss improved from 4922.00664 to 4821.28044, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 20/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 4810.3854 - val_loss: 4750.5549\n",
      "\n",
      "Epoch 00020: val_loss improved from 4821.28044 to 4750.55487, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 21/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 4746.6686 - val_loss: 4671.9383\n",
      "\n",
      "Epoch 00021: val_loss improved from 4750.55487 to 4671.93832, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 22/3000\n",
      "2100/2100 [==============================] - 1s 689us/step - loss: 4658.8883 - val_loss: 4592.8429\n",
      "\n",
      "Epoch 00022: val_loss improved from 4671.93832 to 4592.84288, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 23/3000\n",
      "2100/2100 [==============================] - 1s 665us/step - loss: 4567.3297 - val_loss: 4507.5861\n",
      "\n",
      "Epoch 00023: val_loss improved from 4592.84288 to 4507.58606, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 24/3000\n",
      "2100/2100 [==============================] - 1s 532us/step - loss: 4516.8229 - val_loss: 4466.5913\n",
      "\n",
      "Epoch 00024: val_loss improved from 4507.58606 to 4466.59126, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 25/3000\n",
      "2100/2100 [==============================] - 2s 744us/step - loss: 4463.0546 - val_loss: 4391.0661\n",
      "\n",
      "Epoch 00025: val_loss improved from 4466.59126 to 4391.06615, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 26/3000\n",
      "2100/2100 [==============================] - 1s 659us/step - loss: 4392.8680 - val_loss: 4365.0576\n",
      "\n",
      "Epoch 00026: val_loss improved from 4391.06615 to 4365.05760, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 27/3000\n",
      "2100/2100 [==============================] - 1s 638us/step - loss: 4343.6848 - val_loss: 4323.5625\n",
      "\n",
      "Epoch 00027: val_loss improved from 4365.05760 to 4323.56248, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 28/3000\n",
      "2100/2100 [==============================] - 1s 541us/step - loss: 4316.0452 - val_loss: 4274.4735\n",
      "\n",
      "Epoch 00028: val_loss improved from 4323.56248 to 4274.47349, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 29/3000\n",
      "2100/2100 [==============================] - 1s 624us/step - loss: 4259.8697 - val_loss: 4231.4609\n",
      "\n",
      "Epoch 00029: val_loss improved from 4274.47349 to 4231.46095, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 30/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 4218.3651 - val_loss: 4199.8940\n",
      "\n",
      "Epoch 00030: val_loss improved from 4231.46095 to 4199.89397, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 31/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 4188.8175 - val_loss: 4181.3307\n",
      "\n",
      "Epoch 00031: val_loss improved from 4199.89397 to 4181.33068, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 32/3000\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 4165.7122 - val_loss: 4172.3250\n",
      "\n",
      "Epoch 00032: val_loss improved from 4181.33068 to 4172.32495, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/3000\n",
      "2100/2100 [==============================] - 1s 474us/step - loss: 4136.5440 - val_loss: 4118.4368\n",
      "\n",
      "Epoch 00033: val_loss improved from 4172.32495 to 4118.43680, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 34/3000\n",
      "2100/2100 [==============================] - 1s 500us/step - loss: 4107.1944 - val_loss: 4109.6512\n",
      "\n",
      "Epoch 00034: val_loss improved from 4118.43680 to 4109.65117, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 35/3000\n",
      "2100/2100 [==============================] - 1s 568us/step - loss: 4081.2163 - val_loss: 4065.7883\n",
      "\n",
      "Epoch 00035: val_loss improved from 4109.65117 to 4065.78834, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 36/3000\n",
      "2100/2100 [==============================] - 1s 640us/step - loss: 4030.3031 - val_loss: 4016.9480\n",
      "\n",
      "Epoch 00036: val_loss improved from 4065.78834 to 4016.94797, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 37/3000\n",
      "2100/2100 [==============================] - 1s 678us/step - loss: 3981.7037 - val_loss: 3989.3546\n",
      "\n",
      "Epoch 00037: val_loss improved from 4016.94797 to 3989.35460, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 38/3000\n",
      "2100/2100 [==============================] - 1s 517us/step - loss: 3938.4443 - val_loss: 3918.5595\n",
      "\n",
      "Epoch 00038: val_loss improved from 3989.35460 to 3918.55951, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 39/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 3880.0511 - val_loss: 3903.9889\n",
      "\n",
      "Epoch 00039: val_loss improved from 3918.55951 to 3903.98889, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 40/3000\n",
      "2100/2100 [==============================] - 1s 586us/step - loss: 3838.6292 - val_loss: 3852.1927\n",
      "\n",
      "Epoch 00040: val_loss improved from 3903.98889 to 3852.19268, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 41/3000\n",
      "2100/2100 [==============================] - 1s 632us/step - loss: 3792.9392 - val_loss: 3795.1306\n",
      "\n",
      "Epoch 00041: val_loss improved from 3852.19268 to 3795.13057, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 42/3000\n",
      "2100/2100 [==============================] - 1s 636us/step - loss: 3753.6698 - val_loss: 3746.5073\n",
      "\n",
      "Epoch 00042: val_loss improved from 3795.13057 to 3746.50728, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 43/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 3714.9467 - val_loss: 3720.7391\n",
      "\n",
      "Epoch 00043: val_loss improved from 3746.50728 to 3720.73915, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 44/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 3679.8652 - val_loss: 3692.8067\n",
      "\n",
      "Epoch 00044: val_loss improved from 3720.73915 to 3692.80674, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 45/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 3653.6743 - val_loss: 3677.4684\n",
      "\n",
      "Epoch 00045: val_loss improved from 3692.80674 to 3677.46840, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 46/3000\n",
      "2100/2100 [==============================] - 1s 556us/step - loss: 3654.8897 - val_loss: 3658.5076\n",
      "\n",
      "Epoch 00046: val_loss improved from 3677.46840 to 3658.50758, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 47/3000\n",
      "2100/2100 [==============================] - 1s 609us/step - loss: 3625.3840 - val_loss: 3649.3519\n",
      "\n",
      "Epoch 00047: val_loss improved from 3658.50758 to 3649.35192, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 48/3000\n",
      "2100/2100 [==============================] - 1s 565us/step - loss: 3597.3563 - val_loss: 3635.9536\n",
      "\n",
      "Epoch 00048: val_loss improved from 3649.35192 to 3635.95360, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 49/3000\n",
      "2100/2100 [==============================] - 1s 574us/step - loss: 3581.6196 - val_loss: 3609.0350\n",
      "\n",
      "Epoch 00049: val_loss improved from 3635.95360 to 3609.03499, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 50/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 3551.3293 - val_loss: 3586.3153\n",
      "\n",
      "Epoch 00050: val_loss improved from 3609.03499 to 3586.31529, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 51/3000\n",
      "2100/2100 [==============================] - 1s 501us/step - loss: 3529.9913 - val_loss: 3575.5541\n",
      "\n",
      "Epoch 00051: val_loss improved from 3586.31529 to 3575.55405, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 52/3000\n",
      "2100/2100 [==============================] - 1s 687us/step - loss: 3534.9376 - val_loss: 3567.4274\n",
      "\n",
      "Epoch 00052: val_loss improved from 3575.55405 to 3567.42736, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 53/3000\n",
      "2100/2100 [==============================] - 1s 662us/step - loss: 3511.3134 - val_loss: 3546.0307\n",
      "\n",
      "Epoch 00053: val_loss improved from 3567.42736 to 3546.03068, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 54/3000\n",
      "2100/2100 [==============================] - 1s 440us/step - loss: 3480.1714 - val_loss: 3513.3801\n",
      "\n",
      "Epoch 00054: val_loss improved from 3546.03068 to 3513.38005, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 55/3000\n",
      "2100/2100 [==============================] - 1s 423us/step - loss: 3451.0173 - val_loss: 3505.1852\n",
      "\n",
      "Epoch 00055: val_loss improved from 3513.38005 to 3505.18521, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 56/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 3434.3203 - val_loss: 3490.2137\n",
      "\n",
      "Epoch 00056: val_loss improved from 3505.18521 to 3490.21374, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 57/3000\n",
      "2100/2100 [==============================] - 1s 607us/step - loss: 3420.2148 - val_loss: 3465.2217\n",
      "\n",
      "Epoch 00057: val_loss improved from 3490.21374 to 3465.22172, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 58/3000\n",
      "2100/2100 [==============================] - 1s 617us/step - loss: 3381.4424 - val_loss: 3446.4959\n",
      "\n",
      "Epoch 00058: val_loss improved from 3465.22172 to 3446.49590, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 59/3000\n",
      "2100/2100 [==============================] - 1s 573us/step - loss: 3363.8078 - val_loss: 3426.6182\n",
      "\n",
      "Epoch 00059: val_loss improved from 3446.49590 to 3426.61818, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 60/3000\n",
      "2100/2100 [==============================] - 1s 610us/step - loss: 3346.4267 - val_loss: 3405.4023\n",
      "\n",
      "Epoch 00060: val_loss improved from 3426.61818 to 3405.40228, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 61/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 3359.9709 - val_loss: 3390.5750\n",
      "\n",
      "Epoch 00061: val_loss improved from 3405.40228 to 3390.57501, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 62/3000\n",
      "2100/2100 [==============================] - 1s 522us/step - loss: 3310.2592 - val_loss: 3386.2488\n",
      "\n",
      "Epoch 00062: val_loss improved from 3390.57501 to 3386.24877, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 63/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 3286.4304 - val_loss: 3357.7837\n",
      "\n",
      "Epoch 00063: val_loss improved from 3386.24877 to 3357.78374, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 64/3000\n",
      "2100/2100 [==============================] - 1s 426us/step - loss: 3259.6723 - val_loss: 3323.9443\n",
      "\n",
      "Epoch 00064: val_loss improved from 3357.78374 to 3323.94430, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/3000\n",
      "2100/2100 [==============================] - 1s 437us/step - loss: 3237.2533 - val_loss: 3314.7878\n",
      "\n",
      "Epoch 00065: val_loss improved from 3323.94430 to 3314.78775, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 66/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 3217.7404 - val_loss: 3298.3035\n",
      "\n",
      "Epoch 00066: val_loss improved from 3314.78775 to 3298.30350, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 67/3000\n",
      "2100/2100 [==============================] - 1s 591us/step - loss: 3207.5090 - val_loss: 3291.8348\n",
      "\n",
      "Epoch 00067: val_loss improved from 3298.30350 to 3291.83483, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 68/3000\n",
      "2100/2100 [==============================] - 1s 574us/step - loss: 3195.7801 - val_loss: 3281.2352\n",
      "\n",
      "Epoch 00068: val_loss improved from 3291.83483 to 3281.23522, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 69/3000\n",
      "2100/2100 [==============================] - 1s 583us/step - loss: 3188.5150 - val_loss: 3273.6016\n",
      "\n",
      "Epoch 00069: val_loss improved from 3281.23522 to 3273.60157, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 70/3000\n",
      "2100/2100 [==============================] - 1s 559us/step - loss: 3172.3763 - val_loss: 3262.7759\n",
      "\n",
      "Epoch 00070: val_loss improved from 3273.60157 to 3262.77592, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 71/3000\n",
      "2100/2100 [==============================] - 1s 671us/step - loss: 3163.4182 - val_loss: 3266.6914\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 3262.77592\n",
      "Epoch 72/3000\n",
      "2100/2100 [==============================] - 1s 581us/step - loss: 3152.5077 - val_loss: 3242.2886\n",
      "\n",
      "Epoch 00072: val_loss improved from 3262.77592 to 3242.28861, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 73/3000\n",
      "2100/2100 [==============================] - 1s 611us/step - loss: 3130.2813 - val_loss: 3233.0757\n",
      "\n",
      "Epoch 00073: val_loss improved from 3242.28861 to 3233.07571, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 74/3000\n",
      "2100/2100 [==============================] - 1s 690us/step - loss: 3119.7025 - val_loss: 3216.5733\n",
      "\n",
      "Epoch 00074: val_loss improved from 3233.07571 to 3216.57334, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 75/3000\n",
      "2100/2100 [==============================] - 2s 734us/step - loss: 3101.7492 - val_loss: 3201.8400\n",
      "\n",
      "Epoch 00075: val_loss improved from 3216.57334 to 3201.84001, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 76/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 3081.5444 - val_loss: 3177.8651\n",
      "\n",
      "Epoch 00076: val_loss improved from 3201.84001 to 3177.86508, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 77/3000\n",
      "2100/2100 [==============================] - 1s 506us/step - loss: 3073.2140 - val_loss: 3219.3225\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 3177.86508\n",
      "Epoch 78/3000\n",
      "2100/2100 [==============================] - 1s 605us/step - loss: 3050.1081 - val_loss: 3155.7176\n",
      "\n",
      "Epoch 00078: val_loss improved from 3177.86508 to 3155.71765, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 79/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 3014.4593 - val_loss: 3128.9689\n",
      "\n",
      "Epoch 00079: val_loss improved from 3155.71765 to 3128.96890, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 80/3000\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2999.5201 - val_loss: 3111.4199\n",
      "\n",
      "Epoch 00080: val_loss improved from 3128.96890 to 3111.41994, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 81/3000\n",
      "2100/2100 [==============================] - 1s 463us/step - loss: 2982.7886 - val_loss: 3101.4614\n",
      "\n",
      "Epoch 00081: val_loss improved from 3111.41994 to 3101.46136, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 82/3000\n",
      "2100/2100 [==============================] - 1s 475us/step - loss: 2961.9200 - val_loss: 3078.0984\n",
      "\n",
      "Epoch 00082: val_loss improved from 3101.46136 to 3078.09835, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 83/3000\n",
      "2100/2100 [==============================] - 2s 720us/step - loss: 2942.0403 - val_loss: 3061.1139\n",
      "\n",
      "Epoch 00083: val_loss improved from 3078.09835 to 3061.11390, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 84/3000\n",
      "2100/2100 [==============================] - 1s 652us/step - loss: 2919.8789 - val_loss: 3062.6136\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 3061.11390\n",
      "Epoch 85/3000\n",
      "2100/2100 [==============================] - 1s 459us/step - loss: 2921.1940 - val_loss: 3047.0432\n",
      "\n",
      "Epoch 00085: val_loss improved from 3061.11390 to 3047.04318, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 86/3000\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2898.3465 - val_loss: 3033.1224\n",
      "\n",
      "Epoch 00086: val_loss improved from 3047.04318 to 3033.12237, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 87/3000\n",
      "2100/2100 [==============================] - 1s 594us/step - loss: 2887.7890 - val_loss: 3027.0097\n",
      "\n",
      "Epoch 00087: val_loss improved from 3033.12237 to 3027.00972, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 88/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2872.1228 - val_loss: 3012.4233\n",
      "\n",
      "Epoch 00088: val_loss improved from 3027.00972 to 3012.42326, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 89/3000\n",
      "2100/2100 [==============================] - 1s 692us/step - loss: 2902.4179 - val_loss: 3057.5658\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 3012.42326\n",
      "Epoch 90/3000\n",
      "2100/2100 [==============================] - 1s 460us/step - loss: 2905.7474 - val_loss: 3034.1919\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 3012.42326\n",
      "Epoch 91/3000\n",
      "2100/2100 [==============================] - 1s 669us/step - loss: 2858.9700 - val_loss: 2993.5123\n",
      "\n",
      "Epoch 00091: val_loss improved from 3012.42326 to 2993.51228, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 92/3000\n",
      "2100/2100 [==============================] - 1s 480us/step - loss: 2831.3632 - val_loss: 2980.7557\n",
      "\n",
      "Epoch 00092: val_loss improved from 2993.51228 to 2980.75570, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 93/3000\n",
      "2100/2100 [==============================] - 2s 732us/step - loss: 2815.3259 - val_loss: 2961.4470\n",
      "\n",
      "Epoch 00093: val_loss improved from 2980.75570 to 2961.44700, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 94/3000\n",
      "2100/2100 [==============================] - 1s 613us/step - loss: 2793.7885 - val_loss: 2954.0388\n",
      "\n",
      "Epoch 00094: val_loss improved from 2961.44700 to 2954.03882, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 95/3000\n",
      "2100/2100 [==============================] - 1s 643us/step - loss: 2787.5975 - val_loss: 2949.9190\n",
      "\n",
      "Epoch 00095: val_loss improved from 2954.03882 to 2949.91901, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 96/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2779.6437 - val_loss: 2939.3490\n",
      "\n",
      "Epoch 00096: val_loss improved from 2949.91901 to 2939.34901, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 97/3000\n",
      "2100/2100 [==============================] - 1s 425us/step - loss: 2767.0840 - val_loss: 2933.9093\n",
      "\n",
      "Epoch 00097: val_loss improved from 2939.34901 to 2933.90930, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 98/3000\n",
      "2100/2100 [==============================] - 1s 464us/step - loss: 2759.4776 - val_loss: 2925.1473\n",
      "\n",
      "Epoch 00098: val_loss improved from 2933.90930 to 2925.14728, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/3000\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 2752.9676 - val_loss: 2930.2905\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2925.14728\n",
      "Epoch 100/3000\n",
      "2100/2100 [==============================] - 1s 632us/step - loss: 2744.1657 - val_loss: 2919.3694\n",
      "\n",
      "Epoch 00100: val_loss improved from 2925.14728 to 2919.36940, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 101/3000\n",
      "2100/2100 [==============================] - 1s 593us/step - loss: 2746.8628 - val_loss: 2913.2812\n",
      "\n",
      "Epoch 00101: val_loss improved from 2919.36940 to 2913.28124, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 102/3000\n",
      "2100/2100 [==============================] - 1s 551us/step - loss: 2731.8739 - val_loss: 2906.2583\n",
      "\n",
      "Epoch 00102: val_loss improved from 2913.28124 to 2906.25834, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 103/3000\n",
      "2100/2100 [==============================] - 1s 657us/step - loss: 2725.2001 - val_loss: 2894.8111\n",
      "\n",
      "Epoch 00103: val_loss improved from 2906.25834 to 2894.81111, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 104/3000\n",
      "2100/2100 [==============================] - 1s 670us/step - loss: 2705.4337 - val_loss: 2897.4940\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 2894.81111\n",
      "Epoch 105/3000\n",
      "2100/2100 [==============================] - 1s 522us/step - loss: 2703.9469 - val_loss: 2881.0731\n",
      "\n",
      "Epoch 00105: val_loss improved from 2894.81111 to 2881.07306, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 106/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2690.3301 - val_loss: 2882.4852\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 2881.07306\n",
      "Epoch 107/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2680.3941 - val_loss: 2892.4035\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 2881.07306\n",
      "Epoch 108/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2692.4729 - val_loss: 2872.7974\n",
      "\n",
      "Epoch 00108: val_loss improved from 2881.07306 to 2872.79745, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 109/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2682.2730 - val_loss: 2857.5690\n",
      "\n",
      "Epoch 00109: val_loss improved from 2872.79745 to 2857.56896, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 110/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 2658.7675 - val_loss: 2864.8052\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 2857.56896\n",
      "Epoch 111/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2658.9023 - val_loss: 2857.5360\n",
      "\n",
      "Epoch 00111: val_loss improved from 2857.56896 to 2857.53604, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 112/3000\n",
      "2100/2100 [==============================] - 2s 740us/step - loss: 2649.2724 - val_loss: 2854.2275\n",
      "\n",
      "Epoch 00112: val_loss improved from 2857.53604 to 2854.22749, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 113/3000\n",
      "2100/2100 [==============================] - 1s 646us/step - loss: 2630.4652 - val_loss: 2835.6546\n",
      "\n",
      "Epoch 00113: val_loss improved from 2854.22749 to 2835.65464, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 114/3000\n",
      "2100/2100 [==============================] - 1s 580us/step - loss: 2618.1010 - val_loss: 2826.6919\n",
      "\n",
      "Epoch 00114: val_loss improved from 2835.65464 to 2826.69195, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 115/3000\n",
      "2100/2100 [==============================] - 1s 673us/step - loss: 2609.1625 - val_loss: 2818.9364\n",
      "\n",
      "Epoch 00115: val_loss improved from 2826.69195 to 2818.93636, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 116/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2611.8032 - val_loss: 2863.9954\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 2818.93636\n",
      "Epoch 117/3000\n",
      "2100/2100 [==============================] - 1s 424us/step - loss: 2628.1747 - val_loss: 2811.8311\n",
      "\n",
      "Epoch 00117: val_loss improved from 2818.93636 to 2811.83110, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 118/3000\n",
      "2100/2100 [==============================] - 1s 436us/step - loss: 2601.0128 - val_loss: 2790.2904\n",
      "\n",
      "Epoch 00118: val_loss improved from 2811.83110 to 2790.29044, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 119/3000\n",
      "2100/2100 [==============================] - 1s 458us/step - loss: 2566.0572 - val_loss: 2793.2316\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 2790.29044\n",
      "Epoch 120/3000\n",
      "2100/2100 [==============================] - 1s 694us/step - loss: 2556.4256 - val_loss: 2784.4546\n",
      "\n",
      "Epoch 00120: val_loss improved from 2790.29044 to 2784.45459, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 121/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2548.8920 - val_loss: 2782.0041\n",
      "\n",
      "Epoch 00121: val_loss improved from 2784.45459 to 2782.00411, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 122/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2542.3751 - val_loss: 2773.0034\n",
      "\n",
      "Epoch 00122: val_loss improved from 2782.00411 to 2773.00340, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 123/3000\n",
      "2100/2100 [==============================] - 1s 511us/step - loss: 2536.2837 - val_loss: 2773.2415\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 2773.00340\n",
      "Epoch 124/3000\n",
      "2100/2100 [==============================] - 1s 689us/step - loss: 2532.5757 - val_loss: 2776.4847\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 2773.00340\n",
      "Epoch 125/3000\n",
      "2100/2100 [==============================] - 1s 639us/step - loss: 2610.7125 - val_loss: 2792.4792\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 2773.00340\n",
      "Epoch 126/3000\n",
      "2100/2100 [==============================] - 1s 531us/step - loss: 2565.9030 - val_loss: 2790.2421\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 2773.00340\n",
      "Epoch 127/3000\n",
      "2100/2100 [==============================] - 1s 692us/step - loss: 2534.6898 - val_loss: 2768.4723\n",
      "\n",
      "Epoch 00127: val_loss improved from 2773.00340 to 2768.47225, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 128/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2513.8538 - val_loss: 2760.4624\n",
      "\n",
      "Epoch 00128: val_loss improved from 2768.47225 to 2760.46245, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 129/3000\n",
      "2100/2100 [==============================] - 2s 720us/step - loss: 2507.5453 - val_loss: 2757.6735\n",
      "\n",
      "Epoch 00129: val_loss improved from 2760.46245 to 2757.67349, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 130/3000\n",
      "2100/2100 [==============================] - 1s 628us/step - loss: 2502.0799 - val_loss: 2752.5432\n",
      "\n",
      "Epoch 00130: val_loss improved from 2757.67349 to 2752.54320, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 131/3000\n",
      "2100/2100 [==============================] - 1s 663us/step - loss: 2498.4361 - val_loss: 2749.9632\n",
      "\n",
      "Epoch 00131: val_loss improved from 2752.54320 to 2749.96317, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 132/3000\n",
      "2100/2100 [==============================] - 1s 526us/step - loss: 2493.3140 - val_loss: 2757.5538\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 2749.96317\n",
      "Epoch 133/3000\n",
      "2100/2100 [==============================] - 1s 690us/step - loss: 2490.1664 - val_loss: 2739.6889\n",
      "\n",
      "Epoch 00133: val_loss improved from 2749.96317 to 2739.68891, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 134/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 2476.2795 - val_loss: 2735.8468\n",
      "\n",
      "Epoch 00134: val_loss improved from 2739.68891 to 2735.84683, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/3000\n",
      "2100/2100 [==============================] - 1s 511us/step - loss: 2471.2461 - val_loss: 2740.6630\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 2735.84683\n",
      "Epoch 136/3000\n",
      "2100/2100 [==============================] - 1s 634us/step - loss: 2468.0815 - val_loss: 2728.5691\n",
      "\n",
      "Epoch 00136: val_loss improved from 2735.84683 to 2728.56908, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 137/3000\n",
      "2100/2100 [==============================] - 1s 582us/step - loss: 2462.5733 - val_loss: 2739.0213\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 2728.56908\n",
      "Epoch 138/3000\n",
      "2100/2100 [==============================] - 1s 601us/step - loss: 2458.1934 - val_loss: 2736.5806\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 2728.56908\n",
      "Epoch 139/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2470.2704 - val_loss: 2720.5128\n",
      "\n",
      "Epoch 00139: val_loss improved from 2728.56908 to 2720.51282, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 140/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2457.3412 - val_loss: 2716.8035\n",
      "\n",
      "Epoch 00140: val_loss improved from 2720.51282 to 2716.80355, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 141/3000\n",
      "2100/2100 [==============================] - 1s 457us/step - loss: 2442.1396 - val_loss: 2710.1683\n",
      "\n",
      "Epoch 00141: val_loss improved from 2716.80355 to 2710.16835, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 142/3000\n",
      "2100/2100 [==============================] - 2s 746us/step - loss: 2431.8010 - val_loss: 2712.5612\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 2710.16835\n",
      "Epoch 143/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2426.8823 - val_loss: 2722.4641\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 2710.16835\n",
      "Epoch 144/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2445.3090 - val_loss: 2716.9258\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 2710.16835\n",
      "Epoch 145/3000\n",
      "2100/2100 [==============================] - 1s 477us/step - loss: 2435.4451 - val_loss: 2735.0458\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 2710.16835\n",
      "Epoch 146/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2440.8201 - val_loss: 2728.3111\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 2710.16835\n",
      "Epoch 147/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2421.2222 - val_loss: 2732.5565\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 2710.16835\n",
      "Epoch 148/3000\n",
      "2100/2100 [==============================] - 1s 409us/step - loss: 2421.9775 - val_loss: 2712.5481\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 2710.16835\n",
      "Epoch 149/3000\n",
      "2100/2100 [==============================] - 1s 687us/step - loss: 2404.7180 - val_loss: 2698.4600\n",
      "\n",
      "Epoch 00149: val_loss improved from 2710.16835 to 2698.46003, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 150/3000\n",
      "2100/2100 [==============================] - 1s 527us/step - loss: 2396.2308 - val_loss: 2697.1999\n",
      "\n",
      "Epoch 00150: val_loss improved from 2698.46003 to 2697.19994, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 151/3000\n",
      "2100/2100 [==============================] - 1s 527us/step - loss: 2393.6432 - val_loss: 2692.4514\n",
      "\n",
      "Epoch 00151: val_loss improved from 2697.19994 to 2692.45137, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 152/3000\n",
      "2100/2100 [==============================] - 1s 575us/step - loss: 2386.7679 - val_loss: 2699.9020\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 2692.45137\n",
      "Epoch 153/3000\n",
      "2100/2100 [==============================] - 1s 594us/step - loss: 2381.2569 - val_loss: 2691.3146\n",
      "\n",
      "Epoch 00153: val_loss improved from 2692.45137 to 2691.31465, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 154/3000\n",
      "2100/2100 [==============================] - 1s 595us/step - loss: 2375.9845 - val_loss: 2689.7695\n",
      "\n",
      "Epoch 00154: val_loss improved from 2691.31465 to 2689.76946, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 155/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2373.2748 - val_loss: 2692.6266\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 2689.76946\n",
      "Epoch 156/3000\n",
      "2100/2100 [==============================] - 1s 646us/step - loss: 2391.7772 - val_loss: 2691.9791\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 2689.76946\n",
      "Epoch 157/3000\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 2381.5443 - val_loss: 2686.9959\n",
      "\n",
      "Epoch 00157: val_loss improved from 2689.76946 to 2686.99590, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 158/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2368.6584 - val_loss: 2694.8372\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 2686.99590\n",
      "Epoch 159/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2385.0827 - val_loss: 2690.0662\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 2686.99590\n",
      "Epoch 160/3000\n",
      "2100/2100 [==============================] - 1s 475us/step - loss: 2359.1622 - val_loss: 2686.9170\n",
      "\n",
      "Epoch 00160: val_loss improved from 2686.99590 to 2686.91701, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 161/3000\n",
      "2100/2100 [==============================] - 1s 436us/step - loss: 2349.4951 - val_loss: 2680.2010\n",
      "\n",
      "Epoch 00161: val_loss improved from 2686.91701 to 2680.20097, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 162/3000\n",
      "2100/2100 [==============================] - 1s 434us/step - loss: 2342.5776 - val_loss: 2672.2819\n",
      "\n",
      "Epoch 00162: val_loss improved from 2680.20097 to 2672.28194, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 163/3000\n",
      "2100/2100 [==============================] - 1s 486us/step - loss: 2337.6774 - val_loss: 2668.1099\n",
      "\n",
      "Epoch 00163: val_loss improved from 2672.28194 to 2668.10987, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 164/3000\n",
      "2100/2100 [==============================] - 1s 531us/step - loss: 2338.6167 - val_loss: 2674.9572\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 2668.10987\n",
      "Epoch 165/3000\n",
      "2100/2100 [==============================] - 1s 605us/step - loss: 2332.8685 - val_loss: 2675.9560\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 2668.10987\n",
      "Epoch 166/3000\n",
      "2100/2100 [==============================] - 1s 692us/step - loss: 2339.4245 - val_loss: 2663.1096\n",
      "\n",
      "Epoch 00166: val_loss improved from 2668.10987 to 2663.10957, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 167/3000\n",
      "2100/2100 [==============================] - 1s 554us/step - loss: 2334.1606 - val_loss: 2666.2459\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 2663.10957\n",
      "Epoch 168/3000\n",
      "2100/2100 [==============================] - 2s 754us/step - loss: 2319.6383 - val_loss: 2690.6507\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 2663.10957\n",
      "Epoch 169/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2346.9039 - val_loss: 2662.4887\n",
      "\n",
      "Epoch 00169: val_loss improved from 2663.10957 to 2662.48869, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 170/3000\n",
      "2100/2100 [==============================] - 1s 483us/step - loss: 2316.3702 - val_loss: 2665.5803\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 2662.48869\n",
      "Epoch 171/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2308.6968 - val_loss: 2655.6891\n",
      "\n",
      "Epoch 00171: val_loss improved from 2662.48869 to 2655.68907, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 172/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2301.1699 - val_loss: 2659.9451\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 2655.68907\n",
      "Epoch 173/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2303.1207 - val_loss: 2659.5914\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 2655.68907\n",
      "Epoch 174/3000\n",
      "2100/2100 [==============================] - 1s 411us/step - loss: 2296.8655 - val_loss: 2657.3770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00174: val_loss did not improve from 2655.68907\n",
      "Epoch 175/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2297.4210 - val_loss: 2667.4474\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 2655.68907\n",
      "Epoch 176/3000\n",
      "2100/2100 [==============================] - 1s 457us/step - loss: 2303.4984 - val_loss: 2683.8023\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 2655.68907\n",
      "Epoch 177/3000\n",
      "2100/2100 [==============================] - 1s 680us/step - loss: 2313.8559 - val_loss: 2655.5784\n",
      "\n",
      "Epoch 00177: val_loss improved from 2655.68907 to 2655.57844, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 178/3000\n",
      "2100/2100 [==============================] - 1s 449us/step - loss: 2287.6741 - val_loss: 2653.9752\n",
      "\n",
      "Epoch 00178: val_loss improved from 2655.57844 to 2653.97515, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 179/3000\n",
      "2100/2100 [==============================] - 1s 462us/step - loss: 2279.3652 - val_loss: 2644.5886\n",
      "\n",
      "Epoch 00179: val_loss improved from 2653.97515 to 2644.58861, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 180/3000\n",
      "2100/2100 [==============================] - 1s 493us/step - loss: 2277.2321 - val_loss: 2652.6924\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 2644.58861\n",
      "Epoch 181/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2296.1468 - val_loss: 2666.7305\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 2644.58861\n",
      "Epoch 182/3000\n",
      "2100/2100 [==============================] - 1s 425us/step - loss: 2279.0026 - val_loss: 2688.4966\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 2644.58861\n",
      "Epoch 183/3000\n",
      "2100/2100 [==============================] - 1s 665us/step - loss: 2278.8669 - val_loss: 2662.1431\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 2644.58861\n",
      "Epoch 184/3000\n",
      "2100/2100 [==============================] - 1s 459us/step - loss: 2274.5117 - val_loss: 2691.1360\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 2644.58861\n",
      "Epoch 185/3000\n",
      "2100/2100 [==============================] - 1s 692us/step - loss: 2270.9920 - val_loss: 2654.1273\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 2644.58861\n",
      "Epoch 186/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2253.9243 - val_loss: 2629.0118\n",
      "\n",
      "Epoch 00186: val_loss improved from 2644.58861 to 2629.01180, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 187/3000\n",
      "2100/2100 [==============================] - 1s 404us/step - loss: 2245.7643 - val_loss: 2635.1192\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 2629.01180\n",
      "Epoch 188/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2239.0134 - val_loss: 2620.8607\n",
      "\n",
      "Epoch 00188: val_loss improved from 2629.01180 to 2620.86070, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 189/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2232.3161 - val_loss: 2624.0039\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 2620.86070\n",
      "Epoch 190/3000\n",
      "2100/2100 [==============================] - 1s 514us/step - loss: 2225.4642 - val_loss: 2630.3997\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 2620.86070\n",
      "Epoch 191/3000\n",
      "2100/2100 [==============================] - 1s 668us/step - loss: 2225.8413 - val_loss: 2618.8951\n",
      "\n",
      "Epoch 00191: val_loss improved from 2620.86070 to 2618.89509, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 192/3000\n",
      "2100/2100 [==============================] - 1s 591us/step - loss: 2228.5734 - val_loss: 2638.2503\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 2618.89509\n",
      "Epoch 193/3000\n",
      "2100/2100 [==============================] - 1s 630us/step - loss: 2235.1142 - val_loss: 2629.1945\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 2618.89509\n",
      "Epoch 194/3000\n",
      "2100/2100 [==============================] - 2s 765us/step - loss: 2240.7958 - val_loss: 2618.4745\n",
      "\n",
      "Epoch 00194: val_loss improved from 2618.89509 to 2618.47452, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 195/3000\n",
      "2100/2100 [==============================] - 2s 829us/step - loss: 2234.6301 - val_loss: 2621.7314\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 2618.47452\n",
      "Epoch 196/3000\n",
      "2100/2100 [==============================] - 2s 743us/step - loss: 2217.7963 - val_loss: 2617.1162\n",
      "\n",
      "Epoch 00196: val_loss improved from 2618.47452 to 2617.11624, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 197/3000\n",
      "2100/2100 [==============================] - 1s 644us/step - loss: 2203.0248 - val_loss: 2607.4354\n",
      "\n",
      "Epoch 00197: val_loss improved from 2617.11624 to 2607.43535, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 198/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2195.2972 - val_loss: 2604.5477\n",
      "\n",
      "Epoch 00198: val_loss improved from 2607.43535 to 2604.54771, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 199/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2192.6102 - val_loss: 2622.0673\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 2604.54771\n",
      "Epoch 200/3000\n",
      "2100/2100 [==============================] - 1s 436us/step - loss: 2191.6367 - val_loss: 2608.2486\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 2604.54771\n",
      "Epoch 201/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2185.4780 - val_loss: 2614.1104\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 2604.54771\n",
      "Epoch 202/3000\n",
      "2100/2100 [==============================] - 1s 464us/step - loss: 2193.4053 - val_loss: 2600.8937\n",
      "\n",
      "Epoch 00202: val_loss improved from 2604.54771 to 2600.89370, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 203/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2183.8850 - val_loss: 2602.8962\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 2600.89370\n",
      "Epoch 204/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2181.9364 - val_loss: 2619.5357\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 2600.89370\n",
      "Epoch 205/3000\n",
      "2100/2100 [==============================] - 1s 458us/step - loss: 2195.8463 - val_loss: 2601.2755\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 2600.89370\n",
      "Epoch 206/3000\n",
      "2100/2100 [==============================] - 1s 694us/step - loss: 2175.8506 - val_loss: 2597.4684\n",
      "\n",
      "Epoch 00206: val_loss improved from 2600.89370 to 2597.46841, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 207/3000\n",
      "2100/2100 [==============================] - 1s 496us/step - loss: 2160.9491 - val_loss: 2587.7377\n",
      "\n",
      "Epoch 00207: val_loss improved from 2597.46841 to 2587.73770, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 208/3000\n",
      "2100/2100 [==============================] - 2s 729us/step - loss: 2157.4213 - val_loss: 2593.5371\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 2587.73770\n",
      "Epoch 209/3000\n",
      "2100/2100 [==============================] - 1s 472us/step - loss: 2164.8301 - val_loss: 2604.7018\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 2587.73770\n",
      "Epoch 210/3000\n",
      "2100/2100 [==============================] - 1s 664us/step - loss: 2153.5589 - val_loss: 2596.1854\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 2587.73770\n",
      "Epoch 211/3000\n",
      "2100/2100 [==============================] - 1s 477us/step - loss: 2153.4821 - val_loss: 2595.5985\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 2587.73770\n",
      "Epoch 212/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2157.5569 - val_loss: 2643.0882\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 2587.73770\n",
      "Epoch 213/3000\n",
      "2100/2100 [==============================] - 1s 410us/step - loss: 2178.5595 - val_loss: 2596.4307\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 2587.73770\n",
      "Epoch 214/3000\n",
      "2100/2100 [==============================] - 1s 695us/step - loss: 2148.3160 - val_loss: 2599.3521\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 2587.73770\n",
      "Epoch 215/3000\n",
      "2100/2100 [==============================] - 1s 462us/step - loss: 2143.6578 - val_loss: 2584.2448\n",
      "\n",
      "Epoch 00215: val_loss improved from 2587.73770 to 2584.24482, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/3000\n",
      "2100/2100 [==============================] - 1s 447us/step - loss: 2134.1375 - val_loss: 2587.7808\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 2584.24482\n",
      "Epoch 217/3000\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2131.9580 - val_loss: 2593.3962\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 2584.24482\n",
      "Epoch 218/3000\n",
      "2100/2100 [==============================] - 1s 692us/step - loss: 2132.3424 - val_loss: 2588.3430\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 2584.24482\n",
      "Epoch 219/3000\n",
      "2100/2100 [==============================] - 1s 443us/step - loss: 2132.1647 - val_loss: 2595.3565\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 2584.24482\n",
      "Epoch 220/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2133.7283 - val_loss: 2589.0445\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 2584.24482\n",
      "Epoch 221/3000\n",
      "2100/2100 [==============================] - 1s 431us/step - loss: 2120.2973 - val_loss: 2586.3151\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 2584.24482\n",
      "Epoch 222/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 2130.9872 - val_loss: 2596.1506\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 2584.24482\n",
      "Epoch 223/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2121.4947 - val_loss: 2590.6726\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 2584.24482\n",
      "Epoch 224/3000\n",
      "2100/2100 [==============================] - 1s 477us/step - loss: 2119.3390 - val_loss: 2602.6076\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 2584.24482\n",
      "Epoch 225/3000\n",
      "2100/2100 [==============================] - 1s 669us/step - loss: 2127.2059 - val_loss: 2599.3719\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 2584.24482\n",
      "\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 226/3000\n",
      "2100/2100 [==============================] - 1s 528us/step - loss: 2100.3290 - val_loss: 2562.2153\n",
      "\n",
      "Epoch 00226: val_loss improved from 2584.24482 to 2562.21525, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 227/3000\n",
      "2100/2100 [==============================] - 1s 464us/step - loss: 2081.6765 - val_loss: 2557.5954\n",
      "\n",
      "Epoch 00227: val_loss improved from 2562.21525 to 2557.59542, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 228/3000\n",
      "2100/2100 [==============================] - 2s 747us/step - loss: 2076.6697 - val_loss: 2558.5800\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 2557.59542\n",
      "Epoch 229/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 2074.3519 - val_loss: 2557.1642\n",
      "\n",
      "Epoch 00229: val_loss improved from 2557.59542 to 2557.16424, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 230/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2072.9443 - val_loss: 2556.6282\n",
      "\n",
      "Epoch 00230: val_loss improved from 2557.16424 to 2556.62820, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 231/3000\n",
      "2100/2100 [==============================] - 1s 543us/step - loss: 2072.0224 - val_loss: 2556.7467\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 2556.62820\n",
      "Epoch 232/3000\n",
      "2100/2100 [==============================] - 1s 654us/step - loss: 2071.3171 - val_loss: 2555.9142\n",
      "\n",
      "Epoch 00232: val_loss improved from 2556.62820 to 2555.91419, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 233/3000\n",
      "2100/2100 [==============================] - 1s 559us/step - loss: 2070.6566 - val_loss: 2555.5213\n",
      "\n",
      "Epoch 00233: val_loss improved from 2555.91419 to 2555.52135, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 234/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2070.0161 - val_loss: 2555.0425\n",
      "\n",
      "Epoch 00234: val_loss improved from 2555.52135 to 2555.04247, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 235/3000\n",
      "2100/2100 [==============================] - 1s 460us/step - loss: 2069.5987 - val_loss: 2555.1611\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 2555.04247\n",
      "Epoch 236/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 2069.2373 - val_loss: 2555.1350\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 2555.04247\n",
      "Epoch 237/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 2068.7715 - val_loss: 2555.1856\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 2555.04247\n",
      "Epoch 238/3000\n",
      "2100/2100 [==============================] - 2s 714us/step - loss: 2068.3682 - val_loss: 2555.5084\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 2555.04247\n",
      "Epoch 239/3000\n",
      "2100/2100 [==============================] - 1s 674us/step - loss: 2067.9327 - val_loss: 2554.8510\n",
      "\n",
      "Epoch 00239: val_loss improved from 2555.04247 to 2554.85104, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 240/3000\n",
      "2100/2100 [==============================] - 2s 736us/step - loss: 2067.5992 - val_loss: 2554.4258\n",
      "\n",
      "Epoch 00240: val_loss improved from 2554.85104 to 2554.42576, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 241/3000\n",
      "2100/2100 [==============================] - 1s 516us/step - loss: 2067.1409 - val_loss: 2554.2280\n",
      "\n",
      "Epoch 00241: val_loss improved from 2554.42576 to 2554.22800, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 242/3000\n",
      "2100/2100 [==============================] - 2s 731us/step - loss: 2066.8192 - val_loss: 2554.1710\n",
      "\n",
      "Epoch 00242: val_loss improved from 2554.22800 to 2554.17097, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 243/3000\n",
      "2100/2100 [==============================] - 1s 637us/step - loss: 2066.3393 - val_loss: 2553.9440\n",
      "\n",
      "Epoch 00243: val_loss improved from 2554.17097 to 2553.94403, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 244/3000\n",
      "2100/2100 [==============================] - 1s 605us/step - loss: 2065.9997 - val_loss: 2554.0960\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 2553.94403\n",
      "Epoch 245/3000\n",
      "2100/2100 [==============================] - 1s 565us/step - loss: 2065.5841 - val_loss: 2553.7792\n",
      "\n",
      "Epoch 00245: val_loss improved from 2553.94403 to 2553.77922, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 246/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2065.2268 - val_loss: 2553.6689\n",
      "\n",
      "Epoch 00246: val_loss improved from 2553.77922 to 2553.66892, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 247/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2064.8148 - val_loss: 2553.6689\n",
      "\n",
      "Epoch 00247: val_loss improved from 2553.66892 to 2553.66889, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 248/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2064.4292 - val_loss: 2553.9394\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 2553.66889\n",
      "Epoch 249/3000\n",
      "2100/2100 [==============================] - 1s 469us/step - loss: 2064.0504 - val_loss: 2553.8974\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 2553.66889\n",
      "Epoch 250/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2063.6290 - val_loss: 2554.0847\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 2553.66889\n",
      "Epoch 251/3000\n",
      "2100/2100 [==============================] - 1s 498us/step - loss: 2063.3537 - val_loss: 2553.6714\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 2553.66889\n",
      "Epoch 252/3000\n",
      "2100/2100 [==============================] - 1s 689us/step - loss: 2063.0144 - val_loss: 2554.1088\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 2553.66889\n",
      "Epoch 253/3000\n",
      "2100/2100 [==============================] - 1s 674us/step - loss: 2062.6815 - val_loss: 2553.8029\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 2553.66889\n",
      "Epoch 254/3000\n",
      "2100/2100 [==============================] - 1s 507us/step - loss: 2062.2712 - val_loss: 2553.3290\n",
      "\n",
      "Epoch 00254: val_loss improved from 2553.66889 to 2553.32902, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 255/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2061.8058 - val_loss: 2553.2746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00255: val_loss improved from 2553.32902 to 2553.27463, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 256/3000\n",
      "2100/2100 [==============================] - 2s 714us/step - loss: 2061.3895 - val_loss: 2553.3435\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 2553.27463\n",
      "Epoch 257/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2061.0811 - val_loss: 2553.3552\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 2553.27463\n",
      "Epoch 258/3000\n",
      "2100/2100 [==============================] - 1s 590us/step - loss: 2060.8049 - val_loss: 2553.2288\n",
      "\n",
      "Epoch 00258: val_loss improved from 2553.27463 to 2553.22876, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 259/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2060.3819 - val_loss: 2553.3602\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 2553.22876\n",
      "Epoch 260/3000\n",
      "2100/2100 [==============================] - 1s 632us/step - loss: 2060.0542 - val_loss: 2553.4499\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 2553.22876\n",
      "Epoch 261/3000\n",
      "2100/2100 [==============================] - 1s 485us/step - loss: 2059.7195 - val_loss: 2553.2660\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 2553.22876\n",
      "Epoch 262/3000\n",
      "2100/2100 [==============================] - 1s 671us/step - loss: 2059.2904 - val_loss: 2553.2485\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 2553.22876\n",
      "Epoch 263/3000\n",
      "2100/2100 [==============================] - 2s 729us/step - loss: 2058.8226 - val_loss: 2552.7746\n",
      "\n",
      "Epoch 00263: val_loss improved from 2553.22876 to 2552.77459, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 264/3000\n",
      "2100/2100 [==============================] - 1s 624us/step - loss: 2058.6194 - val_loss: 2552.8961\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 2552.77459\n",
      "Epoch 265/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2058.0966 - val_loss: 2553.2990\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 2552.77459\n",
      "Epoch 266/3000\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 2057.8525 - val_loss: 2553.1846\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 2552.77459\n",
      "Epoch 267/3000\n",
      "2100/2100 [==============================] - 2s 731us/step - loss: 2057.5015 - val_loss: 2552.5888\n",
      "\n",
      "Epoch 00267: val_loss improved from 2552.77459 to 2552.58882, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 268/3000\n",
      "2100/2100 [==============================] - 2s 725us/step - loss: 2057.0696 - val_loss: 2552.5384\n",
      "\n",
      "Epoch 00268: val_loss improved from 2552.58882 to 2552.53840, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 269/3000\n",
      "2100/2100 [==============================] - 2s 737us/step - loss: 2056.7036 - val_loss: 2553.0672\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 2552.53840\n",
      "Epoch 270/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2056.5338 - val_loss: 2553.1826\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 2552.53840\n",
      "Epoch 271/3000\n",
      "2100/2100 [==============================] - 1s 624us/step - loss: 2056.0637 - val_loss: 2552.7343\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 2552.53840\n",
      "Epoch 272/3000\n",
      "2100/2100 [==============================] - 1s 590us/step - loss: 2055.6862 - val_loss: 2552.6724\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 2552.53840\n",
      "Epoch 273/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2055.3983 - val_loss: 2552.7227\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 2552.53840\n",
      "Epoch 274/3000\n",
      "2100/2100 [==============================] - 1s 577us/step - loss: 2054.8320 - val_loss: 2552.7490\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 2552.53840\n",
      "Epoch 275/3000\n",
      "2100/2100 [==============================] - 1s 530us/step - loss: 2054.5668 - val_loss: 2552.4863\n",
      "\n",
      "Epoch 00275: val_loss improved from 2552.53840 to 2552.48626, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 276/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2054.0824 - val_loss: 2552.5855\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 2552.48626\n",
      "Epoch 277/3000\n",
      "2100/2100 [==============================] - 1s 464us/step - loss: 2053.7688 - val_loss: 2552.4073\n",
      "\n",
      "Epoch 00277: val_loss improved from 2552.48626 to 2552.40728, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 278/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2053.3136 - val_loss: 2551.9739\n",
      "\n",
      "Epoch 00278: val_loss improved from 2552.40728 to 2551.97391, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 279/3000\n",
      "2100/2100 [==============================] - 1s 661us/step - loss: 2053.0512 - val_loss: 2551.8419\n",
      "\n",
      "Epoch 00279: val_loss improved from 2551.97391 to 2551.84187, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 280/3000\n",
      "2100/2100 [==============================] - 1s 555us/step - loss: 2052.4881 - val_loss: 2551.5744\n",
      "\n",
      "Epoch 00280: val_loss improved from 2551.84187 to 2551.57445, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 281/3000\n",
      "2100/2100 [==============================] - 2s 759us/step - loss: 2052.1640 - val_loss: 2551.5747\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 2551.57445\n",
      "Epoch 282/3000\n",
      "2100/2100 [==============================] - 1s 558us/step - loss: 2051.9003 - val_loss: 2551.9742\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 2551.57445\n",
      "Epoch 283/3000\n",
      "2100/2100 [==============================] - 1s 631us/step - loss: 2051.4365 - val_loss: 2551.5772\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 2551.57445\n",
      "Epoch 284/3000\n",
      "2100/2100 [==============================] - 2s 737us/step - loss: 2051.1311 - val_loss: 2551.3565\n",
      "\n",
      "Epoch 00284: val_loss improved from 2551.57445 to 2551.35649, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 285/3000\n",
      "2100/2100 [==============================] - 1s 648us/step - loss: 2050.7974 - val_loss: 2551.5365\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 2551.35649\n",
      "Epoch 286/3000\n",
      "2100/2100 [==============================] - 1s 526us/step - loss: 2050.3843 - val_loss: 2551.8282\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 2551.35649\n",
      "Epoch 287/3000\n",
      "2100/2100 [==============================] - 1s 653us/step - loss: 2050.0636 - val_loss: 2551.2484\n",
      "\n",
      "Epoch 00287: val_loss improved from 2551.35649 to 2551.24837, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 288/3000\n",
      "2100/2100 [==============================] - 1s 591us/step - loss: 2049.5931 - val_loss: 2551.6390\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 2551.24837\n",
      "Epoch 289/3000\n",
      "2100/2100 [==============================] - 1s 571us/step - loss: 2049.2282 - val_loss: 2552.1058\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 2551.24837\n",
      "Epoch 290/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2049.0086 - val_loss: 2551.4487\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 2551.24837\n",
      "Epoch 291/3000\n",
      "2100/2100 [==============================] - 1s 573us/step - loss: 2048.5807 - val_loss: 2551.4596\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 2551.24837\n",
      "Epoch 292/3000\n",
      "2100/2100 [==============================] - 1s 633us/step - loss: 2048.0117 - val_loss: 2551.2261\n",
      "\n",
      "Epoch 00292: val_loss improved from 2551.24837 to 2551.22610, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 293/3000\n",
      "2100/2100 [==============================] - 1s 646us/step - loss: 2047.7272 - val_loss: 2550.7943\n",
      "\n",
      "Epoch 00293: val_loss improved from 2551.22610 to 2550.79429, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 294/3000\n",
      "2100/2100 [==============================] - 2s 741us/step - loss: 2047.4479 - val_loss: 2551.0728\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 2550.79429\n",
      "Epoch 295/3000\n",
      "2100/2100 [==============================] - 2s 725us/step - loss: 2047.0322 - val_loss: 2551.3412\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 2550.79429\n",
      "Epoch 296/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2046.6897 - val_loss: 2551.1341\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 2550.79429\n",
      "Epoch 297/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 622us/step - loss: 2046.3554 - val_loss: 2551.1581\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 2550.79429\n",
      "Epoch 298/3000\n",
      "2100/2100 [==============================] - 1s 518us/step - loss: 2045.9887 - val_loss: 2551.4235\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 2550.79429\n",
      "Epoch 299/3000\n",
      "2100/2100 [==============================] - 1s 611us/step - loss: 2045.5458 - val_loss: 2551.3962\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 2550.79429\n",
      "Epoch 300/3000\n",
      "2100/2100 [==============================] - 1s 535us/step - loss: 2045.1513 - val_loss: 2550.9713\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 2550.79429\n",
      "Epoch 301/3000\n",
      "2100/2100 [==============================] - 1s 613us/step - loss: 2044.8054 - val_loss: 2551.0372\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 2550.79429\n",
      "Epoch 302/3000\n",
      "2100/2100 [==============================] - 1s 549us/step - loss: 2044.2885 - val_loss: 2550.7095\n",
      "\n",
      "Epoch 00302: val_loss improved from 2550.79429 to 2550.70951, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 303/3000\n",
      "2100/2100 [==============================] - 1s 687us/step - loss: 2043.9835 - val_loss: 2550.8588\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 2550.70951\n",
      "Epoch 304/3000\n",
      "2100/2100 [==============================] - 1s 498us/step - loss: 2043.6392 - val_loss: 2551.0092\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 2550.70951\n",
      "Epoch 305/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2043.2442 - val_loss: 2550.9759\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 2550.70951\n",
      "Epoch 306/3000\n",
      "2100/2100 [==============================] - 1s 648us/step - loss: 2042.9421 - val_loss: 2550.6455\n",
      "\n",
      "Epoch 00306: val_loss improved from 2550.70951 to 2550.64547, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 307/3000\n",
      "2100/2100 [==============================] - 2s 742us/step - loss: 2042.4729 - val_loss: 2550.9579\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 2550.64547\n",
      "Epoch 308/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2042.0902 - val_loss: 2550.4095\n",
      "\n",
      "Epoch 00308: val_loss improved from 2550.64547 to 2550.40953, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 309/3000\n",
      "2100/2100 [==============================] - 2s 736us/step - loss: 2041.8073 - val_loss: 2550.9518\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 2550.40953\n",
      "Epoch 310/3000\n",
      "2100/2100 [==============================] - 2s 728us/step - loss: 2041.5009 - val_loss: 2550.8093\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 2550.40953\n",
      "Epoch 311/3000\n",
      "2100/2100 [==============================] - 1s 486us/step - loss: 2041.0483 - val_loss: 2550.7938\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 2550.40953\n",
      "Epoch 312/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2040.6794 - val_loss: 2550.6812\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 2550.40953\n",
      "Epoch 313/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2040.2637 - val_loss: 2549.9614\n",
      "\n",
      "Epoch 00313: val_loss improved from 2550.40953 to 2549.96142, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 314/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2039.8015 - val_loss: 2550.2589\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 2549.96142\n",
      "Epoch 315/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2039.4386 - val_loss: 2550.5666\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 2549.96142\n",
      "Epoch 316/3000\n",
      "2100/2100 [==============================] - 1s 552us/step - loss: 2038.9791 - val_loss: 2550.5947\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 2549.96142\n",
      "Epoch 317/3000\n",
      "2100/2100 [==============================] - 1s 555us/step - loss: 2038.8621 - val_loss: 2550.3782\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 2549.96142\n",
      "Epoch 318/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2038.3844 - val_loss: 2549.9355\n",
      "\n",
      "Epoch 00318: val_loss improved from 2549.96142 to 2549.93553, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 319/3000\n",
      "2100/2100 [==============================] - 1s 522us/step - loss: 2037.8460 - val_loss: 2550.1500\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 2549.93553\n",
      "Epoch 320/3000\n",
      "2100/2100 [==============================] - 1s 673us/step - loss: 2037.6560 - val_loss: 2549.9164\n",
      "\n",
      "Epoch 00320: val_loss improved from 2549.93553 to 2549.91636, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 321/3000\n",
      "2100/2100 [==============================] - 1s 688us/step - loss: 2037.1960 - val_loss: 2550.1954\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 2549.91636\n",
      "Epoch 322/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2036.8541 - val_loss: 2549.7731\n",
      "\n",
      "Epoch 00322: val_loss improved from 2549.91636 to 2549.77310, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 323/3000\n",
      "2100/2100 [==============================] - 1s 683us/step - loss: 2036.3544 - val_loss: 2549.8021\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 2549.77310\n",
      "Epoch 324/3000\n",
      "2100/2100 [==============================] - 1s 641us/step - loss: 2036.1073 - val_loss: 2550.0906\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 2549.77310\n",
      "Epoch 325/3000\n",
      "2100/2100 [==============================] - 1s 499us/step - loss: 2035.7492 - val_loss: 2549.5459\n",
      "\n",
      "Epoch 00325: val_loss improved from 2549.77310 to 2549.54589, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 326/3000\n",
      "2100/2100 [==============================] - 2s 725us/step - loss: 2035.1724 - val_loss: 2549.8648\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 2549.54589\n",
      "Epoch 327/3000\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2034.7700 - val_loss: 2549.7972\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 2549.54589\n",
      "Epoch 328/3000\n",
      "2100/2100 [==============================] - 2s 749us/step - loss: 2034.4675 - val_loss: 2549.8749\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 2549.54589\n",
      "Epoch 329/3000\n",
      "2100/2100 [==============================] - 1s 692us/step - loss: 2033.9960 - val_loss: 2549.8744\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 2549.54589\n",
      "Epoch 330/3000\n",
      "2100/2100 [==============================] - 1s 477us/step - loss: 2033.5291 - val_loss: 2549.3904\n",
      "\n",
      "Epoch 00330: val_loss improved from 2549.54589 to 2549.39042, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 331/3000\n",
      "2100/2100 [==============================] - 1s 490us/step - loss: 2033.1631 - val_loss: 2549.6108\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 2549.39042\n",
      "Epoch 332/3000\n",
      "2100/2100 [==============================] - 1s 684us/step - loss: 2032.5993 - val_loss: 2549.2114\n",
      "\n",
      "Epoch 00332: val_loss improved from 2549.39042 to 2549.21136, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 333/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2032.2272 - val_loss: 2549.2796\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 2549.21136\n",
      "Epoch 334/3000\n",
      "2100/2100 [==============================] - 2s 720us/step - loss: 2032.0550 - val_loss: 2549.4290\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 2549.21136\n",
      "Epoch 335/3000\n",
      "2100/2100 [==============================] - 1s 459us/step - loss: 2031.6232 - val_loss: 2549.0452\n",
      "\n",
      "Epoch 00335: val_loss improved from 2549.21136 to 2549.04523, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 336/3000\n",
      "2100/2100 [==============================] - 2s 739us/step - loss: 2031.3511 - val_loss: 2549.1740\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 2549.04523\n",
      "Epoch 337/3000\n",
      "2100/2100 [==============================] - 1s 535us/step - loss: 2030.8571 - val_loss: 2549.4230\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 2549.04523\n",
      "Epoch 338/3000\n",
      "2100/2100 [==============================] - 1s 619us/step - loss: 2030.2132 - val_loss: 2549.9535\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 2549.04523\n",
      "Epoch 339/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2029.9390 - val_loss: 2548.9124\n",
      "\n",
      "Epoch 00339: val_loss improved from 2549.04523 to 2548.91241, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/3000\n",
      "2100/2100 [==============================] - 1s 655us/step - loss: 2029.5803 - val_loss: 2549.1053\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 2548.91241\n",
      "Epoch 341/3000\n",
      "2100/2100 [==============================] - 1s 511us/step - loss: 2029.1566 - val_loss: 2549.4482\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 2548.91241\n",
      "Epoch 342/3000\n",
      "2100/2100 [==============================] - 1s 637us/step - loss: 2028.7078 - val_loss: 2549.0333\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 2548.91241\n",
      "Epoch 343/3000\n",
      "2100/2100 [==============================] - 1s 470us/step - loss: 2028.3767 - val_loss: 2549.1473\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 2548.91241\n",
      "Epoch 344/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2028.0120 - val_loss: 2548.6599\n",
      "\n",
      "Epoch 00344: val_loss improved from 2548.91241 to 2548.65986, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 345/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2027.3789 - val_loss: 2548.9920\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 2548.65986\n",
      "Epoch 346/3000\n",
      "2100/2100 [==============================] - 1s 516us/step - loss: 2027.0777 - val_loss: 2549.0726\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 2548.65986\n",
      "Epoch 347/3000\n",
      "2100/2100 [==============================] - 1s 681us/step - loss: 2026.7257 - val_loss: 2548.3299\n",
      "\n",
      "Epoch 00347: val_loss improved from 2548.65986 to 2548.32986, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 348/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 2026.3620 - val_loss: 2548.5991\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 2548.32986\n",
      "Epoch 349/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2025.9633 - val_loss: 2548.8349\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 2548.32986\n",
      "Epoch 350/3000\n",
      "2100/2100 [==============================] - 1s 565us/step - loss: 2025.6471 - val_loss: 2548.3652\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 2548.32986\n",
      "Epoch 351/3000\n",
      "2100/2100 [==============================] - 1s 643us/step - loss: 2025.1655 - val_loss: 2548.8741\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 2548.32986\n",
      "Epoch 352/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 2024.6873 - val_loss: 2548.3403\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 2548.32986\n",
      "Epoch 353/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2024.1512 - val_loss: 2548.8881\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 2548.32986\n",
      "Epoch 354/3000\n",
      "2100/2100 [==============================] - 2s 751us/step - loss: 2023.8176 - val_loss: 2548.4251\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 2548.32986\n",
      "Epoch 355/3000\n",
      "2100/2100 [==============================] - 1s 530us/step - loss: 2023.5480 - val_loss: 2548.3355\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 2548.32986\n",
      "Epoch 356/3000\n",
      "2100/2100 [==============================] - 1s 634us/step - loss: 2022.9994 - val_loss: 2548.0941\n",
      "\n",
      "Epoch 00356: val_loss improved from 2548.32986 to 2548.09408, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 357/3000\n",
      "2100/2100 [==============================] - 1s 567us/step - loss: 2022.5053 - val_loss: 2548.4401\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 2548.09408\n",
      "Epoch 358/3000\n",
      "2100/2100 [==============================] - 2s 734us/step - loss: 2022.1434 - val_loss: 2547.7719\n",
      "\n",
      "Epoch 00358: val_loss improved from 2548.09408 to 2547.77186, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 359/3000\n",
      "2100/2100 [==============================] - 1s 646us/step - loss: 2021.7154 - val_loss: 2548.3442\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 2547.77186\n",
      "Epoch 360/3000\n",
      "2100/2100 [==============================] - 1s 467us/step - loss: 2021.4290 - val_loss: 2547.3649\n",
      "\n",
      "Epoch 00360: val_loss improved from 2547.77186 to 2547.36488, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 361/3000\n",
      "2100/2100 [==============================] - 1s 423us/step - loss: 2020.9272 - val_loss: 2548.1673\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 2547.36488\n",
      "Epoch 362/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2020.6271 - val_loss: 2548.4264\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 2547.36488\n",
      "Epoch 363/3000\n",
      "2100/2100 [==============================] - 1s 424us/step - loss: 2020.3815 - val_loss: 2547.3553\n",
      "\n",
      "Epoch 00363: val_loss improved from 2547.36488 to 2547.35526, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 364/3000\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 2019.9370 - val_loss: 2547.9309\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 2547.35526\n",
      "Epoch 365/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2019.5823 - val_loss: 2547.6175\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 2547.35526\n",
      "Epoch 366/3000\n",
      "2100/2100 [==============================] - 1s 427us/step - loss: 2019.0744 - val_loss: 2547.7290\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 2547.35526\n",
      "Epoch 367/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2018.4692 - val_loss: 2547.4374\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 2547.35526\n",
      "Epoch 368/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2018.0551 - val_loss: 2547.8183\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 2547.35526\n",
      "Epoch 369/3000\n",
      "2100/2100 [==============================] - 1s 413us/step - loss: 2017.4999 - val_loss: 2547.6500\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 2547.35526\n",
      "Epoch 370/3000\n",
      "2100/2100 [==============================] - 2s 731us/step - loss: 2017.1872 - val_loss: 2547.3766\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 2547.35526\n",
      "Epoch 371/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2016.6688 - val_loss: 2547.3249\n",
      "\n",
      "Epoch 00371: val_loss improved from 2547.35526 to 2547.32485, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 372/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2016.3830 - val_loss: 2546.8642\n",
      "\n",
      "Epoch 00372: val_loss improved from 2547.32485 to 2546.86424, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 373/3000\n",
      "2100/2100 [==============================] - 1s 464us/step - loss: 2016.2029 - val_loss: 2547.3942\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 2546.86424\n",
      "Epoch 374/3000\n",
      "2100/2100 [==============================] - 1s 654us/step - loss: 2015.5915 - val_loss: 2546.6948\n",
      "\n",
      "Epoch 00374: val_loss improved from 2546.86424 to 2546.69479, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 375/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2015.1618 - val_loss: 2546.9013\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 2546.69479\n",
      "Epoch 376/3000\n",
      "2100/2100 [==============================] - 1s 594us/step - loss: 2014.6406 - val_loss: 2546.7137\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 2546.69479\n",
      "Epoch 377/3000\n",
      "2100/2100 [==============================] - 1s 559us/step - loss: 2014.0962 - val_loss: 2546.9470\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 2546.69479\n",
      "Epoch 378/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2013.5856 - val_loss: 2546.5703\n",
      "\n",
      "Epoch 00378: val_loss improved from 2546.69479 to 2546.57028, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 379/3000\n",
      "2100/2100 [==============================] - 1s 640us/step - loss: 2013.2563 - val_loss: 2546.9795\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 2546.57028\n",
      "Epoch 380/3000\n",
      "2100/2100 [==============================] - 1s 568us/step - loss: 2012.9448 - val_loss: 2546.7963\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 2546.57028\n",
      "Epoch 381/3000\n",
      "2100/2100 [==============================] - 1s 536us/step - loss: 2012.5571 - val_loss: 2545.7782\n",
      "\n",
      "Epoch 00381: val_loss improved from 2546.57028 to 2545.77823, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 382/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2012.1712 - val_loss: 2546.9339\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 2545.77823\n",
      "Epoch 383/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 706us/step - loss: 2011.7684 - val_loss: 2546.1435\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 2545.77823\n",
      "Epoch 384/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2011.3073 - val_loss: 2546.9961\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 2545.77823\n",
      "Epoch 385/3000\n",
      "2100/2100 [==============================] - 1s 611us/step - loss: 2010.7730 - val_loss: 2546.1036\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 2545.77823\n",
      "Epoch 386/3000\n",
      "2100/2100 [==============================] - 1s 569us/step - loss: 2010.4384 - val_loss: 2546.7794\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 2545.77823\n",
      "Epoch 387/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2009.8964 - val_loss: 2546.4935\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 2545.77823\n",
      "Epoch 388/3000\n",
      "2100/2100 [==============================] - 1s 572us/step - loss: 2009.4503 - val_loss: 2546.0480\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 2545.77823\n",
      "Epoch 389/3000\n",
      "2100/2100 [==============================] - 1s 610us/step - loss: 2009.4206 - val_loss: 2546.5969\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 2545.77823\n",
      "Epoch 390/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2008.7559 - val_loss: 2545.9237\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 2545.77823\n",
      "Epoch 391/3000\n",
      "2100/2100 [==============================] - 1s 581us/step - loss: 2008.2682 - val_loss: 2546.4817\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 2545.77823\n",
      "\n",
      "Epoch 00391: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 392/3000\n",
      "2100/2100 [==============================] - 1s 605us/step - loss: 2006.1257 - val_loss: 2545.4059\n",
      "\n",
      "Epoch 00392: val_loss improved from 2545.77823 to 2545.40590, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 393/3000\n",
      "2100/2100 [==============================] - 1s 570us/step - loss: 2005.5469 - val_loss: 2545.1372\n",
      "\n",
      "Epoch 00393: val_loss improved from 2545.40590 to 2545.13721, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 394/3000\n",
      "2100/2100 [==============================] - 1s 671us/step - loss: 2005.2124 - val_loss: 2545.0858\n",
      "\n",
      "Epoch 00394: val_loss improved from 2545.13721 to 2545.08585, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 395/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2005.0364 - val_loss: 2544.8985\n",
      "\n",
      "Epoch 00395: val_loss improved from 2545.08585 to 2544.89854, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 396/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2004.9385 - val_loss: 2544.9508\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 2544.89854\n",
      "Epoch 397/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2004.8451 - val_loss: 2544.9211\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 2544.89854\n",
      "Epoch 398/3000\n",
      "2100/2100 [==============================] - 1s 477us/step - loss: 2004.7889 - val_loss: 2544.9766\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 2544.89854\n",
      "Epoch 399/3000\n",
      "2100/2100 [==============================] - 1s 656us/step - loss: 2004.7263 - val_loss: 2544.9154\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 2544.89854\n",
      "Epoch 400/3000\n",
      "2100/2100 [==============================] - 1s 573us/step - loss: 2004.6667 - val_loss: 2544.9291\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 2544.89854\n",
      "Epoch 401/3000\n",
      "2100/2100 [==============================] - 1s 582us/step - loss: 2004.6025 - val_loss: 2544.8847\n",
      "\n",
      "Epoch 00401: val_loss improved from 2544.89854 to 2544.88471, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 402/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2004.5583 - val_loss: 2544.9455\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 2544.88471\n",
      "Epoch 403/3000\n",
      "2100/2100 [==============================] - 1s 641us/step - loss: 2004.5146 - val_loss: 2544.9116\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 2544.88471\n",
      "Epoch 404/3000\n",
      "2100/2100 [==============================] - 1s 497us/step - loss: 2004.4707 - val_loss: 2544.8207\n",
      "\n",
      "Epoch 00404: val_loss improved from 2544.88471 to 2544.82075, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 405/3000\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 2004.4226 - val_loss: 2544.8208\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 2544.82075\n",
      "Epoch 406/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2004.3643 - val_loss: 2544.8521\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 2544.82075\n",
      "Epoch 407/3000\n",
      "2100/2100 [==============================] - 1s 620us/step - loss: 2004.3089 - val_loss: 2544.9428\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 2544.82075\n",
      "Epoch 408/3000\n",
      "2100/2100 [==============================] - 1s 603us/step - loss: 2004.2439 - val_loss: 2544.7498\n",
      "\n",
      "Epoch 00408: val_loss improved from 2544.82075 to 2544.74977, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 409/3000\n",
      "2100/2100 [==============================] - 1s 636us/step - loss: 2004.2069 - val_loss: 2544.8180\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 2544.74977\n",
      "Epoch 410/3000\n",
      "2100/2100 [==============================] - 1s 566us/step - loss: 2004.1547 - val_loss: 2544.7901\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 2544.74977\n",
      "Epoch 411/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2004.1080 - val_loss: 2544.8039\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 2544.74977\n",
      "Epoch 412/3000\n",
      "2100/2100 [==============================] - 2s 722us/step - loss: 2004.0565 - val_loss: 2544.8206\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 2544.74977\n",
      "Epoch 413/3000\n",
      "2100/2100 [==============================] - 2s 782us/step - loss: 2004.0115 - val_loss: 2544.8796\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 2544.74977\n",
      "Epoch 414/3000\n",
      "2100/2100 [==============================] - 2s 742us/step - loss: 2003.9777 - val_loss: 2544.7373\n",
      "\n",
      "Epoch 00414: val_loss improved from 2544.74977 to 2544.73728, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 415/3000\n",
      "2100/2100 [==============================] - 1s 478us/step - loss: 2003.9076 - val_loss: 2544.8271\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 2544.73728\n",
      "Epoch 416/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2003.8415 - val_loss: 2544.7999\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 2544.73728\n",
      "Epoch 417/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2003.8186 - val_loss: 2544.7123\n",
      "\n",
      "Epoch 00417: val_loss improved from 2544.73728 to 2544.71229, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 418/3000\n",
      "2100/2100 [==============================] - 2s 728us/step - loss: 2003.7668 - val_loss: 2544.7289\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 2544.71229\n",
      "Epoch 419/3000\n",
      "2100/2100 [==============================] - 1s 697us/step - loss: 2003.7050 - val_loss: 2544.8028\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 2544.71229\n",
      "Epoch 420/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2003.6551 - val_loss: 2544.7367\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 2544.71229\n",
      "Epoch 421/3000\n",
      "2100/2100 [==============================] - 1s 455us/step - loss: 2003.5934 - val_loss: 2544.7560\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 2544.71229\n",
      "Epoch 422/3000\n",
      "2100/2100 [==============================] - 1s 681us/step - loss: 2003.5647 - val_loss: 2544.7456\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 2544.71229\n",
      "Epoch 423/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2003.5098 - val_loss: 2544.7183\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 2544.71229\n",
      "Epoch 424/3000\n",
      "2100/2100 [==============================] - 1s 474us/step - loss: 2003.4618 - val_loss: 2544.7705\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 2544.71229\n",
      "Epoch 425/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2003.4121 - val_loss: 2544.7317\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 2544.71229\n",
      "Epoch 426/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 714us/step - loss: 2003.3411 - val_loss: 2544.6717\n",
      "\n",
      "Epoch 00426: val_loss improved from 2544.71229 to 2544.67168, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 427/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2003.3025 - val_loss: 2544.6750\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 2544.67168\n",
      "Epoch 428/3000\n",
      "2100/2100 [==============================] - 1s 574us/step - loss: 2003.2414 - val_loss: 2544.7145\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 2544.67168\n",
      "Epoch 429/3000\n",
      "2100/2100 [==============================] - 1s 584us/step - loss: 2003.1947 - val_loss: 2544.6742\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 2544.67168\n",
      "Epoch 430/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2003.1486 - val_loss: 2544.6972\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 2544.67168\n",
      "Epoch 431/3000\n",
      "2100/2100 [==============================] - 1s 546us/step - loss: 2003.1048 - val_loss: 2544.6754\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 2544.67168\n",
      "Epoch 432/3000\n",
      "2100/2100 [==============================] - 1s 588us/step - loss: 2003.0386 - val_loss: 2544.6974\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 2544.67168\n",
      "Epoch 433/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2002.9855 - val_loss: 2544.6533\n",
      "\n",
      "Epoch 00433: val_loss improved from 2544.67168 to 2544.65328, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 434/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2002.9492 - val_loss: 2544.6742\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 2544.65328\n",
      "Epoch 435/3000\n",
      "2100/2100 [==============================] - 1s 669us/step - loss: 2002.8985 - val_loss: 2544.6569\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 2544.65328\n",
      "Epoch 436/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2002.8403 - val_loss: 2544.6577\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 2544.65328\n",
      "Epoch 437/3000\n",
      "2100/2100 [==============================] - 1s 504us/step - loss: 2002.7879 - val_loss: 2544.6952\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 2544.65328\n",
      "Epoch 438/3000\n",
      "2100/2100 [==============================] - 1s 631us/step - loss: 2002.7390 - val_loss: 2544.6348\n",
      "\n",
      "Epoch 00438: val_loss improved from 2544.65328 to 2544.63485, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 439/3000\n",
      "2100/2100 [==============================] - 1s 683us/step - loss: 2002.6647 - val_loss: 2544.6665\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 2544.63485\n",
      "Epoch 440/3000\n",
      "2100/2100 [==============================] - 1s 431us/step - loss: 2002.6320 - val_loss: 2544.6418\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 2544.63485\n",
      "Epoch 441/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2002.5870 - val_loss: 2544.5617\n",
      "\n",
      "Epoch 00441: val_loss improved from 2544.63485 to 2544.56167, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 442/3000\n",
      "2100/2100 [==============================] - 1s 457us/step - loss: 2002.5144 - val_loss: 2544.6693\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 2544.56167\n",
      "Epoch 443/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2002.4758 - val_loss: 2544.6794\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 2544.56167\n",
      "Epoch 444/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 2002.4200 - val_loss: 2544.6187\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 2544.56167\n",
      "Epoch 445/3000\n",
      "2100/2100 [==============================] - 1s 458us/step - loss: 2002.3798 - val_loss: 2544.5851\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 2544.56167\n",
      "Epoch 446/3000\n",
      "2100/2100 [==============================] - 1s 697us/step - loss: 2002.3264 - val_loss: 2544.7007\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 2544.56167\n",
      "Epoch 447/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2002.2592 - val_loss: 2544.5690\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 2544.56167\n",
      "Epoch 448/3000\n",
      "2100/2100 [==============================] - 1s 489us/step - loss: 2002.1968 - val_loss: 2544.5999\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 2544.56167\n",
      "Epoch 449/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2002.1548 - val_loss: 2544.5898\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 2544.56167\n",
      "Epoch 450/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2002.1295 - val_loss: 2544.6188\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 2544.56167\n",
      "Epoch 451/3000\n",
      "2100/2100 [==============================] - 1s 454us/step - loss: 2002.0484 - val_loss: 2544.6003\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 2544.56167\n",
      "\n",
      "Epoch 00451: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 452/3000\n",
      "2100/2100 [==============================] - 1s 694us/step - loss: 2001.8453 - val_loss: 2544.5991\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 2544.56167\n",
      "Epoch 453/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.8269 - val_loss: 2544.5826\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 2544.56167\n",
      "Epoch 454/3000\n",
      "2100/2100 [==============================] - 1s 475us/step - loss: 2001.8101 - val_loss: 2544.5822\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 2544.56167\n",
      "Epoch 455/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2001.7989 - val_loss: 2544.5672\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 2544.56167\n",
      "Epoch 456/3000\n",
      "2100/2100 [==============================] - 1s 672us/step - loss: 2001.7877 - val_loss: 2544.5524\n",
      "\n",
      "Epoch 00456: val_loss improved from 2544.56167 to 2544.55243, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 457/3000\n",
      "2100/2100 [==============================] - 2s 725us/step - loss: 2001.7774 - val_loss: 2544.5560\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 2544.55243\n",
      "Epoch 458/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2001.7684 - val_loss: 2544.5404\n",
      "\n",
      "Epoch 00458: val_loss improved from 2544.55243 to 2544.54037, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 459/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.7595 - val_loss: 2544.5306\n",
      "\n",
      "Epoch 00459: val_loss improved from 2544.54037 to 2544.53059, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 460/3000\n",
      "2100/2100 [==============================] - 2s 755us/step - loss: 2001.7508 - val_loss: 2544.5334\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 2544.53059\n",
      "Epoch 461/3000\n",
      "2100/2100 [==============================] - 1s 597us/step - loss: 2001.7433 - val_loss: 2544.5025\n",
      "\n",
      "Epoch 00461: val_loss improved from 2544.53059 to 2544.50255, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 462/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2001.7344 - val_loss: 2544.5077\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 2544.50255\n",
      "Epoch 463/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2001.7262 - val_loss: 2544.5254\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 2544.50255\n",
      "Epoch 464/3000\n",
      "2100/2100 [==============================] - 1s 658us/step - loss: 2001.7212 - val_loss: 2544.5086\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 2544.50255\n",
      "Epoch 465/3000\n",
      "2100/2100 [==============================] - 1s 523us/step - loss: 2001.7140 - val_loss: 2544.5253\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 2544.50255\n",
      "Epoch 466/3000\n",
      "2100/2100 [==============================] - 1s 616us/step - loss: 2001.7079 - val_loss: 2544.5414\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 2544.50255\n",
      "Epoch 467/3000\n",
      "2100/2100 [==============================] - 1s 556us/step - loss: 2001.6987 - val_loss: 2544.5330\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 2544.50255\n",
      "Epoch 468/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2001.6913 - val_loss: 2544.5345\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 2544.50255\n",
      "Epoch 469/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 2001.6837 - val_loss: 2544.5425\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 2544.50255\n",
      "Epoch 470/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 2s 745us/step - loss: 2001.6780 - val_loss: 2544.5345\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 2544.50255\n",
      "Epoch 471/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 2001.6713 - val_loss: 2544.5272\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00471: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 472/3000\n",
      "2100/2100 [==============================] - 1s 624us/step - loss: 2001.6514 - val_loss: 2544.5277\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 2544.50255\n",
      "Epoch 473/3000\n",
      "2100/2100 [==============================] - 1s 502us/step - loss: 2001.6504 - val_loss: 2544.5299\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 2544.50255\n",
      "Epoch 474/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.6496 - val_loss: 2544.5311\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 2544.50255\n",
      "Epoch 475/3000\n",
      "2100/2100 [==============================] - 2s 747us/step - loss: 2001.6484 - val_loss: 2544.5310\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 2544.50255\n",
      "Epoch 476/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2001.6476 - val_loss: 2544.5275\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 2544.50255\n",
      "Epoch 477/3000\n",
      "2100/2100 [==============================] - 1s 687us/step - loss: 2001.6467 - val_loss: 2544.5258\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 2544.50255\n",
      "Epoch 478/3000\n",
      "2100/2100 [==============================] - 1s 487us/step - loss: 2001.6458 - val_loss: 2544.5243\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 2544.50255\n",
      "Epoch 479/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.6451 - val_loss: 2544.5242\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 2544.50255\n",
      "Epoch 480/3000\n",
      "2100/2100 [==============================] - 1s 684us/step - loss: 2001.6441 - val_loss: 2544.5236\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 2544.50255\n",
      "Epoch 481/3000\n",
      "2100/2100 [==============================] - 1s 486us/step - loss: 2001.6433 - val_loss: 2544.5243\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00481: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 482/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2001.6424 - val_loss: 2544.5252\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 2544.50255\n",
      "Epoch 483/3000\n",
      "2100/2100 [==============================] - 1s 674us/step - loss: 2001.6418 - val_loss: 2544.5250\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 2544.50255\n",
      "Epoch 484/3000\n",
      "2100/2100 [==============================] - 1s 508us/step - loss: 2001.6410 - val_loss: 2544.5251\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 2544.50255\n",
      "Epoch 485/3000\n",
      "2100/2100 [==============================] - 2s 745us/step - loss: 2001.6401 - val_loss: 2544.5270\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 2544.50255\n",
      "Epoch 486/3000\n",
      "2100/2100 [==============================] - 2s 741us/step - loss: 2001.6394 - val_loss: 2544.5265\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 2544.50255\n",
      "Epoch 487/3000\n",
      "2100/2100 [==============================] - 1s 611us/step - loss: 2001.6388 - val_loss: 2544.5264\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 2544.50255\n",
      "Epoch 488/3000\n",
      "2100/2100 [==============================] - 1s 559us/step - loss: 2001.6380 - val_loss: 2544.5263\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 2544.50255\n",
      "Epoch 489/3000\n",
      "2100/2100 [==============================] - 2s 736us/step - loss: 2001.6371 - val_loss: 2544.5258\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 2544.50255\n",
      "Epoch 490/3000\n",
      "2100/2100 [==============================] - 1s 580us/step - loss: 2001.6365 - val_loss: 2544.5261\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 2544.50255\n",
      "Epoch 491/3000\n",
      "2100/2100 [==============================] - 1s 524us/step - loss: 2001.6357 - val_loss: 2544.5249\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00491: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 492/3000\n",
      "2100/2100 [==============================] - 1s 590us/step - loss: 2001.6349 - val_loss: 2544.5233\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 2544.50255\n",
      "Epoch 493/3000\n",
      "2100/2100 [==============================] - 1s 538us/step - loss: 2001.6342 - val_loss: 2544.5208\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 2544.50255\n",
      "Epoch 494/3000\n",
      "2100/2100 [==============================] - 1s 604us/step - loss: 2001.6335 - val_loss: 2544.5229\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 2544.50255\n",
      "Epoch 495/3000\n",
      "2100/2100 [==============================] - 1s 558us/step - loss: 2001.6327 - val_loss: 2544.5240\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 2544.50255\n",
      "Epoch 496/3000\n",
      "2100/2100 [==============================] - 1s 574us/step - loss: 2001.6320 - val_loss: 2544.5235\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 2544.50255\n",
      "Epoch 497/3000\n",
      "2100/2100 [==============================] - 1s 661us/step - loss: 2001.6312 - val_loss: 2544.5226\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 2544.50255\n",
      "Epoch 498/3000\n",
      "2100/2100 [==============================] - 2s 743us/step - loss: 2001.6305 - val_loss: 2544.5219\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 2544.50255\n",
      "Epoch 499/3000\n",
      "2100/2100 [==============================] - 1s 504us/step - loss: 2001.6298 - val_loss: 2544.5233\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 2544.50255\n",
      "Epoch 500/3000\n",
      "2100/2100 [==============================] - 1s 645us/step - loss: 2001.6289 - val_loss: 2544.5218\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 2544.50255\n",
      "Epoch 501/3000\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2001.6283 - val_loss: 2544.5194\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00501: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 502/3000\n",
      "2100/2100 [==============================] - 1s 496us/step - loss: 2001.6276 - val_loss: 2544.5176\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 2544.50255\n",
      "Epoch 503/3000\n",
      "2100/2100 [==============================] - 1s 599us/step - loss: 2001.6269 - val_loss: 2544.5165\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 2544.50255\n",
      "Epoch 504/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.6261 - val_loss: 2544.5160\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 2544.50255\n",
      "Epoch 505/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2001.6255 - val_loss: 2544.5161\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 2544.50255\n",
      "Epoch 506/3000\n",
      "2100/2100 [==============================] - 2s 728us/step - loss: 2001.6247 - val_loss: 2544.5154\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 2544.50255\n",
      "Epoch 507/3000\n",
      "2100/2100 [==============================] - 1s 632us/step - loss: 2001.6238 - val_loss: 2544.5138\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 2544.50255\n",
      "Epoch 508/3000\n",
      "2100/2100 [==============================] - 1s 520us/step - loss: 2001.6234 - val_loss: 2544.5132\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 2544.50255\n",
      "Epoch 509/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.6225 - val_loss: 2544.5140\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 2544.50255\n",
      "Epoch 510/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2001.6219 - val_loss: 2544.5148\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 2544.50255\n",
      "Epoch 511/3000\n",
      "2100/2100 [==============================] - 1s 685us/step - loss: 2001.6210 - val_loss: 2544.5148\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00511: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 512/3000\n",
      "2100/2100 [==============================] - 1s 646us/step - loss: 2001.6202 - val_loss: 2544.5145\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 2544.50255\n",
      "Epoch 513/3000\n",
      "2100/2100 [==============================] - 1s 524us/step - loss: 2001.6195 - val_loss: 2544.5164\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 2544.50255\n",
      "Epoch 514/3000\n",
      "2100/2100 [==============================] - 1s 695us/step - loss: 2001.6189 - val_loss: 2544.5177\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 2544.50255\n",
      "Epoch 515/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.6180 - val_loss: 2544.5183\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 2544.50255\n",
      "Epoch 516/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.6174 - val_loss: 2544.5168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00516: val_loss did not improve from 2544.50255\n",
      "Epoch 517/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.6168 - val_loss: 2544.5160\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 2544.50255\n",
      "Epoch 518/3000\n",
      "2100/2100 [==============================] - 2s 744us/step - loss: 2001.6159 - val_loss: 2544.5170\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 2544.50255\n",
      "Epoch 519/3000\n",
      "2100/2100 [==============================] - 2s 720us/step - loss: 2001.6152 - val_loss: 2544.5187\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 2544.50255\n",
      "Epoch 520/3000\n",
      "2100/2100 [==============================] - 1s 695us/step - loss: 2001.6144 - val_loss: 2544.5191\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 2544.50255\n",
      "Epoch 521/3000\n",
      "2100/2100 [==============================] - 1s 420us/step - loss: 2001.6140 - val_loss: 2544.5169\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00521: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 522/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2001.6129 - val_loss: 2544.5155\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 2544.50255\n",
      "Epoch 523/3000\n",
      "2100/2100 [==============================] - 2s 746us/step - loss: 2001.6123 - val_loss: 2544.5161\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 2544.50255\n",
      "Epoch 524/3000\n",
      "2100/2100 [==============================] - 2s 722us/step - loss: 2001.6116 - val_loss: 2544.5172\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 2544.50255\n",
      "Epoch 525/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.6109 - val_loss: 2544.5179\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 2544.50255\n",
      "Epoch 526/3000\n",
      "2100/2100 [==============================] - 1s 484us/step - loss: 2001.6102 - val_loss: 2544.5176\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 2544.50255\n",
      "Epoch 527/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.6095 - val_loss: 2544.5181\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 2544.50255\n",
      "Epoch 528/3000\n",
      "2100/2100 [==============================] - 1s 685us/step - loss: 2001.6087 - val_loss: 2544.5202\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 2544.50255\n",
      "Epoch 529/3000\n",
      "2100/2100 [==============================] - 1s 467us/step - loss: 2001.6079 - val_loss: 2544.5195\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 2544.50255\n",
      "Epoch 530/3000\n",
      "2100/2100 [==============================] - 1s 678us/step - loss: 2001.6075 - val_loss: 2544.5220\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 2544.50255\n",
      "Epoch 531/3000\n",
      "2100/2100 [==============================] - 1s 456us/step - loss: 2001.6066 - val_loss: 2544.5238\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00531: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 532/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.6059 - val_loss: 2544.5250\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 2544.50255\n",
      "Epoch 533/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2001.6052 - val_loss: 2544.5224\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 2544.50255\n",
      "Epoch 534/3000\n",
      "2100/2100 [==============================] - 1s 683us/step - loss: 2001.6044 - val_loss: 2544.5213\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 2544.50255\n",
      "Epoch 535/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.6036 - val_loss: 2544.5207\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 2544.50255\n",
      "Epoch 536/3000\n",
      "2100/2100 [==============================] - 1s 513us/step - loss: 2001.6029 - val_loss: 2544.5211\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 2544.50255\n",
      "Epoch 537/3000\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2001.6020 - val_loss: 2544.5207\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 2544.50255\n",
      "Epoch 538/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 2001.6014 - val_loss: 2544.5184\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 2544.50255\n",
      "Epoch 539/3000\n",
      "2100/2100 [==============================] - 1s 678us/step - loss: 2001.6007 - val_loss: 2544.5162\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 2544.50255\n",
      "Epoch 540/3000\n",
      "2100/2100 [==============================] - 2s 729us/step - loss: 2001.5998 - val_loss: 2544.5171\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 2544.50255\n",
      "Epoch 541/3000\n",
      "2100/2100 [==============================] - 1s 480us/step - loss: 2001.5991 - val_loss: 2544.5169\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00541: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 542/3000\n",
      "2100/2100 [==============================] - 2s 725us/step - loss: 2001.5984 - val_loss: 2544.5192\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 2544.50255\n",
      "Epoch 543/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2001.5976 - val_loss: 2544.5200\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 2544.50255\n",
      "Epoch 544/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.5967 - val_loss: 2544.5201\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 2544.50255\n",
      "Epoch 545/3000\n",
      "2100/2100 [==============================] - 1s 690us/step - loss: 2001.5963 - val_loss: 2544.5179\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 2544.50255\n",
      "Epoch 546/3000\n",
      "2100/2100 [==============================] - 2s 729us/step - loss: 2001.5953 - val_loss: 2544.5160\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 2544.50255\n",
      "Epoch 547/3000\n",
      "2100/2100 [==============================] - 1s 533us/step - loss: 2001.5946 - val_loss: 2544.5151\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 2544.50255\n",
      "Epoch 548/3000\n",
      "2100/2100 [==============================] - 1s 632us/step - loss: 2001.5938 - val_loss: 2544.5158\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 2544.50255\n",
      "Epoch 549/3000\n",
      "2100/2100 [==============================] - 1s 703us/step - loss: 2001.5931 - val_loss: 2544.5144\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 2544.50255\n",
      "Epoch 550/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.5923 - val_loss: 2544.5108\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 2544.50255\n",
      "Epoch 551/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.5915 - val_loss: 2544.5087\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00551: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 552/3000\n",
      "2100/2100 [==============================] - 1s 572us/step - loss: 2001.5908 - val_loss: 2544.5097\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 2544.50255\n",
      "Epoch 553/3000\n",
      "2100/2100 [==============================] - 1s 577us/step - loss: 2001.5899 - val_loss: 2544.5144\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 2544.50255\n",
      "Epoch 554/3000\n",
      "2100/2100 [==============================] - 1s 684us/step - loss: 2001.5890 - val_loss: 2544.5172\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 2544.50255\n",
      "Epoch 555/3000\n",
      "2100/2100 [==============================] - 1s 582us/step - loss: 2001.5885 - val_loss: 2544.5143\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 2544.50255\n",
      "Epoch 556/3000\n",
      "2100/2100 [==============================] - 1s 581us/step - loss: 2001.5876 - val_loss: 2544.5119\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 2544.50255\n",
      "Epoch 557/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.5869 - val_loss: 2544.5097\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 2544.50255\n",
      "Epoch 558/3000\n",
      "2100/2100 [==============================] - 1s 572us/step - loss: 2001.5861 - val_loss: 2544.5103\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 2544.50255\n",
      "Epoch 559/3000\n",
      "2100/2100 [==============================] - 1s 635us/step - loss: 2001.5853 - val_loss: 2544.5112\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 2544.50255\n",
      "Epoch 560/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2001.5844 - val_loss: 2544.5124\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 2544.50255\n",
      "Epoch 561/3000\n",
      "2100/2100 [==============================] - 1s 543us/step - loss: 2001.5838 - val_loss: 2544.5142\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00561: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 562/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2001.5828 - val_loss: 2544.5139\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 2544.50255\n",
      "Epoch 563/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 671us/step - loss: 2001.5821 - val_loss: 2544.5140\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 2544.50255\n",
      "Epoch 564/3000\n",
      "2100/2100 [==============================] - 2s 739us/step - loss: 2001.5813 - val_loss: 2544.5152\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 2544.50255\n",
      "Epoch 565/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2001.5807 - val_loss: 2544.5155\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 2544.50255\n",
      "Epoch 566/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2001.5799 - val_loss: 2544.5138\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 2544.50255\n",
      "Epoch 567/3000\n",
      "2100/2100 [==============================] - 1s 677us/step - loss: 2001.5792 - val_loss: 2544.5144\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 2544.50255\n",
      "Epoch 568/3000\n",
      "2100/2100 [==============================] - 1s 592us/step - loss: 2001.5784 - val_loss: 2544.5157\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 2544.50255\n",
      "Epoch 569/3000\n",
      "2100/2100 [==============================] - 1s 553us/step - loss: 2001.5775 - val_loss: 2544.5146\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 2544.50255\n",
      "Epoch 570/3000\n",
      "2100/2100 [==============================] - 1s 588us/step - loss: 2001.5768 - val_loss: 2544.5116\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 2544.50255\n",
      "Epoch 571/3000\n",
      "2100/2100 [==============================] - 1s 597us/step - loss: 2001.5759 - val_loss: 2544.5100\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00571: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 572/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.5751 - val_loss: 2544.5100\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 2544.50255\n",
      "Epoch 573/3000\n",
      "2100/2100 [==============================] - 1s 556us/step - loss: 2001.5743 - val_loss: 2544.5119\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 2544.50255\n",
      "Epoch 574/3000\n",
      "2100/2100 [==============================] - 1s 642us/step - loss: 2001.5736 - val_loss: 2544.5133\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 2544.50255\n",
      "Epoch 575/3000\n",
      "2100/2100 [==============================] - 1s 488us/step - loss: 2001.5727 - val_loss: 2544.5108\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 2544.50255\n",
      "Epoch 576/3000\n",
      "2100/2100 [==============================] - 1s 672us/step - loss: 2001.5720 - val_loss: 2544.5099\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 2544.50255\n",
      "Epoch 577/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2001.5712 - val_loss: 2544.5073\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 2544.50255\n",
      "Epoch 578/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2001.5704 - val_loss: 2544.5068\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 2544.50255\n",
      "Epoch 579/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.5696 - val_loss: 2544.5068\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 2544.50255\n",
      "Epoch 580/3000\n",
      "2100/2100 [==============================] - 1s 703us/step - loss: 2001.5688 - val_loss: 2544.5069\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 2544.50255\n",
      "Epoch 581/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 2001.5680 - val_loss: 2544.5095\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00581: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 582/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.5670 - val_loss: 2544.5083\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 2544.50255\n",
      "Epoch 583/3000\n",
      "2100/2100 [==============================] - 1s 569us/step - loss: 2001.5663 - val_loss: 2544.5087\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 2544.50255\n",
      "Epoch 584/3000\n",
      "2100/2100 [==============================] - 1s 636us/step - loss: 2001.5656 - val_loss: 2544.5085\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 2544.50255\n",
      "Epoch 585/3000\n",
      "2100/2100 [==============================] - 2s 729us/step - loss: 2001.5648 - val_loss: 2544.5078\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 2544.50255\n",
      "Epoch 586/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.5639 - val_loss: 2544.5050\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 2544.50255\n",
      "Epoch 587/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.5631 - val_loss: 2544.5028\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 2544.50255\n",
      "Epoch 588/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.5625 - val_loss: 2544.5045\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 2544.50255\n",
      "Epoch 589/3000\n",
      "2100/2100 [==============================] - 1s 563us/step - loss: 2001.5615 - val_loss: 2544.5066\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 2544.50255\n",
      "Epoch 590/3000\n",
      "2100/2100 [==============================] - 1s 633us/step - loss: 2001.5609 - val_loss: 2544.5084\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 2544.50255\n",
      "Epoch 591/3000\n",
      "2100/2100 [==============================] - 1s 535us/step - loss: 2001.5598 - val_loss: 2544.5100\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 2544.50255\n",
      "\n",
      "Epoch 00591: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 592/3000\n",
      "2100/2100 [==============================] - 1s 645us/step - loss: 2001.5588 - val_loss: 2544.5132\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 2544.50255\n",
      "Epoch 593/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2001.5584 - val_loss: 2544.5124\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 2544.50255\n",
      "Epoch 594/3000\n",
      "2100/2100 [==============================] - 2s 742us/step - loss: 2001.5576 - val_loss: 2544.5143\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 2544.50255\n",
      "Epoch 595/3000\n",
      "2100/2100 [==============================] - 1s 519us/step - loss: 2001.5567 - val_loss: 2544.5119\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 2544.50255\n",
      "Epoch 596/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2001.5559 - val_loss: 2544.5085\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 2544.50255\n",
      "Epoch 597/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.5549 - val_loss: 2544.5066\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 2544.50255\n",
      "Epoch 598/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2001.5544 - val_loss: 2544.5047\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 2544.50255\n",
      "Epoch 599/3000\n",
      "2100/2100 [==============================] - 2s 744us/step - loss: 2001.5534 - val_loss: 2544.5040\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 2544.50255\n",
      "Epoch 600/3000\n",
      "2100/2100 [==============================] - 1s 676us/step - loss: 2001.5528 - val_loss: 2544.5033\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 2544.50255\n",
      "Epoch 601/3000\n",
      "2100/2100 [==============================] - 1s 527us/step - loss: 2001.5519 - val_loss: 2544.5022\n",
      "\n",
      "Epoch 00601: val_loss improved from 2544.50255 to 2544.50223, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 602/3000\n",
      "2100/2100 [==============================] - 2s 765us/step - loss: 2001.5511 - val_loss: 2544.5035\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 2544.50223\n",
      "Epoch 603/3000\n",
      "2100/2100 [==============================] - 1s 533us/step - loss: 2001.5500 - val_loss: 2544.5043\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 2544.50223\n",
      "Epoch 604/3000\n",
      "2100/2100 [==============================] - 1s 651us/step - loss: 2001.5493 - val_loss: 2544.5051\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 2544.50223\n",
      "Epoch 605/3000\n",
      "2100/2100 [==============================] - 2s 736us/step - loss: 2001.5486 - val_loss: 2544.5033\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 2544.50223\n",
      "Epoch 606/3000\n",
      "2100/2100 [==============================] - 2s 761us/step - loss: 2001.5477 - val_loss: 2544.5036\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 2544.50223\n",
      "Epoch 607/3000\n",
      "2100/2100 [==============================] - 2s 751us/step - loss: 2001.5469 - val_loss: 2544.5051\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 2544.50223\n",
      "Epoch 608/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.5462 - val_loss: 2544.5032\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 2544.50223\n",
      "Epoch 609/3000\n",
      "2100/2100 [==============================] - 1s 531us/step - loss: 2001.5452 - val_loss: 2544.5024\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 2544.50223\n",
      "Epoch 610/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 659us/step - loss: 2001.5445 - val_loss: 2544.5018\n",
      "\n",
      "Epoch 00610: val_loss improved from 2544.50223 to 2544.50179, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 611/3000\n",
      "2100/2100 [==============================] - 2s 749us/step - loss: 2001.5436 - val_loss: 2544.5037\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 2544.50179\n",
      "Epoch 612/3000\n",
      "2100/2100 [==============================] - 2s 734us/step - loss: 2001.5428 - val_loss: 2544.5021\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 2544.50179\n",
      "Epoch 613/3000\n",
      "2100/2100 [==============================] - 1s 643us/step - loss: 2001.5421 - val_loss: 2544.5051\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 2544.50179\n",
      "Epoch 614/3000\n",
      "2100/2100 [==============================] - 1s 535us/step - loss: 2001.5411 - val_loss: 2544.5036\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 2544.50179\n",
      "Epoch 615/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2001.5403 - val_loss: 2544.5048\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 2544.50179\n",
      "Epoch 616/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2001.5392 - val_loss: 2544.5074\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 2544.50179\n",
      "Epoch 617/3000\n",
      "2100/2100 [==============================] - 2s 741us/step - loss: 2001.5386 - val_loss: 2544.5066\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 2544.50179\n",
      "Epoch 618/3000\n",
      "2100/2100 [==============================] - 1s 633us/step - loss: 2001.5377 - val_loss: 2544.5092\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 2544.50179\n",
      "Epoch 619/3000\n",
      "2100/2100 [==============================] - 1s 639us/step - loss: 2001.5371 - val_loss: 2544.5092\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 2544.50179\n",
      "Epoch 620/3000\n",
      "2100/2100 [==============================] - 1s 509us/step - loss: 2001.5361 - val_loss: 2544.5091\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 2544.50179\n",
      "\n",
      "Epoch 00620: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 621/3000\n",
      "2100/2100 [==============================] - 1s 661us/step - loss: 2001.5356 - val_loss: 2544.5088\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 2544.50179\n",
      "Epoch 622/3000\n",
      "2100/2100 [==============================] - 1s 683us/step - loss: 2001.5344 - val_loss: 2544.5053\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 2544.50179\n",
      "Epoch 623/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2001.5334 - val_loss: 2544.5006\n",
      "\n",
      "Epoch 00623: val_loss improved from 2544.50179 to 2544.50063, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 624/3000\n",
      "2100/2100 [==============================] - 1s 664us/step - loss: 2001.5328 - val_loss: 2544.4969\n",
      "\n",
      "Epoch 00624: val_loss improved from 2544.50063 to 2544.49693, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 625/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2001.5320 - val_loss: 2544.4947\n",
      "\n",
      "Epoch 00625: val_loss improved from 2544.49693 to 2544.49473, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 626/3000\n",
      "2100/2100 [==============================] - 2s 758us/step - loss: 2001.5312 - val_loss: 2544.4950\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 2544.49473\n",
      "Epoch 627/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2001.5305 - val_loss: 2544.4961\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 2544.49473\n",
      "Epoch 628/3000\n",
      "2100/2100 [==============================] - 1s 697us/step - loss: 2001.5295 - val_loss: 2544.4981\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 2544.49473\n",
      "Epoch 629/3000\n",
      "2100/2100 [==============================] - 1s 476us/step - loss: 2001.5286 - val_loss: 2544.4995\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 2544.49473\n",
      "Epoch 630/3000\n",
      "2100/2100 [==============================] - 2s 729us/step - loss: 2001.5278 - val_loss: 2544.5002\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 2544.49473\n",
      "Epoch 631/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2001.5269 - val_loss: 2544.5002\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 2544.49473\n",
      "Epoch 632/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.5259 - val_loss: 2544.5000\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 2544.49473\n",
      "Epoch 633/3000\n",
      "2100/2100 [==============================] - 1s 472us/step - loss: 2001.5249 - val_loss: 2544.4993\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 2544.49473\n",
      "Epoch 634/3000\n",
      "2100/2100 [==============================] - 1s 678us/step - loss: 2001.5242 - val_loss: 2544.4983\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 2544.49473\n",
      "Epoch 635/3000\n",
      "2100/2100 [==============================] - 1s 516us/step - loss: 2001.5232 - val_loss: 2544.4994\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 2544.49473\n",
      "\n",
      "Epoch 00635: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 636/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.5227 - val_loss: 2544.4957\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 2544.49473\n",
      "Epoch 637/3000\n",
      "2100/2100 [==============================] - 1s 652us/step - loss: 2001.5216 - val_loss: 2544.4959\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 2544.49473\n",
      "Epoch 638/3000\n",
      "2100/2100 [==============================] - 1s 495us/step - loss: 2001.5209 - val_loss: 2544.4965\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 2544.49473\n",
      "Epoch 639/3000\n",
      "2100/2100 [==============================] - 2s 738us/step - loss: 2001.5199 - val_loss: 2544.4939\n",
      "\n",
      "Epoch 00639: val_loss improved from 2544.49473 to 2544.49395, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 640/3000\n",
      "2100/2100 [==============================] - 1s 703us/step - loss: 2001.5190 - val_loss: 2544.4916\n",
      "\n",
      "Epoch 00640: val_loss improved from 2544.49395 to 2544.49164, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 641/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.5181 - val_loss: 2544.4898\n",
      "\n",
      "Epoch 00641: val_loss improved from 2544.49164 to 2544.48980, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 642/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2001.5173 - val_loss: 2544.4900\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 2544.48980\n",
      "Epoch 643/3000\n",
      "2100/2100 [==============================] - 1s 537us/step - loss: 2001.5167 - val_loss: 2544.4897\n",
      "\n",
      "Epoch 00643: val_loss improved from 2544.48980 to 2544.48967, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 644/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2001.5155 - val_loss: 2544.4884\n",
      "\n",
      "Epoch 00644: val_loss improved from 2544.48967 to 2544.48837, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 645/3000\n",
      "2100/2100 [==============================] - 1s 520us/step - loss: 2001.5147 - val_loss: 2544.4912\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 2544.48837\n",
      "Epoch 646/3000\n",
      "2100/2100 [==============================] - 1s 625us/step - loss: 2001.5137 - val_loss: 2544.4896\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 2544.48837\n",
      "Epoch 647/3000\n",
      "2100/2100 [==============================] - 1s 530us/step - loss: 2001.5128 - val_loss: 2544.4906\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 2544.48837\n",
      "Epoch 648/3000\n",
      "2100/2100 [==============================] - 1s 624us/step - loss: 2001.5120 - val_loss: 2544.4918\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 2544.48837\n",
      "Epoch 649/3000\n",
      "2100/2100 [==============================] - 1s 572us/step - loss: 2001.5112 - val_loss: 2544.4921\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 2544.48837\n",
      "Epoch 650/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 2001.5104 - val_loss: 2544.4916\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 2544.48837\n",
      "Epoch 651/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.5095 - val_loss: 2544.4943\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 2544.48837\n",
      "Epoch 652/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2001.5086 - val_loss: 2544.4955\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 2544.48837\n",
      "Epoch 653/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 2s 727us/step - loss: 2001.5077 - val_loss: 2544.4978\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 2544.48837\n",
      "Epoch 654/3000\n",
      "2100/2100 [==============================] - 2s 720us/step - loss: 2001.5067 - val_loss: 2544.5001\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 2544.48837\n",
      "\n",
      "Epoch 00654: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 655/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.5056 - val_loss: 2544.5037\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 2544.48837\n",
      "Epoch 656/3000\n",
      "2100/2100 [==============================] - 1s 690us/step - loss: 2001.5048 - val_loss: 2544.5054\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 2544.48837\n",
      "Epoch 657/3000\n",
      "2100/2100 [==============================] - 1s 534us/step - loss: 2001.5040 - val_loss: 2544.5064\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 2544.48837\n",
      "Epoch 658/3000\n",
      "2100/2100 [==============================] - 1s 628us/step - loss: 2001.5030 - val_loss: 2544.5093\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 2544.48837\n",
      "Epoch 659/3000\n",
      "2100/2100 [==============================] - 1s 554us/step - loss: 2001.5021 - val_loss: 2544.5104\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 2544.48837\n",
      "Epoch 660/3000\n",
      "2100/2100 [==============================] - 2s 748us/step - loss: 2001.5015 - val_loss: 2544.5078\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 2544.48837\n",
      "Epoch 661/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.5004 - val_loss: 2544.5010\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 2544.48837\n",
      "Epoch 662/3000\n",
      "2100/2100 [==============================] - 1s 607us/step - loss: 2001.4996 - val_loss: 2544.4961\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 2544.48837\n",
      "Epoch 663/3000\n",
      "2100/2100 [==============================] - 1s 587us/step - loss: 2001.4987 - val_loss: 2544.4921\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 2544.48837\n",
      "Epoch 664/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2001.4982 - val_loss: 2544.4870\n",
      "\n",
      "Epoch 00664: val_loss improved from 2544.48837 to 2544.48695, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 665/3000\n",
      "2100/2100 [==============================] - 1s 493us/step - loss: 2001.4971 - val_loss: 2544.4852\n",
      "\n",
      "Epoch 00665: val_loss improved from 2544.48695 to 2544.48522, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 666/3000\n",
      "2100/2100 [==============================] - 1s 697us/step - loss: 2001.4960 - val_loss: 2544.4885\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 2544.48522\n",
      "Epoch 667/3000\n",
      "2100/2100 [==============================] - 1s 673us/step - loss: 2001.4951 - val_loss: 2544.4930\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 2544.48522\n",
      "Epoch 668/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2001.4943 - val_loss: 2544.4977\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 2544.48522\n",
      "Epoch 669/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.4933 - val_loss: 2544.4947\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 2544.48522\n",
      "Epoch 670/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2001.4923 - val_loss: 2544.4976\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 2544.48522\n",
      "Epoch 671/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.4913 - val_loss: 2544.4975\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 2544.48522\n",
      "Epoch 672/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2001.4904 - val_loss: 2544.4942\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 2544.48522\n",
      "Epoch 673/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.4893 - val_loss: 2544.4952\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 2544.48522\n",
      "Epoch 674/3000\n",
      "2100/2100 [==============================] - 1s 656us/step - loss: 2001.4887 - val_loss: 2544.4982\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 2544.48522\n",
      "Epoch 675/3000\n",
      "2100/2100 [==============================] - 1s 674us/step - loss: 2001.4877 - val_loss: 2544.4998\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00675: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 676/3000\n",
      "2100/2100 [==============================] - 1s 648us/step - loss: 2001.4869 - val_loss: 2544.4975\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 2544.48522\n",
      "Epoch 677/3000\n",
      "2100/2100 [==============================] - 1s 570us/step - loss: 2001.4858 - val_loss: 2544.4999\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 2544.48522\n",
      "Epoch 678/3000\n",
      "2100/2100 [==============================] - 1s 606us/step - loss: 2001.4851 - val_loss: 2544.4971\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 2544.48522\n",
      "Epoch 679/3000\n",
      "2100/2100 [==============================] - 1s 581us/step - loss: 2001.4843 - val_loss: 2544.4974\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 2544.48522\n",
      "Epoch 680/3000\n",
      "2100/2100 [==============================] - 1s 565us/step - loss: 2001.4835 - val_loss: 2544.4998\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 2544.48522\n",
      "Epoch 681/3000\n",
      "2100/2100 [==============================] - 1s 667us/step - loss: 2001.4823 - val_loss: 2544.5004\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 2544.48522\n",
      "Epoch 682/3000\n",
      "2100/2100 [==============================] - 1s 479us/step - loss: 2001.4816 - val_loss: 2544.5014\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 2544.48522\n",
      "Epoch 683/3000\n",
      "2100/2100 [==============================] - 1s 635us/step - loss: 2001.4804 - val_loss: 2544.5052\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 2544.48522\n",
      "Epoch 684/3000\n",
      "2100/2100 [==============================] - 1s 513us/step - loss: 2001.4795 - val_loss: 2544.5019\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 2544.48522\n",
      "Epoch 685/3000\n",
      "2100/2100 [==============================] - 1s 652us/step - loss: 2001.4786 - val_loss: 2544.4989\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00685: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 686/3000\n",
      "2100/2100 [==============================] - 1s 489us/step - loss: 2001.4778 - val_loss: 2544.4986\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 2544.48522\n",
      "Epoch 687/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2001.4769 - val_loss: 2544.5042\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 2544.48522\n",
      "Epoch 688/3000\n",
      "2100/2100 [==============================] - 1s 417us/step - loss: 2001.4758 - val_loss: 2544.5047\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 2544.48522\n",
      "Epoch 689/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2001.4749 - val_loss: 2544.5030\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 2544.48522\n",
      "Epoch 690/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2001.4738 - val_loss: 2544.5064\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 2544.48522\n",
      "Epoch 691/3000\n",
      "2100/2100 [==============================] - 1s 442us/step - loss: 2001.4728 - val_loss: 2544.5076\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 2544.48522\n",
      "Epoch 692/3000\n",
      "2100/2100 [==============================] - 2s 740us/step - loss: 2001.4721 - val_loss: 2544.5072\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 2544.48522\n",
      "Epoch 693/3000\n",
      "2100/2100 [==============================] - 1s 675us/step - loss: 2001.4712 - val_loss: 2544.5024\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 2544.48522\n",
      "Epoch 694/3000\n",
      "2100/2100 [==============================] - 1s 497us/step - loss: 2001.4704 - val_loss: 2544.4977\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 2544.48522\n",
      "Epoch 695/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2001.4692 - val_loss: 2544.4954\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00695: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 696/3000\n",
      "2100/2100 [==============================] - 1s 463us/step - loss: 2001.4684 - val_loss: 2544.4952\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 2544.48522\n",
      "Epoch 697/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.4671 - val_loss: 2544.4982\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 2544.48522\n",
      "Epoch 698/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.4662 - val_loss: 2544.4976\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 2544.48522\n",
      "Epoch 699/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 479us/step - loss: 2001.4657 - val_loss: 2544.5010\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 2544.48522\n",
      "Epoch 700/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2001.4645 - val_loss: 2544.4986\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 2544.48522\n",
      "Epoch 701/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2001.4635 - val_loss: 2544.4993\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 2544.48522\n",
      "Epoch 702/3000\n",
      "2100/2100 [==============================] - 1s 498us/step - loss: 2001.4625 - val_loss: 2544.5050\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 2544.48522\n",
      "Epoch 703/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.4616 - val_loss: 2544.5032\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 2544.48522\n",
      "Epoch 704/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.4607 - val_loss: 2544.5075\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 2544.48522\n",
      "Epoch 705/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2001.4597 - val_loss: 2544.5084\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00705: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 706/3000\n",
      "2100/2100 [==============================] - 1s 475us/step - loss: 2001.4586 - val_loss: 2544.5076\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 2544.48522\n",
      "Epoch 707/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2001.4576 - val_loss: 2544.5108\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 2544.48522\n",
      "Epoch 708/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2001.4568 - val_loss: 2544.5126\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 2544.48522\n",
      "Epoch 709/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2001.4557 - val_loss: 2544.5096\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 2544.48522\n",
      "Epoch 710/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2001.4547 - val_loss: 2544.5080\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 2544.48522\n",
      "Epoch 711/3000\n",
      "2100/2100 [==============================] - 1s 457us/step - loss: 2001.4540 - val_loss: 2544.5057\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 2544.48522\n",
      "Epoch 712/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.4529 - val_loss: 2544.5011\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 2544.48522\n",
      "Epoch 713/3000\n",
      "2100/2100 [==============================] - 2s 722us/step - loss: 2001.4518 - val_loss: 2544.4979\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 2544.48522\n",
      "Epoch 714/3000\n",
      "2100/2100 [==============================] - 1s 471us/step - loss: 2001.4509 - val_loss: 2544.4987\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 2544.48522\n",
      "Epoch 715/3000\n",
      "2100/2100 [==============================] - 1s 689us/step - loss: 2001.4499 - val_loss: 2544.4986\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00715: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 716/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2001.4491 - val_loss: 2544.5033\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 2544.48522\n",
      "Epoch 717/3000\n",
      "2100/2100 [==============================] - 1s 462us/step - loss: 2001.4481 - val_loss: 2544.5076\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 2544.48522\n",
      "Epoch 718/3000\n",
      "2100/2100 [==============================] - 1s 646us/step - loss: 2001.4472 - val_loss: 2544.5017\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 2544.48522\n",
      "Epoch 719/3000\n",
      "2100/2100 [==============================] - 1s 533us/step - loss: 2001.4462 - val_loss: 2544.4976\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 2544.48522\n",
      "Epoch 720/3000\n",
      "2100/2100 [==============================] - 1s 588us/step - loss: 2001.4449 - val_loss: 2544.5041\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 2544.48522\n",
      "Epoch 721/3000\n",
      "2100/2100 [==============================] - 1s 616us/step - loss: 2001.4441 - val_loss: 2544.5052\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 2544.48522\n",
      "Epoch 722/3000\n",
      "2100/2100 [==============================] - 1s 536us/step - loss: 2001.4428 - val_loss: 2544.5034\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 2544.48522\n",
      "Epoch 723/3000\n",
      "2100/2100 [==============================] - 1s 619us/step - loss: 2001.4420 - val_loss: 2544.5022\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 2544.48522\n",
      "Epoch 724/3000\n",
      "2100/2100 [==============================] - 1s 509us/step - loss: 2001.4409 - val_loss: 2544.4985\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 2544.48522\n",
      "Epoch 725/3000\n",
      "2100/2100 [==============================] - 1s 597us/step - loss: 2001.4401 - val_loss: 2544.5002\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00725: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 726/3000\n",
      "2100/2100 [==============================] - 1s 529us/step - loss: 2001.4390 - val_loss: 2544.5016\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 2544.48522\n",
      "Epoch 727/3000\n",
      "2100/2100 [==============================] - 1s 660us/step - loss: 2001.4381 - val_loss: 2544.5037\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 2544.48522\n",
      "Epoch 728/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2001.4368 - val_loss: 2544.5037\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 2544.48522\n",
      "Epoch 729/3000\n",
      "2100/2100 [==============================] - 1s 664us/step - loss: 2001.4359 - val_loss: 2544.5079\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 2544.48522\n",
      "Epoch 730/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2001.4352 - val_loss: 2544.5111\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 2544.48522\n",
      "Epoch 731/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2001.4341 - val_loss: 2544.5092\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 2544.48522\n",
      "Epoch 732/3000\n",
      "2100/2100 [==============================] - 2s 722us/step - loss: 2001.4331 - val_loss: 2544.5065\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 2544.48522\n",
      "Epoch 733/3000\n",
      "2100/2100 [==============================] - 1s 625us/step - loss: 2001.4320 - val_loss: 2544.5049\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 2544.48522\n",
      "Epoch 734/3000\n",
      "2100/2100 [==============================] - 1s 536us/step - loss: 2001.4311 - val_loss: 2544.5035\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 2544.48522\n",
      "Epoch 735/3000\n",
      "2100/2100 [==============================] - 1s 577us/step - loss: 2001.4301 - val_loss: 2544.5047\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00735: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 736/3000\n",
      "2100/2100 [==============================] - 1s 528us/step - loss: 2001.4287 - val_loss: 2544.5054\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 2544.48522\n",
      "Epoch 737/3000\n",
      "2100/2100 [==============================] - 1s 576us/step - loss: 2001.4283 - val_loss: 2544.5060\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 2544.48522\n",
      "Epoch 738/3000\n",
      "2100/2100 [==============================] - 1s 602us/step - loss: 2001.4272 - val_loss: 2544.5061\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 2544.48522\n",
      "Epoch 739/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.4260 - val_loss: 2544.5025\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 2544.48522\n",
      "Epoch 740/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.4251 - val_loss: 2544.4992\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 2544.48522\n",
      "Epoch 741/3000\n",
      "2100/2100 [==============================] - 1s 575us/step - loss: 2001.4240 - val_loss: 2544.5015\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 2544.48522\n",
      "Epoch 742/3000\n",
      "2100/2100 [==============================] - 1s 600us/step - loss: 2001.4231 - val_loss: 2544.5003\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 2544.48522\n",
      "Epoch 743/3000\n",
      "2100/2100 [==============================] - 1s 531us/step - loss: 2001.4219 - val_loss: 2544.4970\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 2544.48522\n",
      "Epoch 744/3000\n",
      "2100/2100 [==============================] - 1s 590us/step - loss: 2001.4207 - val_loss: 2544.4949\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 2544.48522\n",
      "Epoch 745/3000\n",
      "2100/2100 [==============================] - 1s 541us/step - loss: 2001.4200 - val_loss: 2544.4939\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00745: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 746/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 642us/step - loss: 2001.4190 - val_loss: 2544.4963\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 2544.48522\n",
      "Epoch 747/3000\n",
      "2100/2100 [==============================] - 1s 672us/step - loss: 2001.4178 - val_loss: 2544.4933\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 2544.48522\n",
      "Epoch 748/3000\n",
      "2100/2100 [==============================] - 1s 542us/step - loss: 2001.4169 - val_loss: 2544.4902\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 2544.48522\n",
      "Epoch 749/3000\n",
      "2100/2100 [==============================] - 1s 645us/step - loss: 2001.4158 - val_loss: 2544.4910\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 2544.48522\n",
      "Epoch 750/3000\n",
      "2100/2100 [==============================] - 2s 740us/step - loss: 2001.4148 - val_loss: 2544.4937\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 2544.48522\n",
      "Epoch 751/3000\n",
      "2100/2100 [==============================] - 2s 739us/step - loss: 2001.4136 - val_loss: 2544.4942\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 2544.48522\n",
      "Epoch 752/3000\n",
      "2100/2100 [==============================] - 1s 510us/step - loss: 2001.4127 - val_loss: 2544.4957\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 2544.48522\n",
      "Epoch 753/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2001.4119 - val_loss: 2544.4952\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 2544.48522\n",
      "Epoch 754/3000\n",
      "2100/2100 [==============================] - 1s 435us/step - loss: 2001.4109 - val_loss: 2544.4949\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 2544.48522\n",
      "Epoch 755/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.4097 - val_loss: 2544.4920\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 2544.48522\n",
      "\n",
      "Epoch 00755: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 756/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2001.4086 - val_loss: 2544.4863\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 2544.48522\n",
      "Epoch 757/3000\n",
      "2100/2100 [==============================] - 2s 738us/step - loss: 2001.4078 - val_loss: 2544.4848\n",
      "\n",
      "Epoch 00757: val_loss improved from 2544.48522 to 2544.48477, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 758/3000\n",
      "2100/2100 [==============================] - 2s 732us/step - loss: 2001.4065 - val_loss: 2544.4866\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 2544.48477\n",
      "Epoch 759/3000\n",
      "2100/2100 [==============================] - 2s 729us/step - loss: 2001.4054 - val_loss: 2544.4863\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 2544.48477\n",
      "Epoch 760/3000\n",
      "2100/2100 [==============================] - 1s 451us/step - loss: 2001.4042 - val_loss: 2544.4872\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 2544.48477\n",
      "Epoch 761/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 2001.4033 - val_loss: 2544.4901\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 2544.48477\n",
      "Epoch 762/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2001.4023 - val_loss: 2544.4960\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 2544.48477\n",
      "Epoch 763/3000\n",
      "2100/2100 [==============================] - 2s 746us/step - loss: 2001.4012 - val_loss: 2544.4974\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 2544.48477\n",
      "Epoch 764/3000\n",
      "2100/2100 [==============================] - 1s 485us/step - loss: 2001.4000 - val_loss: 2544.4926\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 2544.48477\n",
      "Epoch 765/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.3991 - val_loss: 2544.4946\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 2544.48477\n",
      "Epoch 766/3000\n",
      "2100/2100 [==============================] - 1s 666us/step - loss: 2001.3981 - val_loss: 2544.4984\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 2544.48477\n",
      "Epoch 767/3000\n",
      "2100/2100 [==============================] - 1s 514us/step - loss: 2001.3969 - val_loss: 2544.4991\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 2544.48477\n",
      "\n",
      "Epoch 00767: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 768/3000\n",
      "2100/2100 [==============================] - 1s 705us/step - loss: 2001.3957 - val_loss: 2544.4975\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 2544.48477\n",
      "Epoch 769/3000\n",
      "2100/2100 [==============================] - 1s 644us/step - loss: 2001.3946 - val_loss: 2544.4966\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 2544.48477\n",
      "Epoch 770/3000\n",
      "2100/2100 [==============================] - 1s 528us/step - loss: 2001.3937 - val_loss: 2544.4984\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 2544.48477\n",
      "Epoch 771/3000\n",
      "2100/2100 [==============================] - 1s 689us/step - loss: 2001.3928 - val_loss: 2544.5010\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 2544.48477\n",
      "Epoch 772/3000\n",
      "2100/2100 [==============================] - 1s 652us/step - loss: 2001.3917 - val_loss: 2544.4974\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 2544.48477\n",
      "Epoch 773/3000\n",
      "2100/2100 [==============================] - 1s 560us/step - loss: 2001.3906 - val_loss: 2544.4966\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 2544.48477\n",
      "Epoch 774/3000\n",
      "2100/2100 [==============================] - 2s 743us/step - loss: 2001.3894 - val_loss: 2544.4986\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 2544.48477\n",
      "Epoch 775/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.3885 - val_loss: 2544.4968\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 2544.48477\n",
      "Epoch 776/3000\n",
      "2100/2100 [==============================] - 2s 725us/step - loss: 2001.3877 - val_loss: 2544.4995\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 2544.48477\n",
      "Epoch 777/3000\n",
      "2100/2100 [==============================] - 2s 737us/step - loss: 2001.3866 - val_loss: 2544.4997\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 2544.48477\n",
      "\n",
      "Epoch 00777: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 778/3000\n",
      "2100/2100 [==============================] - 1s 598us/step - loss: 2001.3856 - val_loss: 2544.5016\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 2544.48477\n",
      "Epoch 779/3000\n",
      "2100/2100 [==============================] - 1s 579us/step - loss: 2001.3844 - val_loss: 2544.5024\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 2544.48477\n",
      "Epoch 780/3000\n",
      "2100/2100 [==============================] - 1s 573us/step - loss: 2001.3833 - val_loss: 2544.5049\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 2544.48477\n",
      "Epoch 781/3000\n",
      "2100/2100 [==============================] - 1s 586us/step - loss: 2001.3822 - val_loss: 2544.5056\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 2544.48477\n",
      "Epoch 782/3000\n",
      "2100/2100 [==============================] - 1s 666us/step - loss: 2001.3812 - val_loss: 2544.5014\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 2544.48477\n",
      "Epoch 783/3000\n",
      "2100/2100 [==============================] - 1s 568us/step - loss: 2001.3797 - val_loss: 2544.5024\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 2544.48477\n",
      "Epoch 784/3000\n",
      "2100/2100 [==============================] - 1s 566us/step - loss: 2001.3785 - val_loss: 2544.5009\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 2544.48477\n",
      "Epoch 785/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.3780 - val_loss: 2544.5002\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 2544.48477\n",
      "Epoch 786/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.3766 - val_loss: 2544.5059\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 2544.48477\n",
      "Epoch 787/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2001.3753 - val_loss: 2544.5094\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 2544.48477\n",
      "\n",
      "Epoch 00787: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 788/3000\n",
      "2100/2100 [==============================] - 1s 607us/step - loss: 2001.3742 - val_loss: 2544.5108\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 2544.48477\n",
      "Epoch 789/3000\n",
      "2100/2100 [==============================] - 1s 551us/step - loss: 2001.3735 - val_loss: 2544.5132\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 2544.48477\n",
      "Epoch 790/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 2001.3722 - val_loss: 2544.5097\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 2544.48477\n",
      "Epoch 791/3000\n",
      "2100/2100 [==============================] - 1s 555us/step - loss: 2001.3715 - val_loss: 2544.5048\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 2544.48477\n",
      "Epoch 792/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 599us/step - loss: 2001.3702 - val_loss: 2544.5114\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 2544.48477\n",
      "Epoch 793/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.3692 - val_loss: 2544.5122\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 2544.48477\n",
      "Epoch 794/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2001.3681 - val_loss: 2544.5114\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 2544.48477\n",
      "Epoch 795/3000\n",
      "2100/2100 [==============================] - 1s 589us/step - loss: 2001.3669 - val_loss: 2544.5079\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 2544.48477\n",
      "Epoch 796/3000\n",
      "2100/2100 [==============================] - 1s 635us/step - loss: 2001.3657 - val_loss: 2544.5042\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 2544.48477\n",
      "Epoch 797/3000\n",
      "2100/2100 [==============================] - 1s 511us/step - loss: 2001.3647 - val_loss: 2544.4996\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 2544.48477\n",
      "\n",
      "Epoch 00797: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 798/3000\n",
      "2100/2100 [==============================] - 1s 613us/step - loss: 2001.3637 - val_loss: 2544.4984\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 2544.48477\n",
      "Epoch 799/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2001.3627 - val_loss: 2544.5030\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 2544.48477\n",
      "Epoch 800/3000\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2001.3614 - val_loss: 2544.4994\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 2544.48477\n",
      "Epoch 801/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2001.3604 - val_loss: 2544.4984\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 2544.48477\n",
      "Epoch 802/3000\n",
      "2100/2100 [==============================] - 1s 674us/step - loss: 2001.3593 - val_loss: 2544.5010\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 2544.48477\n",
      "Epoch 803/3000\n",
      "2100/2100 [==============================] - 1s 632us/step - loss: 2001.3581 - val_loss: 2544.5002\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 2544.48477\n",
      "Epoch 804/3000\n",
      "2100/2100 [==============================] - 1s 556us/step - loss: 2001.3574 - val_loss: 2544.4912\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 2544.48477\n",
      "Epoch 805/3000\n",
      "2100/2100 [==============================] - 1s 595us/step - loss: 2001.3562 - val_loss: 2544.4881\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 2544.48477\n",
      "Epoch 806/3000\n",
      "2100/2100 [==============================] - 1s 614us/step - loss: 2001.3550 - val_loss: 2544.4913\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 2544.48477\n",
      "Epoch 807/3000\n",
      "2100/2100 [==============================] - 1s 550us/step - loss: 2001.3541 - val_loss: 2544.4887\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 2544.48477\n",
      "\n",
      "Epoch 00807: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 808/3000\n",
      "2100/2100 [==============================] - 1s 637us/step - loss: 2001.3529 - val_loss: 2544.4848\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 2544.48477\n",
      "Epoch 809/3000\n",
      "2100/2100 [==============================] - 1s 523us/step - loss: 2001.3513 - val_loss: 2544.4885\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 2544.48477\n",
      "Epoch 810/3000\n",
      "2100/2100 [==============================] - 1s 653us/step - loss: 2001.3502 - val_loss: 2544.4893\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 2544.48477\n",
      "Epoch 811/3000\n",
      "2100/2100 [==============================] - 1s 486us/step - loss: 2001.3493 - val_loss: 2544.4930\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 2544.48477\n",
      "Epoch 812/3000\n",
      "2100/2100 [==============================] - 1s 699us/step - loss: 2001.3481 - val_loss: 2544.4902\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 2544.48477\n",
      "Epoch 813/3000\n",
      "2100/2100 [==============================] - 2s 722us/step - loss: 2001.3470 - val_loss: 2544.4914\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 2544.48477\n",
      "Epoch 814/3000\n",
      "2100/2100 [==============================] - 1s 681us/step - loss: 2001.3460 - val_loss: 2544.4937\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 2544.48477\n",
      "Epoch 815/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.3447 - val_loss: 2544.4988\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 2544.48477\n",
      "Epoch 816/3000\n",
      "2100/2100 [==============================] - 1s 517us/step - loss: 2001.3436 - val_loss: 2544.4953\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 2544.48477\n",
      "Epoch 817/3000\n",
      "2100/2100 [==============================] - 1s 651us/step - loss: 2001.3426 - val_loss: 2544.4958\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 2544.48477\n",
      "\n",
      "Epoch 00817: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 818/3000\n",
      "2100/2100 [==============================] - 1s 484us/step - loss: 2001.3416 - val_loss: 2544.4936\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 2544.48477\n",
      "Epoch 819/3000\n",
      "2100/2100 [==============================] - 1s 666us/step - loss: 2001.3402 - val_loss: 2544.4865\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 2544.48477\n",
      "Epoch 820/3000\n",
      "2100/2100 [==============================] - 2s 751us/step - loss: 2001.3390 - val_loss: 2544.4798\n",
      "\n",
      "Epoch 00820: val_loss improved from 2544.48477 to 2544.47975, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 821/3000\n",
      "2100/2100 [==============================] - 2s 714us/step - loss: 2001.3380 - val_loss: 2544.4785\n",
      "\n",
      "Epoch 00821: val_loss improved from 2544.47975 to 2544.47853, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 822/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.3367 - val_loss: 2544.4818\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 2544.47853\n",
      "Epoch 823/3000\n",
      "2100/2100 [==============================] - 1s 680us/step - loss: 2001.3357 - val_loss: 2544.4837\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 2544.47853\n",
      "Epoch 824/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.3349 - val_loss: 2544.4865\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 2544.47853\n",
      "Epoch 825/3000\n",
      "2100/2100 [==============================] - 1s 483us/step - loss: 2001.3336 - val_loss: 2544.4871\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 2544.47853\n",
      "Epoch 826/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2001.3320 - val_loss: 2544.4920\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 2544.47853\n",
      "Epoch 827/3000\n",
      "2100/2100 [==============================] - 2s 714us/step - loss: 2001.3311 - val_loss: 2544.4929\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 2544.47853\n",
      "Epoch 828/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.3301 - val_loss: 2544.4947\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 2544.47853\n",
      "Epoch 829/3000\n",
      "2100/2100 [==============================] - 1s 694us/step - loss: 2001.3287 - val_loss: 2544.4922\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 2544.47853\n",
      "Epoch 830/3000\n",
      "2100/2100 [==============================] - 2s 725us/step - loss: 2001.3278 - val_loss: 2544.4905\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 2544.47853\n",
      "Epoch 831/3000\n",
      "2100/2100 [==============================] - 1s 466us/step - loss: 2001.3266 - val_loss: 2544.4904\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 2544.47853\n",
      "\n",
      "Epoch 00831: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 832/3000\n",
      "2100/2100 [==============================] - 1s 694us/step - loss: 2001.3252 - val_loss: 2544.4854\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 2544.47853\n",
      "Epoch 833/3000\n",
      "2100/2100 [==============================] - 1s 461us/step - loss: 2001.3241 - val_loss: 2544.4881\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 2544.47853\n",
      "Epoch 834/3000\n",
      "2100/2100 [==============================] - 1s 697us/step - loss: 2001.3230 - val_loss: 2544.4949\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 2544.47853\n",
      "Epoch 835/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.3219 - val_loss: 2544.4938\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 2544.47853\n",
      "Epoch 836/3000\n",
      "2100/2100 [==============================] - 1s 448us/step - loss: 2001.3210 - val_loss: 2544.4902\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 2544.47853\n",
      "Epoch 837/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.3198 - val_loss: 2544.4898\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 2544.47853\n",
      "Epoch 838/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 454us/step - loss: 2001.3186 - val_loss: 2544.4932\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 2544.47853\n",
      "Epoch 839/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.3174 - val_loss: 2544.4943\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 2544.47853\n",
      "Epoch 840/3000\n",
      "2100/2100 [==============================] - 1s 417us/step - loss: 2001.3163 - val_loss: 2544.4963\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 2544.47853\n",
      "Epoch 841/3000\n",
      "2100/2100 [==============================] - 1s 679us/step - loss: 2001.3151 - val_loss: 2544.4993\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 2544.47853\n",
      "\n",
      "Epoch 00841: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 842/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2001.3139 - val_loss: 2544.4984\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 2544.47853\n",
      "Epoch 843/3000\n",
      "2100/2100 [==============================] - 2s 736us/step - loss: 2001.3128 - val_loss: 2544.4987\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 2544.47853\n",
      "Epoch 844/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.3117 - val_loss: 2544.4990\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 2544.47853\n",
      "Epoch 845/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2001.3102 - val_loss: 2544.5003\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 2544.47853\n",
      "Epoch 846/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.3091 - val_loss: 2544.5032\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 2544.47853\n",
      "Epoch 847/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.3078 - val_loss: 2544.4958\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 2544.47853\n",
      "Epoch 848/3000\n",
      "2100/2100 [==============================] - 1s 546us/step - loss: 2001.3068 - val_loss: 2544.4937\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 2544.47853\n",
      "Epoch 849/3000\n",
      "2100/2100 [==============================] - 1s 579us/step - loss: 2001.3056 - val_loss: 2544.4930\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 2544.47853\n",
      "Epoch 850/3000\n",
      "2100/2100 [==============================] - 1s 568us/step - loss: 2001.3043 - val_loss: 2544.4948\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 2544.47853\n",
      "Epoch 851/3000\n",
      "2100/2100 [==============================] - 1s 589us/step - loss: 2001.3032 - val_loss: 2544.4927\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 2544.47853\n",
      "\n",
      "Epoch 00851: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 852/3000\n",
      "2100/2100 [==============================] - 1s 527us/step - loss: 2001.3023 - val_loss: 2544.4934\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 2544.47853\n",
      "Epoch 853/3000\n",
      "2100/2100 [==============================] - 1s 594us/step - loss: 2001.3011 - val_loss: 2544.4864\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 2544.47853\n",
      "Epoch 854/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.3000 - val_loss: 2544.4826\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 2544.47853\n",
      "Epoch 855/3000\n",
      "2100/2100 [==============================] - 1s 560us/step - loss: 2001.2986 - val_loss: 2544.4833\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 2544.47853\n",
      "Epoch 856/3000\n",
      "2100/2100 [==============================] - 1s 619us/step - loss: 2001.2975 - val_loss: 2544.4872\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 2544.47853\n",
      "Epoch 857/3000\n",
      "2100/2100 [==============================] - 1s 514us/step - loss: 2001.2961 - val_loss: 2544.4897\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 2544.47853\n",
      "Epoch 858/3000\n",
      "2100/2100 [==============================] - 1s 604us/step - loss: 2001.2945 - val_loss: 2544.4904\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 2544.47853\n",
      "Epoch 859/3000\n",
      "2100/2100 [==============================] - 1s 532us/step - loss: 2001.2939 - val_loss: 2544.4904\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 2544.47853\n",
      "Epoch 860/3000\n",
      "2100/2100 [==============================] - 1s 628us/step - loss: 2001.2927 - val_loss: 2544.4904\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 2544.47853\n",
      "Epoch 861/3000\n",
      "2100/2100 [==============================] - 1s 512us/step - loss: 2001.2914 - val_loss: 2544.4931\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 2544.47853\n",
      "\n",
      "Epoch 00861: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 862/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2001.2900 - val_loss: 2544.4840\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 2544.47853\n",
      "Epoch 863/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2001.2888 - val_loss: 2544.4860\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 2544.47853\n",
      "Epoch 864/3000\n",
      "2100/2100 [==============================] - 1s 490us/step - loss: 2001.2878 - val_loss: 2544.4871\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 2544.47853\n",
      "Epoch 865/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.2868 - val_loss: 2544.4888\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 2544.47853\n",
      "Epoch 866/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.2853 - val_loss: 2544.4918\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 2544.47853\n",
      "Epoch 867/3000\n",
      "2100/2100 [==============================] - 1s 461us/step - loss: 2001.2839 - val_loss: 2544.4892\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 2544.47853\n",
      "Epoch 868/3000\n",
      "2100/2100 [==============================] - 1s 677us/step - loss: 2001.2831 - val_loss: 2544.4876\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 2544.47853\n",
      "Epoch 869/3000\n",
      "2100/2100 [==============================] - 1s 437us/step - loss: 2001.2819 - val_loss: 2544.4858\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 2544.47853\n",
      "Epoch 870/3000\n",
      "2100/2100 [==============================] - 2s 738us/step - loss: 2001.2807 - val_loss: 2544.4850\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 2544.47853\n",
      "Epoch 871/3000\n",
      "2100/2100 [==============================] - 2s 731us/step - loss: 2001.2794 - val_loss: 2544.4871\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 2544.47853\n",
      "\n",
      "Epoch 00871: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 872/3000\n",
      "2100/2100 [==============================] - 1s 450us/step - loss: 2001.2782 - val_loss: 2544.4915\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 2544.47853\n",
      "Epoch 873/3000\n",
      "2100/2100 [==============================] - 1s 702us/step - loss: 2001.2772 - val_loss: 2544.4930\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 2544.47853\n",
      "Epoch 874/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.2760 - val_loss: 2544.4953\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 2544.47853\n",
      "Epoch 875/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.2747 - val_loss: 2544.4880\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 2544.47853\n",
      "Epoch 876/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2001.2734 - val_loss: 2544.4825\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 2544.47853\n",
      "Epoch 877/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.2724 - val_loss: 2544.4785\n",
      "\n",
      "Epoch 00877: val_loss improved from 2544.47853 to 2544.47848, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 878/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2001.2706 - val_loss: 2544.4795\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 2544.47848\n",
      "Epoch 879/3000\n",
      "2100/2100 [==============================] - 2s 720us/step - loss: 2001.2701 - val_loss: 2544.4786\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 2544.47848\n",
      "Epoch 880/3000\n",
      "2100/2100 [==============================] - 2s 737us/step - loss: 2001.2686 - val_loss: 2544.4841\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 2544.47848\n",
      "Epoch 881/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.2675 - val_loss: 2544.4788\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 2544.47848\n",
      "\n",
      "Epoch 00881: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 882/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2001.2660 - val_loss: 2544.4737\n",
      "\n",
      "Epoch 00882: val_loss improved from 2544.47848 to 2544.47366, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 883/3000\n",
      "2100/2100 [==============================] - 1s 605us/step - loss: 2001.2651 - val_loss: 2544.4765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00883: val_loss did not improve from 2544.47366\n",
      "Epoch 884/3000\n",
      "2100/2100 [==============================] - 1s 562us/step - loss: 2001.2634 - val_loss: 2544.4818\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 2544.47366\n",
      "Epoch 885/3000\n",
      "2100/2100 [==============================] - 1s 617us/step - loss: 2001.2622 - val_loss: 2544.4847\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 2544.47366\n",
      "Epoch 886/3000\n",
      "2100/2100 [==============================] - 1s 532us/step - loss: 2001.2611 - val_loss: 2544.4829\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 2544.47366\n",
      "Epoch 887/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2001.2601 - val_loss: 2544.4846\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 2544.47366\n",
      "Epoch 888/3000\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 2001.2588 - val_loss: 2544.4870\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 2544.47366\n",
      "Epoch 889/3000\n",
      "2100/2100 [==============================] - 1s 468us/step - loss: 2001.2577 - val_loss: 2544.4820\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 2544.47366\n",
      "Epoch 890/3000\n",
      "2100/2100 [==============================] - 1s 669us/step - loss: 2001.2567 - val_loss: 2544.4831\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 2544.47366\n",
      "Epoch 891/3000\n",
      "2100/2100 [==============================] - 1s 684us/step - loss: 2001.2551 - val_loss: 2544.4774\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 2544.47366\n",
      "Epoch 892/3000\n",
      "2100/2100 [==============================] - 1s 492us/step - loss: 2001.2537 - val_loss: 2544.4734\n",
      "\n",
      "Epoch 00892: val_loss improved from 2544.47366 to 2544.47339, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 893/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.2523 - val_loss: 2544.4797\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 2544.47339\n",
      "Epoch 894/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.2512 - val_loss: 2544.4779\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 2544.47339\n",
      "Epoch 895/3000\n",
      "2100/2100 [==============================] - 1s 672us/step - loss: 2001.2501 - val_loss: 2544.4777\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 2544.47339\n",
      "Epoch 896/3000\n",
      "2100/2100 [==============================] - 1s 647us/step - loss: 2001.2485 - val_loss: 2544.4780\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 2544.47339\n",
      "Epoch 897/3000\n",
      "2100/2100 [==============================] - 1s 514us/step - loss: 2001.2478 - val_loss: 2544.4817\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 2544.47339\n",
      "Epoch 898/3000\n",
      "2100/2100 [==============================] - 1s 668us/step - loss: 2001.2464 - val_loss: 2544.4841\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 2544.47339\n",
      "Epoch 899/3000\n",
      "2100/2100 [==============================] - 1s 678us/step - loss: 2001.2450 - val_loss: 2544.4792\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 2544.47339\n",
      "Epoch 900/3000\n",
      "2100/2100 [==============================] - 2s 743us/step - loss: 2001.2437 - val_loss: 2544.4743\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 2544.47339\n",
      "Epoch 901/3000\n",
      "2100/2100 [==============================] - 1s 703us/step - loss: 2001.2426 - val_loss: 2544.4824\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 2544.47339\n",
      "Epoch 902/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.2413 - val_loss: 2544.4849\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 2544.47339\n",
      "\n",
      "Epoch 00902: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 903/3000\n",
      "2100/2100 [==============================] - 1s 478us/step - loss: 2001.2400 - val_loss: 2544.4849\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 2544.47339\n",
      "Epoch 904/3000\n",
      "2100/2100 [==============================] - 1s 683us/step - loss: 2001.2388 - val_loss: 2544.4772\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 2544.47339\n",
      "Epoch 905/3000\n",
      "2100/2100 [==============================] - 1s 419us/step - loss: 2001.2374 - val_loss: 2544.4715\n",
      "\n",
      "Epoch 00905: val_loss improved from 2544.47339 to 2544.47152, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 906/3000\n",
      "2100/2100 [==============================] - 1s 695us/step - loss: 2001.2366 - val_loss: 2544.4698\n",
      "\n",
      "Epoch 00906: val_loss improved from 2544.47152 to 2544.46979, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 907/3000\n",
      "2100/2100 [==============================] - 1s 527us/step - loss: 2001.2351 - val_loss: 2544.4772\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 2544.46979\n",
      "Epoch 908/3000\n",
      "2100/2100 [==============================] - 1s 645us/step - loss: 2001.2340 - val_loss: 2544.4790\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 2544.46979\n",
      "Epoch 909/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.2324 - val_loss: 2544.4843\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 2544.46979\n",
      "Epoch 910/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2001.2316 - val_loss: 2544.4836\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 2544.46979\n",
      "Epoch 911/3000\n",
      "2100/2100 [==============================] - 1s 555us/step - loss: 2001.2303 - val_loss: 2544.4796\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 2544.46979\n",
      "Epoch 912/3000\n",
      "2100/2100 [==============================] - 1s 595us/step - loss: 2001.2290 - val_loss: 2544.4816\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 2544.46979\n",
      "Epoch 913/3000\n",
      "2100/2100 [==============================] - 1s 520us/step - loss: 2001.2277 - val_loss: 2544.4802\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 2544.46979\n",
      "Epoch 914/3000\n",
      "2100/2100 [==============================] - 1s 614us/step - loss: 2001.2262 - val_loss: 2544.4833\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 2544.46979\n",
      "Epoch 915/3000\n",
      "2100/2100 [==============================] - 1s 512us/step - loss: 2001.2257 - val_loss: 2544.4826\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 2544.46979\n",
      "Epoch 916/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2001.2244 - val_loss: 2544.4822\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 2544.46979\n",
      "\n",
      "Epoch 00916: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 917/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.2232 - val_loss: 2544.4893\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 2544.46979\n",
      "Epoch 918/3000\n",
      "2100/2100 [==============================] - 1s 489us/step - loss: 2001.2215 - val_loss: 2544.4854\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 2544.46979\n",
      "Epoch 919/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2001.2200 - val_loss: 2544.4821\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 2544.46979\n",
      "Epoch 920/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2001.2187 - val_loss: 2544.4748\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 2544.46979\n",
      "Epoch 921/3000\n",
      "2100/2100 [==============================] - 1s 418us/step - loss: 2001.2180 - val_loss: 2544.4706\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 2544.46979\n",
      "Epoch 922/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.2165 - val_loss: 2544.4714\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 2544.46979\n",
      "Epoch 923/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2001.2149 - val_loss: 2544.4742\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 2544.46979\n",
      "Epoch 924/3000\n",
      "2100/2100 [==============================] - 1s 703us/step - loss: 2001.2141 - val_loss: 2544.4741\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 2544.46979\n",
      "Epoch 925/3000\n",
      "2100/2100 [==============================] - 1s 470us/step - loss: 2001.2126 - val_loss: 2544.4752\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 2544.46979\n",
      "Epoch 926/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2001.2114 - val_loss: 2544.4752\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 2544.46979\n",
      "\n",
      "Epoch 00926: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 927/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.2100 - val_loss: 2544.4766\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 2544.46979\n",
      "Epoch 928/3000\n",
      "2100/2100 [==============================] - 1s 446us/step - loss: 2001.2086 - val_loss: 2544.4761\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 2544.46979\n",
      "Epoch 929/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 691us/step - loss: 2001.2072 - val_loss: 2544.4832\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 2544.46979\n",
      "Epoch 930/3000\n",
      "2100/2100 [==============================] - 1s 462us/step - loss: 2001.2061 - val_loss: 2544.4768\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 2544.46979\n",
      "Epoch 931/3000\n",
      "2100/2100 [==============================] - 1s 676us/step - loss: 2001.2051 - val_loss: 2544.4745\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 2544.46979\n",
      "Epoch 932/3000\n",
      "2100/2100 [==============================] - 1s 432us/step - loss: 2001.2033 - val_loss: 2544.4760\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 2544.46979\n",
      "Epoch 933/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2001.2022 - val_loss: 2544.4689\n",
      "\n",
      "Epoch 00933: val_loss improved from 2544.46979 to 2544.46888, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 934/3000\n",
      "2100/2100 [==============================] - 1s 464us/step - loss: 2001.2006 - val_loss: 2544.4665\n",
      "\n",
      "Epoch 00934: val_loss improved from 2544.46888 to 2544.46646, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 935/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.1997 - val_loss: 2544.4696\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 2544.46646\n",
      "Epoch 936/3000\n",
      "2100/2100 [==============================] - 1s 511us/step - loss: 2001.1984 - val_loss: 2544.4697\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 2544.46646\n",
      "Epoch 937/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.1971 - val_loss: 2544.4674\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 2544.46646\n",
      "Epoch 938/3000\n",
      "2100/2100 [==============================] - 2s 720us/step - loss: 2001.1960 - val_loss: 2544.4708\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 2544.46646\n",
      "Epoch 939/3000\n",
      "2100/2100 [==============================] - 1s 440us/step - loss: 2001.1945 - val_loss: 2544.4787\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 2544.46646\n",
      "Epoch 940/3000\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 2001.1934 - val_loss: 2544.4824\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 2544.46646\n",
      "Epoch 941/3000\n",
      "2100/2100 [==============================] - 1s 449us/step - loss: 2001.1921 - val_loss: 2544.4744\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 2544.46646\n",
      "Epoch 942/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 2001.1906 - val_loss: 2544.4694\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 2544.46646\n",
      "Epoch 943/3000\n",
      "2100/2100 [==============================] - 2s 735us/step - loss: 2001.1892 - val_loss: 2544.4687\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 2544.46646\n",
      "Epoch 944/3000\n",
      "2100/2100 [==============================] - 1s 508us/step - loss: 2001.1881 - val_loss: 2544.4706\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 2544.46646\n",
      "\n",
      "Epoch 00944: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 945/3000\n",
      "2100/2100 [==============================] - 1s 635us/step - loss: 2001.1865 - val_loss: 2544.4706\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 2544.46646\n",
      "Epoch 946/3000\n",
      "2100/2100 [==============================] - 1s 528us/step - loss: 2001.1851 - val_loss: 2544.4685\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 2544.46646\n",
      "Epoch 947/3000\n",
      "2100/2100 [==============================] - 1s 614us/step - loss: 2001.1843 - val_loss: 2544.4656\n",
      "\n",
      "Epoch 00947: val_loss improved from 2544.46646 to 2544.46564, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 948/3000\n",
      "2100/2100 [==============================] - 1s 674us/step - loss: 2001.1829 - val_loss: 2544.4695\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 2544.46564\n",
      "Epoch 949/3000\n",
      "2100/2100 [==============================] - 1s 577us/step - loss: 2001.1817 - val_loss: 2544.4651\n",
      "\n",
      "Epoch 00949: val_loss improved from 2544.46564 to 2544.46505, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 950/3000\n",
      "2100/2100 [==============================] - 1s 628us/step - loss: 2001.1803 - val_loss: 2544.4610\n",
      "\n",
      "Epoch 00950: val_loss improved from 2544.46505 to 2544.46097, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 951/3000\n",
      "2100/2100 [==============================] - 1s 602us/step - loss: 2001.1790 - val_loss: 2544.4653\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 2544.46097\n",
      "Epoch 952/3000\n",
      "2100/2100 [==============================] - 1s 586us/step - loss: 2001.1774 - val_loss: 2544.4673\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 2544.46097\n",
      "Epoch 953/3000\n",
      "2100/2100 [==============================] - 1s 572us/step - loss: 2001.1764 - val_loss: 2544.4647\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 2544.46097\n",
      "Epoch 954/3000\n",
      "2100/2100 [==============================] - 1s 580us/step - loss: 2001.1749 - val_loss: 2544.4656\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 2544.46097\n",
      "Epoch 955/3000\n",
      "2100/2100 [==============================] - 1s 591us/step - loss: 2001.1739 - val_loss: 2544.4713\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 2544.46097\n",
      "Epoch 956/3000\n",
      "2100/2100 [==============================] - 1s 691us/step - loss: 2001.1724 - val_loss: 2544.4676\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 2544.46097\n",
      "Epoch 957/3000\n",
      "2100/2100 [==============================] - 1s 606us/step - loss: 2001.1712 - val_loss: 2544.4679\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 2544.46097\n",
      "Epoch 958/3000\n",
      "2100/2100 [==============================] - 1s 537us/step - loss: 2001.1694 - val_loss: 2544.4698\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 2544.46097\n",
      "Epoch 959/3000\n",
      "2100/2100 [==============================] - 1s 595us/step - loss: 2001.1681 - val_loss: 2544.4657\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 2544.46097\n",
      "Epoch 960/3000\n",
      "2100/2100 [==============================] - 1s 595us/step - loss: 2001.1670 - val_loss: 2544.4627\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 2544.46097\n",
      "\n",
      "Epoch 00960: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 961/3000\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 2001.1655 - val_loss: 2544.4674\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 2544.46097\n",
      "Epoch 962/3000\n",
      "2100/2100 [==============================] - 1s 600us/step - loss: 2001.1640 - val_loss: 2544.4715\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 2544.46097\n",
      "Epoch 963/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2001.1630 - val_loss: 2544.4721\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 2544.46097\n",
      "Epoch 964/3000\n",
      "2100/2100 [==============================] - 1s 568us/step - loss: 2001.1619 - val_loss: 2544.4694\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 2544.46097\n",
      "Epoch 965/3000\n",
      "2100/2100 [==============================] - 1s 673us/step - loss: 2001.1604 - val_loss: 2544.4728\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 2544.46097\n",
      "Epoch 966/3000\n",
      "2100/2100 [==============================] - 1s 477us/step - loss: 2001.1589 - val_loss: 2544.4711\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 2544.46097\n",
      "Epoch 967/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.1575 - val_loss: 2544.4713\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 2544.46097\n",
      "Epoch 968/3000\n",
      "2100/2100 [==============================] - 1s 444us/step - loss: 2001.1563 - val_loss: 2544.4705\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 2544.46097\n",
      "Epoch 969/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2001.1551 - val_loss: 2544.4754\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 2544.46097\n",
      "Epoch 970/3000\n",
      "2100/2100 [==============================] - 2s 732us/step - loss: 2001.1537 - val_loss: 2544.4740\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 2544.46097\n",
      "\n",
      "Epoch 00970: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 971/3000\n",
      "2100/2100 [==============================] - 1s 700us/step - loss: 2001.1527 - val_loss: 2544.4710\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 2544.46097\n",
      "Epoch 972/3000\n",
      "2100/2100 [==============================] - 1s 425us/step - loss: 2001.1512 - val_loss: 2544.4694\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 2544.46097\n",
      "Epoch 973/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2001.1500 - val_loss: 2544.4721\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 2544.46097\n",
      "Epoch 974/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 430us/step - loss: 2001.1483 - val_loss: 2544.4704\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 2544.46097\n",
      "Epoch 975/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2001.1468 - val_loss: 2544.4617\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 2544.46097\n",
      "Epoch 976/3000\n",
      "2100/2100 [==============================] - 1s 471us/step - loss: 2001.1455 - val_loss: 2544.4540\n",
      "\n",
      "Epoch 00976: val_loss improved from 2544.46097 to 2544.45401, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 977/3000\n",
      "2100/2100 [==============================] - 1s 514us/step - loss: 2001.1444 - val_loss: 2544.4489\n",
      "\n",
      "Epoch 00977: val_loss improved from 2544.45401 to 2544.44889, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 978/3000\n",
      "2100/2100 [==============================] - 1s 458us/step - loss: 2001.1432 - val_loss: 2544.4538\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 2544.44889\n",
      "Epoch 979/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.1417 - val_loss: 2544.4568\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 2544.44889\n",
      "Epoch 980/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2001.1401 - val_loss: 2544.4610\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 2544.44889\n",
      "Epoch 981/3000\n",
      "2100/2100 [==============================] - 1s 694us/step - loss: 2001.1390 - val_loss: 2544.4629\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 2544.44889\n",
      "Epoch 982/3000\n",
      "2100/2100 [==============================] - 1s 495us/step - loss: 2001.1378 - val_loss: 2544.4578\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 2544.44889\n",
      "Epoch 983/3000\n",
      "2100/2100 [==============================] - 2s 745us/step - loss: 2001.1363 - val_loss: 2544.4552\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 2544.44889\n",
      "Epoch 984/3000\n",
      "2100/2100 [==============================] - 1s 684us/step - loss: 2001.1350 - val_loss: 2544.4565\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 2544.44889\n",
      "Epoch 985/3000\n",
      "2100/2100 [==============================] - 1s 575us/step - loss: 2001.1333 - val_loss: 2544.4616\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 2544.44889\n",
      "Epoch 986/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2001.1319 - val_loss: 2544.4673\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 2544.44889\n",
      "Epoch 987/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2001.1306 - val_loss: 2544.4649\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 2544.44889\n",
      "\n",
      "Epoch 00987: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 988/3000\n",
      "2100/2100 [==============================] - 1s 696us/step - loss: 2001.1292 - val_loss: 2544.4627\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 2544.44889\n",
      "Epoch 989/3000\n",
      "2100/2100 [==============================] - 2s 744us/step - loss: 2001.1279 - val_loss: 2544.4589\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 2544.44889\n",
      "Epoch 990/3000\n",
      "2100/2100 [==============================] - 1s 676us/step - loss: 2001.1268 - val_loss: 2544.4535\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 2544.44889\n",
      "Epoch 991/3000\n",
      "2100/2100 [==============================] - 1s 554us/step - loss: 2001.1253 - val_loss: 2544.4564\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 2544.44889\n",
      "Epoch 992/3000\n",
      "2100/2100 [==============================] - 1s 628us/step - loss: 2001.1239 - val_loss: 2544.4589\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 2544.44889\n",
      "Epoch 993/3000\n",
      "2100/2100 [==============================] - 1s 604us/step - loss: 2001.1228 - val_loss: 2544.4615\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 2544.44889\n",
      "Epoch 994/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.1212 - val_loss: 2544.4591\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 2544.44889\n",
      "Epoch 995/3000\n",
      "2100/2100 [==============================] - 1s 588us/step - loss: 2001.1196 - val_loss: 2544.4594\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 2544.44889\n",
      "Epoch 996/3000\n",
      "2100/2100 [==============================] - 1s 589us/step - loss: 2001.1186 - val_loss: 2544.4617\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 2544.44889\n",
      "Epoch 997/3000\n",
      "2100/2100 [==============================] - 1s 711us/step - loss: 2001.1170 - val_loss: 2544.4651\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 2544.44889\n",
      "\n",
      "Epoch 00997: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 998/3000\n",
      "2100/2100 [==============================] - 1s 588us/step - loss: 2001.1157 - val_loss: 2544.4660\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 2544.44889\n",
      "Epoch 999/3000\n",
      "2100/2100 [==============================] - 1s 561us/step - loss: 2001.1142 - val_loss: 2544.4674\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 2544.44889\n",
      "Epoch 1000/3000\n",
      "2100/2100 [==============================] - 2s 716us/step - loss: 2001.1133 - val_loss: 2544.4667\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 2544.44889\n",
      "Epoch 1001/3000\n",
      "2100/2100 [==============================] - 1s 594us/step - loss: 2001.1114 - val_loss: 2544.4680\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 2544.44889\n",
      "Epoch 1002/3000\n",
      "2100/2100 [==============================] - 1s 587us/step - loss: 2001.1103 - val_loss: 2544.4656\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 2544.44889\n",
      "Epoch 1003/3000\n",
      "2100/2100 [==============================] - 2s 754us/step - loss: 2001.1088 - val_loss: 2544.4646\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 2544.44889\n",
      "Epoch 1004/3000\n",
      "2100/2100 [==============================] - 2s 752us/step - loss: 2001.1073 - val_loss: 2544.4583\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 2544.44889\n",
      "Epoch 1005/3000\n",
      "2100/2100 [==============================] - 1s 531us/step - loss: 2001.1063 - val_loss: 2544.4517\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 2544.44889\n",
      "Epoch 1006/3000\n",
      "2100/2100 [==============================] - 1s 638us/step - loss: 2001.1049 - val_loss: 2544.4487\n",
      "\n",
      "Epoch 01006: val_loss improved from 2544.44889 to 2544.44866, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 1007/3000\n",
      "2100/2100 [==============================] - 2s 728us/step - loss: 2001.1032 - val_loss: 2544.4516\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 2544.44866\n",
      "Epoch 1008/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2001.1019 - val_loss: 2544.4560\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 2544.44866\n",
      "Epoch 1009/3000\n",
      "2100/2100 [==============================] - 1s 713us/step - loss: 2001.1008 - val_loss: 2544.4555\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 2544.44866\n",
      "Epoch 1010/3000\n",
      "2100/2100 [==============================] - 1s 461us/step - loss: 2001.0994 - val_loss: 2544.4528\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 2544.44866\n",
      "Epoch 1011/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2001.0981 - val_loss: 2544.4534\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 2544.44866\n",
      "Epoch 1012/3000\n",
      "2100/2100 [==============================] - 1s 458us/step - loss: 2001.0964 - val_loss: 2544.4519\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 2544.44866\n",
      "Epoch 1013/3000\n",
      "2100/2100 [==============================] - 1s 714us/step - loss: 2001.0952 - val_loss: 2544.4499\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 2544.44866\n",
      "Epoch 1014/3000\n",
      "2100/2100 [==============================] - 2s 741us/step - loss: 2001.0938 - val_loss: 2544.4471\n",
      "\n",
      "Epoch 01014: val_loss improved from 2544.44866 to 2544.44715, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 1015/3000\n",
      "2100/2100 [==============================] - 1s 708us/step - loss: 2001.0923 - val_loss: 2544.4509\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 2544.44715\n",
      "Epoch 1016/3000\n",
      "2100/2100 [==============================] - 1s 688us/step - loss: 2001.0910 - val_loss: 2544.4494\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 2544.44715\n",
      "Epoch 1017/3000\n",
      "2100/2100 [==============================] - 1s 472us/step - loss: 2001.0898 - val_loss: 2544.4478\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 2544.44715\n",
      "Epoch 1018/3000\n",
      "2100/2100 [==============================] - 1s 688us/step - loss: 2001.0879 - val_loss: 2544.4501\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 2544.44715\n",
      "Epoch 1019/3000\n",
      "2100/2100 [==============================] - 2s 733us/step - loss: 2001.0867 - val_loss: 2544.4499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01019: val_loss did not improve from 2544.44715\n",
      "Epoch 1020/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.0851 - val_loss: 2544.4525\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 2544.44715\n",
      "Epoch 1021/3000\n",
      "2100/2100 [==============================] - 1s 465us/step - loss: 2001.0835 - val_loss: 2544.4539\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 2544.44715\n",
      "Epoch 1022/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2001.0824 - val_loss: 2544.4524\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 2544.44715\n",
      "Epoch 1023/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.0810 - val_loss: 2544.4573\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 2544.44715\n",
      "Epoch 1024/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2001.0795 - val_loss: 2544.4491\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 2544.44715\n",
      "\n",
      "Epoch 01024: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 1025/3000\n",
      "2100/2100 [==============================] - 1s 467us/step - loss: 2001.0785 - val_loss: 2544.4505\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 2544.44715\n",
      "Epoch 1026/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.0765 - val_loss: 2544.4540\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 2544.44715\n",
      "Epoch 1027/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.0755 - val_loss: 2544.4513\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 2544.44715\n",
      "Epoch 1028/3000\n",
      "2100/2100 [==============================] - 1s 477us/step - loss: 2001.0742 - val_loss: 2544.4557\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 2544.44715\n",
      "Epoch 1029/3000\n",
      "2100/2100 [==============================] - 1s 686us/step - loss: 2001.0724 - val_loss: 2544.4527\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 2544.44715\n",
      "Epoch 1030/3000\n",
      "2100/2100 [==============================] - 1s 447us/step - loss: 2001.0713 - val_loss: 2544.4519\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 2544.44715\n",
      "Epoch 1031/3000\n",
      "2100/2100 [==============================] - 1s 684us/step - loss: 2001.0695 - val_loss: 2544.4531\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 2544.44715\n",
      "Epoch 1032/3000\n",
      "2100/2100 [==============================] - 1s 478us/step - loss: 2001.0688 - val_loss: 2544.4562\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 2544.44715\n",
      "Epoch 1033/3000\n",
      "2100/2100 [==============================] - 1s 709us/step - loss: 2001.0668 - val_loss: 2544.4535\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 2544.44715\n",
      "Epoch 1034/3000\n",
      "2100/2100 [==============================] - 2s 746us/step - loss: 2001.0655 - val_loss: 2544.4488\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 2544.44715\n",
      "\n",
      "Epoch 01034: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 1035/3000\n",
      "2100/2100 [==============================] - 2s 718us/step - loss: 2001.0641 - val_loss: 2544.4490\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 2544.44715\n",
      "Epoch 1036/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.0628 - val_loss: 2544.4473\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 2544.44715\n",
      "Epoch 1037/3000\n",
      "2100/2100 [==============================] - 2s 724us/step - loss: 2001.0617 - val_loss: 2544.4416\n",
      "\n",
      "Epoch 01037: val_loss improved from 2544.44715 to 2544.44161, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 1038/3000\n",
      "2100/2100 [==============================] - 2s 750us/step - loss: 2001.0601 - val_loss: 2544.4464\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 2544.44161\n",
      "Epoch 1039/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.0585 - val_loss: 2544.4450\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 2544.44161\n",
      "Epoch 1040/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.0571 - val_loss: 2544.4459\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 2544.44161\n",
      "Epoch 1041/3000\n",
      "2100/2100 [==============================] - 1s 505us/step - loss: 2001.0558 - val_loss: 2544.4388\n",
      "\n",
      "Epoch 01041: val_loss improved from 2544.44161 to 2544.43878, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 1042/3000\n",
      "2100/2100 [==============================] - 2s 744us/step - loss: 2001.0544 - val_loss: 2544.4417\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 2544.43878\n",
      "Epoch 1043/3000\n",
      "2100/2100 [==============================] - 1s 542us/step - loss: 2001.0526 - val_loss: 2544.4432\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 2544.43878\n",
      "Epoch 1044/3000\n",
      "2100/2100 [==============================] - 1s 644us/step - loss: 2001.0511 - val_loss: 2544.4434\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 2544.43878\n",
      "Epoch 1045/3000\n",
      "2100/2100 [==============================] - 1s 506us/step - loss: 2001.0501 - val_loss: 2544.4422\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 2544.43878\n",
      "Epoch 1046/3000\n",
      "2100/2100 [==============================] - 2s 717us/step - loss: 2001.0485 - val_loss: 2544.4501\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 2544.43878\n",
      "Epoch 1047/3000\n",
      "2100/2100 [==============================] - 1s 685us/step - loss: 2001.0471 - val_loss: 2544.4531\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 2544.43878\n",
      "Epoch 1048/3000\n",
      "2100/2100 [==============================] - 1s 682us/step - loss: 2001.0454 - val_loss: 2544.4488\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 2544.43878\n",
      "Epoch 1049/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.0441 - val_loss: 2544.4426\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 2544.43878\n",
      "Epoch 1050/3000\n",
      "2100/2100 [==============================] - 1s 712us/step - loss: 2001.0429 - val_loss: 2544.4401\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 2544.43878\n",
      "Epoch 1051/3000\n",
      "2100/2100 [==============================] - 1s 552us/step - loss: 2001.0414 - val_loss: 2544.4373\n",
      "\n",
      "Epoch 01051: val_loss improved from 2544.43878 to 2544.43731, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 1052/3000\n",
      "2100/2100 [==============================] - 2s 731us/step - loss: 2001.0400 - val_loss: 2544.4420\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 2544.43731\n",
      "Epoch 1053/3000\n",
      "2100/2100 [==============================] - 1s 646us/step - loss: 2001.0383 - val_loss: 2544.4468\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 2544.43731\n",
      "Epoch 1054/3000\n",
      "2100/2100 [==============================] - 1s 552us/step - loss: 2001.0369 - val_loss: 2544.4540\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 2544.43731\n",
      "Epoch 1055/3000\n",
      "2100/2100 [==============================] - 2s 728us/step - loss: 2001.0354 - val_loss: 2544.4530\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 2544.43731\n",
      "Epoch 1056/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2001.0339 - val_loss: 2544.4523\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 2544.43731\n",
      "Epoch 1057/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2001.0325 - val_loss: 2544.4494\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 2544.43731\n",
      "Epoch 1058/3000\n",
      "2100/2100 [==============================] - 2s 732us/step - loss: 2001.0314 - val_loss: 2544.4487\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 2544.43731\n",
      "Epoch 1059/3000\n",
      "2100/2100 [==============================] - 1s 703us/step - loss: 2001.0299 - val_loss: 2544.4545\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 2544.43731\n",
      "Epoch 1060/3000\n",
      "2100/2100 [==============================] - 1s 689us/step - loss: 2001.0284 - val_loss: 2544.4535\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 2544.43731\n",
      "Epoch 1061/3000\n",
      "2100/2100 [==============================] - 1s 503us/step - loss: 2001.0270 - val_loss: 2544.4553\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 2544.43731\n",
      "\n",
      "Epoch 01061: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 1062/3000\n",
      "2100/2100 [==============================] - 1s 652us/step - loss: 2001.0259 - val_loss: 2544.4472\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 2544.43731\n",
      "Epoch 1063/3000\n",
      "2100/2100 [==============================] - 1s 536us/step - loss: 2001.0241 - val_loss: 2544.4421\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 2544.43731\n",
      "Epoch 1064/3000\n",
      "2100/2100 [==============================] - 1s 690us/step - loss: 2001.0226 - val_loss: 2544.4417\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 2544.43731\n",
      "Epoch 1065/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 698us/step - loss: 2001.0210 - val_loss: 2544.4434\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 2544.43731\n",
      "Epoch 1066/3000\n",
      "2100/2100 [==============================] - 2s 739us/step - loss: 2001.0197 - val_loss: 2544.4412\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 2544.43731\n",
      "Epoch 1067/3000\n",
      "2100/2100 [==============================] - 2s 740us/step - loss: 2001.0181 - val_loss: 2544.4409\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 2544.43731\n",
      "Epoch 1068/3000\n",
      "2100/2100 [==============================] - 2s 759us/step - loss: 2001.0166 - val_loss: 2544.4409\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 2544.43731\n",
      "Epoch 1069/3000\n",
      "2100/2100 [==============================] - 1s 671us/step - loss: 2001.0151 - val_loss: 2544.4394\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 2544.43731\n",
      "Epoch 1070/3000\n",
      "2100/2100 [==============================] - 1s 510us/step - loss: 2001.0138 - val_loss: 2544.4347\n",
      "\n",
      "Epoch 01070: val_loss improved from 2544.43731 to 2544.43471, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 1071/3000\n",
      "2100/2100 [==============================] - 2s 730us/step - loss: 2001.0122 - val_loss: 2544.4362\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 2544.43471\n",
      "Epoch 1072/3000\n",
      "2100/2100 [==============================] - 2s 727us/step - loss: 2001.0108 - val_loss: 2544.4331\n",
      "\n",
      "Epoch 01072: val_loss improved from 2544.43471 to 2544.43312, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_151446.ckpt\n",
      "Epoch 1073/3000\n",
      "2100/2100 [==============================] - 1s 465us/step - loss: 2001.0093 - val_loss: 2544.4345\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 2544.43312\n",
      "Epoch 1074/3000\n",
      "2100/2100 [==============================] - 1s 693us/step - loss: 2001.0082 - val_loss: 2544.4383\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 2544.43312\n",
      "Epoch 1075/3000\n",
      "2100/2100 [==============================] - 2s 715us/step - loss: 2001.0066 - val_loss: 2544.4414\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 2544.43312\n",
      "Epoch 1076/3000\n",
      "2100/2100 [==============================] - 2s 742us/step - loss: 2001.0052 - val_loss: 2544.4427\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 2544.43312\n",
      "Epoch 1077/3000\n",
      "2100/2100 [==============================] - 1s 496us/step - loss: 2001.0036 - val_loss: 2544.4467\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 2544.43312\n",
      "Epoch 1078/3000\n",
      "2100/2100 [==============================] - 1s 669us/step - loss: 2001.0019 - val_loss: 2544.4472\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 2544.43312\n",
      "Epoch 1079/3000\n",
      "2100/2100 [==============================] - 1s 675us/step - loss: 2001.0005 - val_loss: 2544.4430\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 2544.43312\n",
      "Epoch 1080/3000\n",
      "2100/2100 [==============================] - 1s 551us/step - loss: 2000.9996 - val_loss: 2544.4403\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 2544.43312\n",
      "Epoch 1081/3000\n",
      "2100/2100 [==============================] - 1s 679us/step - loss: 2000.9977 - val_loss: 2544.4378\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 2544.43312\n",
      "Epoch 1082/3000\n",
      "2100/2100 [==============================] - 2s 719us/step - loss: 2000.9958 - val_loss: 2544.4338\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 2544.43312\n",
      "\n",
      "Epoch 01082: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 1083/3000\n",
      "2100/2100 [==============================] - 1s 520us/step - loss: 2000.9945 - val_loss: 2544.4348\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 2544.43312\n",
      "Epoch 1084/3000\n",
      "2100/2100 [==============================] - 1s 698us/step - loss: 2000.9931 - val_loss: 2544.4365\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 2544.43312\n",
      "Epoch 1085/3000\n",
      "2100/2100 [==============================] - 1s 703us/step - loss: 2000.9918 - val_loss: 2544.4408\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 2544.43312\n",
      "Epoch 1086/3000\n",
      "2100/2100 [==============================] - 1s 706us/step - loss: 2000.9901 - val_loss: 2544.4418\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 2544.43312\n",
      "Epoch 1087/3000\n",
      "2100/2100 [==============================] - 1s 500us/step - loss: 2000.9887 - val_loss: 2544.4396\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 2544.43312\n",
      "Epoch 1088/3000\n",
      "2100/2100 [==============================] - 1s 667us/step - loss: 2000.9873 - val_loss: 2544.4382\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 2544.43312\n",
      "Epoch 1089/3000\n",
      "2100/2100 [==============================] - 2s 732us/step - loss: 2000.9859 - val_loss: 2544.4369\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 2544.43312\n",
      "Epoch 1090/3000\n",
      "2100/2100 [==============================] - 2s 721us/step - loss: 2000.9845 - val_loss: 2544.4383\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 2544.43312\n",
      "Epoch 1091/3000\n",
      "2100/2100 [==============================] - 1s 710us/step - loss: 2000.9830 - val_loss: 2544.4361\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 2544.43312\n",
      "Epoch 1092/3000\n",
      "2100/2100 [==============================] - 2s 744us/step - loss: 2000.9814 - val_loss: 2544.4410\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 2544.43312\n",
      "\n",
      "Epoch 01092: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 1093/3000\n",
      "2100/2100 [==============================] - 2s 726us/step - loss: 2000.9799 - val_loss: 2544.4442\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 2544.43312\n",
      "Epoch 1094/3000\n",
      "2100/2100 [==============================] - 1s 521us/step - loss: 2000.9781 - val_loss: 2544.4463\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 2544.43312\n",
      "Epoch 1095/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2000.9769 - val_loss: 2544.4490\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 2544.43312\n",
      "Epoch 1096/3000\n",
      "2100/2100 [==============================] - 1s 699us/step - loss: 2000.9752 - val_loss: 2544.4473\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 2544.43312\n",
      "Epoch 1097/3000\n",
      "2100/2100 [==============================] - 2s 723us/step - loss: 2000.9740 - val_loss: 2544.4446\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 2544.43312\n",
      "Epoch 1098/3000\n",
      " 256/2100 [==>...........................] - ETA: 1s - loss: 2024.4855"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0c88bf5fe84a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 callbacks=[checkpoint, tensorboard,rate])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=3000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[checkpoint, tensorboard,rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(X_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc05300b00>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0].reshape((19, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcc6003ba90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(decoded_imgs[0].reshape((19,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc0523ae48>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(decoded_imgs[0].reshape((19,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc05193630>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(decoded_imgs[0].reshape((19,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the neural network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y_ohe, epochs=100, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_X, test_y_ohe, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train[0].shape[0], X_train[0].shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 128, 1)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = Input(shape=(input_shape))\n",
    "\n",
    "x = Conv2D(4, kernel_size=(3,3), activation='relu', input_shape=input_shape, data_format='channels_last', padding='same')(input_feature)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
    "x = Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
    "x = Conv2D(16, (3,3), activation='relu', padding='same')(x)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
    "#x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same', name='pooling')(x)\n",
    "\n",
    "x = (MaxPooling2D(pool_size=(2,4), padding='same'))(x)\n",
    "x = (Conv2D(16, (3,3), activation='relu', padding='same'))(x)\n",
    "x = (MaxPooling2D(pool_size=(2,4), padding='same'))(x)\n",
    "x = (Conv2D(16, (3,3), activation='relu', padding='same'))(x)\n",
    "x = (MaxPooling2D(pool_size=(2,2), padding='same', name='pooling'))(x)\n",
    "encoded = (Conv2D(16, (3,3), activation='relu', padding='same'))(x)\n",
    "#pooling_shape = tuple(x.shape.as_list()[1:])\n",
    "#x = (Flatten(name='flatten'))(x)\n",
    "#x = (Dense(1024, activation='relu'))(x)\n",
    "#encoded = (Dense(512, activation='relu'))(x)\n",
    "#x = (Dense(7000, activation='relu'))(x)\n",
    "#x = (Dense(3500, activation='relu'))(x)\n",
    "#x = (Dense(1500, activation='relu'))(x)\n",
    "#encoded = (Dense(512, activation='relu'))(x)\n",
    "#x = (Dense(1500, activation='relu'))(encoded)\n",
    "#x = (Dense(3500, activation='relu'))(x)\n",
    "#x = (Dense(7000, activation='relu'))(x)\n",
    "#x = (Dense(1024, activation='relu'))(encoded)\n",
    "#x = (Dense(30000, activation='relu'))(x)\n",
    "\n",
    "#x = (Dense(np.prod(pooling_shape), activation='relu'))(encoded)\n",
    "#x = (Reshape((pooling_shape)))(x)\n",
    "\n",
    "#x = (Conv2D(256, (3,1), activation='relu', padding='same'))(encoded)\n",
    "\n",
    "x = (UpSampling2D(size=(2,2)))(encoded)\n",
    "x = (Conv2D(16, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (UpSampling2D(size=(2,4)))(x)\n",
    "x = (Conv2D(16, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (UpSampling2D(size=(2,4)))(x)\n",
    "x = (Conv2D(16, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (Conv2D(8, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (Conv2D(4, (3,3), activation='relu',padding='same'))(x)\n",
    "decoded = (Conv2D(1, (3,3), activation='sigmoid',padding='same'))(x)\n",
    "autoencoder = Model(input_feature, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = Input(shape=(input_shape))\n",
    "\n",
    "x = Conv2D(4, kernel_size=(9,15), activation='relu', input_shape=input_shape, data_format='channels_last')(input_feature)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
    "x = Conv2D(8, (9,15), activation='relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
    "x = Conv2D(16, (9,15), activation='relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
    "#x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "#x = MaxPooling2D(pool_size=(2,2), padding='same', name='pooling')(x)\n",
    "\n",
    "#x = (MaxPooling2D(pool_size=(2,4), padding='same'))(x)\n",
    "x = (Conv2D(16, (8,15), activation='relu'))(x)\n",
    "x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "encoded = (Conv2D(16, (1,13), activation='relu'))(x)\n",
    "#encoded = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "#x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "#x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "#x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "#x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "#x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "#x = (MaxPooling2D(pool_size=(2,4), padding='same', name='pooling'))(x)\n",
    "#x = (Conv2D(16, (9,15), activation='relu', padding='same'))(x)\n",
    "\n",
    "#x = (Conv2D(16, (9,30), activation='relu', padding='same'))(x)\n",
    "\n",
    "\n",
    "#pooling_shape = tuple(x.shape.as_list()[1:])\n",
    "#x = (Flatten(name='flatten'))(x)\n",
    "#x = (Dense(1024, activation='relu'))(x)\n",
    "#encoded = (Dense(512, activation='relu'))(x)\n",
    "#x = (Dense(7000, activation='relu'))(x)\n",
    "#x = (Dense(3500, activation='relu'))(x)\n",
    "#x = (Dense(1500, activation='relu'))(x)\n",
    "#encoded = (Dense(512, activation='relu'))(x)\n",
    "#x = (Dense(1500, activation='relu'))(encoded)\n",
    "#x = (Dense(3500, activation='relu'))(x)\n",
    "#x = (Dense(7000, activation='relu'))(x)\n",
    "#x = (Dense(1024, activation='relu'))(encoded)\n",
    "#x = (Dense(30000, activation='relu'))(x)\n",
    "\n",
    "#x = (Dense(np.prod(pooling_shape), activation='relu'))(encoded)\n",
    "#x = (Reshape((pooling_shape)))(x)\n",
    "\n",
    "#x = (Conv2D(256, (3,1), activation='relu', padding='same'))(encoded)\n",
    "\n",
    "x = (UpSampling2D(size=(2,2)))(encoded)\n",
    "x = (Conv2D(16, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (UpSampling2D(size=(2,2)))(x)\n",
    "x = (Conv2D(16, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (UpSampling2D(size=(2,1)))(x)\n",
    "x = (Conv2D(8, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (UpSampling2D(size=(2,1)))(x)\n",
    "x = (Conv2D(4, (3,3), activation='relu',padding='same'))(x)\n",
    "x = (UpSampling2D(size=(2,1)))(x)\n",
    "decoded = (Conv2D(1, (3,3), activation='relu',padding='same'))(x)\n",
    "autoencoder = Model(input_feature, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = Input(shape=(input_shape))\n",
    "\n",
    "x = Conv2D(4, kernel_size=(7,15), activation='relu', input_shape=input_shape, data_format='channels_last')(input_feature)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(8, (7,15), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (5,15), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = (Conv2D(16, (3,15), activation='relu'))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = (Conv2D(16, (1,15), activation='relu'))(x)\n",
    "x = BatchNormalization()(x)\n",
    "encoded = (Conv2D(16, (1,13), activation='relu'))(x)\n",
    "x = BatchNormalization()(encoded)\n",
    "\n",
    "\n",
    "\n",
    "x = Conv2DTranspose(16, (1,13), dilation_rate=(1, 1), activation='relu')(x)\n",
    "x = Conv2DTranspose(16, (1,15), dilation_rate=(1, 1), activation='relu')(x)\n",
    "x = Conv2DTranspose(16, (1,15), dilation_rate=(1, 1), activation='relu')(x)\n",
    "x = Conv2DTranspose(16, (3,15), dilation_rate=(1, 1), activation='relu')(x)\n",
    "x = Conv2DTranspose(8, (5,15), dilation_rate=(1, 1), activation='relu')(x)\n",
    "x = Conv2DTranspose(4, (7,15), dilation_rate=(1, 1), activation='relu')(x)\n",
    "decoded = Conv2DTranspose(1, (7,15), dilation_rate=(1, 1), activation='relu')(x)\n",
    "\n",
    "autoencoder = Model(input_feature, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 128, 1)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = Input(shape=(input_shape))\n",
    "x = (Flatten(name='flatten'))(input_feature)\n",
    "encoded = (Dense(19*128, activation='relu'))(x)\n",
    "#x = (Dense(19*128, activation='relu'))(encoded)\n",
    "decoded = (Reshape((19,128,1)))(encoded)\n",
    "autoencoder = Model(input_feature, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(tuple(x.shape.as_list()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128, 32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(x.shape.as_list()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        (None, 19, 128, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_566 (Conv2D)          (None, 13, 114, 4)        424       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 13, 114, 4)        16        \n",
      "_________________________________________________________________\n",
      "conv2d_567 (Conv2D)          (None, 7, 100, 8)         3368      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 100, 8)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_568 (Conv2D)          (None, 3, 86, 16)         9616      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 86, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_569 (Conv2D)          (None, 1, 72, 16)         11536     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1, 72, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_570 (Conv2D)          (None, 1, 58, 16)         3856      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1, 58, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_571 (Conv2D)          (None, 1, 44, 16)         3856      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 1, 44, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_572 (Conv2D)          (None, 1, 32, 16)         3344      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1, 32, 16)         64        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_73 (Conv2DT (None, 1, 44, 16)         3344      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_74 (Conv2DT (None, 1, 58, 16)         3856      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_75 (Conv2DT (None, 1, 72, 16)         3856      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_76 (Conv2DT (None, 3, 86, 16)         11536     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_77 (Conv2DT (None, 7, 100, 8)         9608      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_78 (Conv2DT (None, 13, 114, 4)        3364      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_79 (Conv2DT (None, 19, 128, 1)        421       \n",
      "=================================================================\n",
      "Total params: 72,353\n",
      "Trainable params: 72,169\n",
      "Non-trainable params: 184\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.copy(X_train_padded)\n",
    "X_test = np.copy(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 32, 128)\n",
      "(900, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "X_train = (X_train_padded) / 255\n",
    "X_test = (X_test_padded) / 255\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 19, 128, 1)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 19, 128, 1)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\t-anmend\\\\Documents\\\\train_1\\\\models\\\\best_model_CNN_20190718_152715.ckpt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.today()\n",
    "\n",
    "t = today.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_folder = f\"C:\\\\Users\\\\t-anmend\\\\Documents\\\\train_1\\\\models\\\\best_model_CNN_{t}.ckpt\"\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.today()\n",
    "\n",
    "t = today.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_folder = f\"/home/t-anmend/train1/models/best_model_CNN_{t}.ckpt\"\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(model_folder, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=f\"/home/t-anmend/train1/tensor_board/{t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/1000\n",
      "2100/2100 [==============================] - 6s 3ms/step - loss: 29718.3075 - val_loss: 21893.2312\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 21893.23115, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 2/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 21207.3950 - val_loss: 19349.6671\n",
      "\n",
      "Epoch 00002: val_loss improved from 21893.23115 to 19349.66708, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 3/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 17650.7726 - val_loss: 15992.5350\n",
      "\n",
      "Epoch 00003: val_loss improved from 19349.66708 to 15992.53502, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 4/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 15031.7636 - val_loss: 16100.6349\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 15992.53502\n",
      "Epoch 5/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 14149.3887 - val_loss: 12163.3461\n",
      "\n",
      "Epoch 00005: val_loss improved from 15992.53502 to 12163.34611, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 6/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 13181.4053 - val_loss: 10916.1278\n",
      "\n",
      "Epoch 00006: val_loss improved from 12163.34611 to 10916.12775, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 7/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 11360.6643 - val_loss: 10248.3291\n",
      "\n",
      "Epoch 00007: val_loss improved from 10916.12775 to 10248.32908, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 8/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 10258.8051 - val_loss: 11492.7992\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 10248.32908\n",
      "Epoch 9/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 10171.8084 - val_loss: 11292.6370\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 10248.32908\n",
      "Epoch 10/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 9751.7579 - val_loss: 10530.8938\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 10248.32908\n",
      "Epoch 11/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 9503.6308 - val_loss: 9814.8304\n",
      "\n",
      "Epoch 00011: val_loss improved from 10248.32908 to 9814.83040, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 12/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 9309.2485 - val_loss: 9083.0051\n",
      "\n",
      "Epoch 00012: val_loss improved from 9814.83040 to 9083.00507, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 13/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 9048.9734 - val_loss: 8721.3431\n",
      "\n",
      "Epoch 00013: val_loss improved from 9083.00507 to 8721.34305, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 14/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8896.1385 - val_loss: 8960.8535\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 8721.34305\n",
      "Epoch 15/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8750.4675 - val_loss: 8895.8285\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 8721.34305\n",
      "Epoch 16/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8609.0609 - val_loss: 8641.6966\n",
      "\n",
      "Epoch 00016: val_loss improved from 8721.34305 to 8641.69662, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 17/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8494.8878 - val_loss: 8460.4870\n",
      "\n",
      "Epoch 00017: val_loss improved from 8641.69662 to 8460.48701, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 18/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8375.8445 - val_loss: 8474.3506\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 8460.48701\n",
      "Epoch 19/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8253.5445 - val_loss: 8266.2161\n",
      "\n",
      "Epoch 00019: val_loss improved from 8460.48701 to 8266.21614, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 20/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8166.3058 - val_loss: 8320.2154\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 8266.21614\n",
      "Epoch 21/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8070.4386 - val_loss: 8025.5790\n",
      "\n",
      "Epoch 00021: val_loss improved from 8266.21614 to 8025.57903, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 22/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 8009.0945 - val_loss: 8444.3498\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 8025.57903\n",
      "Epoch 23/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7905.2278 - val_loss: 7840.4249\n",
      "\n",
      "Epoch 00023: val_loss improved from 8025.57903 to 7840.42495, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 24/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7838.5841 - val_loss: 8265.0759\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 7840.42495\n",
      "Epoch 25/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7760.8651 - val_loss: 7646.0955\n",
      "\n",
      "Epoch 00025: val_loss improved from 7840.42495 to 7646.09545, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 26/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7655.2118 - val_loss: 7948.7790\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 7646.09545\n",
      "Epoch 27/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7556.6067 - val_loss: 7446.0104\n",
      "\n",
      "Epoch 00027: val_loss improved from 7646.09545 to 7446.01040, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 28/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7494.6979 - val_loss: 8013.7403\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 7446.01040\n",
      "Epoch 29/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7414.1645 - val_loss: 7271.0087\n",
      "\n",
      "Epoch 00029: val_loss improved from 7446.01040 to 7271.00869, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 30/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7361.9303 - val_loss: 7567.2947\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 7271.00869\n",
      "Epoch 31/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7252.6978 - val_loss: 7192.7169\n",
      "\n",
      "Epoch 00031: val_loss improved from 7271.00869 to 7192.71693, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 32/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7205.7375 - val_loss: 7641.8034\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 7192.71693\n",
      "Epoch 33/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7140.0984 - val_loss: 6967.1855\n",
      "\n",
      "Epoch 00033: val_loss improved from 7192.71693 to 6967.18551, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 34/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7086.8556 - val_loss: 7460.3914\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6967.18551\n",
      "Epoch 35/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 7001.4402 - val_loss: 6881.2451\n",
      "\n",
      "Epoch 00035: val_loss improved from 6967.18551 to 6881.24514, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 36/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6964.5818 - val_loss: 7275.2658\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6881.24514\n",
      "Epoch 37/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6917.5738 - val_loss: 6834.7812\n",
      "\n",
      "Epoch 00037: val_loss improved from 6881.24514 to 6834.78125, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 38/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6844.3321 - val_loss: 7169.0676\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6834.78125\n",
      "Epoch 39/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6804.8392 - val_loss: 6628.9160\n",
      "\n",
      "Epoch 00039: val_loss improved from 6834.78125 to 6628.91597, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 40/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6747.7516 - val_loss: 7046.2556\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6628.91597\n",
      "Epoch 41/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6673.9549 - val_loss: 6553.9627\n",
      "\n",
      "Epoch 00041: val_loss improved from 6628.91597 to 6553.96270, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 42/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6639.7194 - val_loss: 6927.7597\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6553.96270\n",
      "Epoch 43/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6587.5805 - val_loss: 6436.9528\n",
      "\n",
      "Epoch 00043: val_loss improved from 6553.96270 to 6436.95281, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 44/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6543.9906 - val_loss: 6903.9179\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6436.95281\n",
      "Epoch 45/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6477.1551 - val_loss: 6425.7681\n",
      "\n",
      "Epoch 00045: val_loss improved from 6436.95281 to 6425.76814, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 46/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6448.9419 - val_loss: 6629.2289\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6425.76814\n",
      "Epoch 47/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6410.4603 - val_loss: 6248.4026\n",
      "\n",
      "Epoch 00047: val_loss improved from 6425.76814 to 6248.40258, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 48/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6369.7899 - val_loss: 6602.9152\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6248.40258\n",
      "Epoch 49/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6267.2474 - val_loss: 6152.3844\n",
      "\n",
      "Epoch 00049: val_loss improved from 6248.40258 to 6152.38441, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 50/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6282.7418 - val_loss: 6670.2774\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6152.38441\n",
      "Epoch 51/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6184.1663 - val_loss: 6188.6136\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 6152.38441\n",
      "Epoch 52/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6158.8849 - val_loss: 6542.3683\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 6152.38441\n",
      "Epoch 53/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6132.5418 - val_loss: 5979.9232\n",
      "\n",
      "Epoch 00053: val_loss improved from 6152.38441 to 5979.92319, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 54/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 6156.3150 - val_loss: 6589.5664\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 5979.92319\n",
      "Epoch 55/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5983.0755 - val_loss: 5878.2453\n",
      "\n",
      "Epoch 00055: val_loss improved from 5979.92319 to 5878.24527, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 56/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5947.5260 - val_loss: 6417.4157\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 5878.24527\n",
      "Epoch 57/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5983.0369 - val_loss: 5760.5966\n",
      "\n",
      "Epoch 00057: val_loss improved from 5878.24527 to 5760.59659, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 58/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5923.5173 - val_loss: 6474.9167\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 5760.59659\n",
      "Epoch 59/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5902.7597 - val_loss: 5921.5774\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 5760.59659\n",
      "Epoch 60/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5854.1875 - val_loss: 6459.3745\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 5760.59659\n",
      "Epoch 61/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5803.9332 - val_loss: 5682.1346\n",
      "\n",
      "Epoch 00061: val_loss improved from 5760.59659 to 5682.13465, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 62/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5807.8582 - val_loss: 6400.1193\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 5682.13465\n",
      "Epoch 63/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5765.9714 - val_loss: 5688.2879\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 5682.13465\n",
      "Epoch 64/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5736.0279 - val_loss: 6027.7852\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 5682.13465\n",
      "Epoch 65/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5672.4659 - val_loss: 5527.2025\n",
      "\n",
      "Epoch 00065: val_loss improved from 5682.13465 to 5527.20250, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 66/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5675.8653 - val_loss: 6024.7164\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 5527.20250\n",
      "Epoch 67/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5641.5402 - val_loss: 5632.1124\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 5527.20250\n",
      "Epoch 68/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5605.9064 - val_loss: 6085.7644\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 5527.20250\n",
      "Epoch 69/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5559.4789 - val_loss: 5551.4168\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 5527.20250\n",
      "Epoch 70/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5555.8713 - val_loss: 6094.5832\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 5527.20250\n",
      "Epoch 71/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5543.8135 - val_loss: 5528.7850\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 5527.20250\n",
      "Epoch 72/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5511.9506 - val_loss: 5803.1015\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 5527.20250\n",
      "Epoch 73/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5478.8404 - val_loss: 5374.8993\n",
      "\n",
      "Epoch 00073: val_loss improved from 5527.20250 to 5374.89932, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 74/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5446.5323 - val_loss: 5893.3941\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 5374.89932\n",
      "Epoch 75/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5427.8138 - val_loss: 5485.4725\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 5374.89932\n",
      "Epoch 76/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5415.8267 - val_loss: 5764.2214\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 5374.89932\n",
      "Epoch 77/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5354.1196 - val_loss: 5246.9850\n",
      "\n",
      "Epoch 00077: val_loss improved from 5374.89932 to 5246.98501, saving model to /home/t-anmend/train1/models/best_model_CNN_20190722_141317.ckpt\n",
      "Epoch 78/1000\n",
      "2100/2100 [==============================] - 4s 2ms/step - loss: 5353.6954 - val_loss: 5829.1522\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 5246.98501\n",
      "Epoch 79/1000\n",
      " 768/2100 [=========>....................] - ETA: 2s - loss: 5257.6012"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-bc19dc6ff786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 callbacks=[checkpoint, tensorboard, rate])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clustering/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=1000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[checkpoint, tensorboard, rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_544_1/kernel/read:0' shape=(7, 15, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.weights[0].value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-89fa410e3bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_img' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = best_model.predict(X_test)\n",
    "#decoded_imgs = decoded.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9ed04eb5f8>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape((19, 128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No max pooling, middle layer (1, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e9fb9ac50>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABTCAYAAACGVlwTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAW7UlEQVR4nO2deXxURbbHfycdskBgwr4lJBASBB1BRQZxQ3HBZYR5z1EcRxnxiW8cF9xG+KC+cXtPxwV13EcZ13EZBpXnuI1xmRkdBFwHQZIAAUICIWxBwEB3av6oW7eK9O2+t7tvd3LzOd/PJ5+urrtU1a3blVPnnDpFQggwDMMwwSOrvSvAMAzDJAcP4AzDMAGFB3CGYZiAwgM4wzBMQOEBnGEYJqDwAM4wDBNQUhrAiWgyEa0iohoimu1XpRiGYRh3KFk/cCIKAagCcDKAOgBLAZwnhFjhX/UYhmGYWGSncO04ADVCiDUAQEQvAZgCIOYAnkO5Ig/dDsijLl3stNi/P+FKUHZIXx+OJHx9MojuXWXZu/ZkpLxYUBfZfWJ/2PH4/gHyWXfZtDtjdYqH2dfIIvlpCBBiX+L931HYN1C/16F91ufW+M890su4Zps+V/Sw3q9m/X6F+8lzsxud79lSIq/JXRf/nWwp1mVmWY87U+9HS5EuO7fOKLMgX35+t9f/Qrvm6fSe7/2/vwXl5dpp8X2L5+vUe5PT4NAH3fLt5K7d9U1CiL5tT0llAB8MYIPxvQ7Aj9qeREQzAcwEgDx0xY9o0oEV6D/IToc31idciVBhLzsd2bot4euTITzuCABA9vufZaS8WGT36Q8ACG/a7Hi8bsYEAEDR/32SsTrFI3vAYDst8nIAANSiB+3whrqM18kvNlwywU4XrJf/lHo+88+41zSfNt5O9/jjYjvdcsyRAIDct5baeY3nyvv3e8i5L6vmymsqLl3qeFxRfYP+iebXS+EnU+/H6llH2emy6/WzEWPGAADo4y99L5MOOliX88U3vt9fESodbqcjq2o8X7d+puzXIbdG94EYPdpOV35y0zqn61PRgZNDXpQ+RgjxhBBirBBibBfkOlzCMAzDJEMqEngdgGLjexGAuCJ0a2E37JkkJYCuCz8FAEQGagkalgT+3dvD7KyCyWviViIZqbvxMi0t9XvE4T/f0VIiyF6h/+lFtm+3035J3tXPHm6nyy/8PO653585DgCQ98YSOy+W5K3os9xZtdKW1Xc7S0Z+0/iYnkK3hKX0N3Dq2qTuVTcn9uyi9fjD7HTWR194ut+e/9CSaY/PG+x0uHa9p+uLb9P1+P7H4zxdY0rdJuRglmqZ2AwACFWW23nVF/Wx0xWXeuu38is+9XSeG02X6nemz+O67NUvyGdfdn70cx8+R88OzCY6Sd4bbtK/UfPZKjYulJL1gAe1UNjSS6vo1PhSO/UHdt6Au0oBAPlnbbLzWr/3rlZpeG0kAGDg1JVRxxKRuiMT9e/eSfJWNI3pqr/EOC0VCXwpgHIiGkpEOQCmAViUwv0YhmGYBEjaCwUAiOh0APcDCAGYL4S4I975PaiXaKsD95O6P8v/ykX/Ga3r2nqJlhh6/95/KbP2Nn3/0pvk/bPGjLLzWr+Mtu02XKOljIH3edNDbrpaXzNgXvxr1t8cW7+WLK3HyNlJ1j9S01cqiVdJSm0JVZQBACJVqz3fs/Hy+HribTNkH/Wan1r/h0ZV2OnIiqqY52UP1vad/aX97HQ6dL0t75YCAHJPqdXlDy0BAITXOqpPPVP/a/3ODfpt/Hep6qmxAICKi5fFP+8xPUup+O8lcc50pmmm7Ms+T8Tvy3W36LoPmyfHhciOnXGviTVDD42Qeu5EpO1UKFuqDbCPj33+MyHE2LbnpKJCgRDiTQBvpnIPhmEYJjl4JSbDMExASUmFkijpUKHQEYab0GdyihSedISdl12ZGVc/r9PqVNl4g57eDb6r/dwD907VU2CybKWmgdXJyJVdpN0Iw3Ub495/+3R5fSxXPKWyUuqqZFFqjlWzSuw805AbGimNhpGV1SmVkwhKvTDyRq0+ijRtTVt5Ddcaqrx7479TgxZ3BwDUj99l52UdepCdbv36W09ltlZq/4esSRuijydhiFaGfuDAdzEe9dcZ6qF7ZNtrntdlD/+5t7KTper3lvvnJdHun7umaTfTT1+8zlGFwhI4wzBMQOEBnGEYJqBkVIXSvbBIjDn+KgBA3v/HnuJkddX+j6172m+5upu/uEnVfD27qZgRbYGnXOmvKlq8L7N1Y92tUo1QcrN3NYLyUY73/OOx5yeW98ir/vgTJ4LygAG8e8Gk6n2kVEX5r+nn5VUVVDNPT4ELv9Xr3ky/aSeqHpVlVvxSlzlimfRxXjXWv3AD1Q/Lviz/lX99Wf20VF+W/yJadbn9L9qHvecZWiX1Tr3sy1MHjYm6Jha1t1sqtBvjP8tNVxleWw90jBXJXjHVOyvuvoZVKAzDMJ2JjErgeYOLRfFlVwOIb3yqevxIO+0W28HJiGmSlSd9KZt/rOMK7B6oA2D1f7Bj/Fce/5WUrBaP7uJyZgKM+6H8XPKvqENZo0fa6davoleWubHjAi3ZFj4XXwoKFcrVcG7+tx0RJUUPv9p51WQ6yC4uAtAmNsz4Q+Xn4q8zVo9kqLnPel7X+P+8TEeBLeN7A0jdpz8ZVt+jZ1Zl16Xvvfjduo/t9MghDSyBMwzDdCZ4AGcYhgkogfcDN3EyOGUKZaQEvBsqlQ8oAIy6RQbycvOPprGH6HKWLY97rprq+TnNy+omA1K17k48hnSoT287vfMEuSy54E+ZN4aaKMNo06HacN5ts44rv6evlHH6Pmb4hvfXy+Ijmxs9lVP9gJ52l1+VfH+YBv7wkSN0vkdfaad1BImoBJyW7B9ApVT/YFJmQgNvuNEIenV7aurQ3WdLo263BYm/k6GePe20GfjO8dwR3kLPmr7w7384l1UoDMMwnYkOKYGv/qN2Jyr7mffAP0rCc1u1lo7VjKkG50lGWt51rrym+8vxr1FSM5Cc5Gzi9IxrnpOSwohZRvhdhzC/BwQWeki6kUW2bHEsRxlZYxlYQ+Uy5HCkOn64Ya8otzTA3TUt96MBdrrl+E1xzoyPkviAGFKfMlwCjsZLt5CrXln7v7rtw+/Tq4jj/Y52/lxL7T94Xr9/1c/IUKnl0+OHRzZZc6csf9hs7wZJFYrZLQxzy2l6lmtukOE3G2cbY8qdSfRFlnasQGv0zmLviQUsgTMMw3QmeABnGIYJKBlVoeSWFokBN10JQAdvqX5ITyPLL49vPIicIKdNoQ/0tEn5zAKZ21ORDpO+58nssbfzfGPq+UJ81YeYoH3X6ZOvEi5Lxdze103/n+61UE7Fk1WlbL5CThV7bNC7/eTslGmzX5zI/6i/nd57QpNMOEwXTWKtpFSrZM0Vsq3HSlVO1t/9C0CUih+4iscNAGKrNmxFmptTr5gHvjtH1r3gFee6Vz1pxe7+r/ixu/3CjI++4jaddlq57IabqkYF6HIKzkVdcuy02L8v6vj632h1yJDfpHedCGVbG5OH4++exSoUhmGYTgYP4AzDMAHFVYVCRPMBnAmgUQhxiJXXC8DLAEoB1AI4RwgR3/kRQP7AYlE64xoAzpvRpoP9J8ngOl3eSy0uuFuMcRUnGTgwVrJfbL3Yio+9aq+dt31EPgCg91P+LSdWoQmcwhKYmKqr7UfLdPeX4qsZ1JQdSH3a7rjc3GLDAu0rX3y2i6+85fGUiLdTOsgeVmqn154n1QtD3tGqFjeff6/U3qFVUv2XSvXVjjK9MZdbPPDuf5cbKe86tinhsmPFDVf95dZXTrj9Lutf1dsa7t4lw2oMeVl7fOT+Jb5niumzvWuU9MBqLtHXD7g/+nmlY2+AVFQoTwOY3CZvNoBKIUQ5gErrO8MwDJNBPBkxiagUwBuGBL4KwEQhRAMRDQTwoRBiRJxbAEjPSsxkduFIB8oXGgCGXyCNaKZ0kLtO+kVHatbaeaaxJJwn+yERX9h04rRTiYkKeQocGPZUoQzOgLtxMx7ZA7Thc8+YIXZ6ixX0yy8//uyB2rc73JC8b3cmqXrE6IPLkn/3xdF63UU6NlxWHOC3PnG9na6qlc/ebSNkE6fwzOYm4nuKCwDECJusgrwBjoHe0o04Sjsn9LtXrp3YMmGHnadWlP/0jnfsvFmjKn01YvYXQjQAgPXZz+V8hmEYxmdS2pXeC0Q0E8BMAMhDV5ezGYZhGK8EXoVyAEkE0lHLu0v+J3oqbu7Is3eAfk5OO+CsvVMbh4bGU4O4LI9mUqfqKcNYmsC0XKHilwNApMJS2xhT7c1X6vfCazx5c4NaN2NvPB/mRPC6XmHNXfrdHXaD8e6StYtQBteKdGRUHzr1n7nJd8EKHYIgUrU66lyTtS/J8aBshg5q5bQLmd9+4IsATLfS0wG8nuR9GIZhmCRxHcCJ6EUA/wQwgojqiOhiAHcCOJmIqgGcbH1nGIZhMki7RyM04/mWzJPLxRNZ5p3I1NQrKuayGDXMzvPLD9dELaMF3JfSKh9a03+2o7BpluxDJ5/YdNF4ubWU/qGOsSVedkmxnQ6v2xB13Iz9rsJIxELFG988tczOc9oI2WnJd9UTRjkzXbYjtGLLp/puN83UKpg+T/jjRdVyuhFF8M30RRF0wozD7TXOeiIhMtTG4gBArXL83TROb6XopM7lpfQMwzCdjHaXwDPF+psNaeXWxKW2Lb/UUkbfR/2RMpyC6qgdYgAg6x+p+eSquOd++UqbrPmtfh4FtdLYZQaWMtk2Q5571GXaoPjWe1KYcPN7NwMghTfWe6pbItKuVyk01KOHnU5HMKpY8bXbYkqmzSV6BtdSKPtgyIM66Nn3x0q/6Jy3/Zdgt/9C93/Pp1P7Pex9ZygAIP/UtS5napTTgOkwED5Rr7toLpG/rV1D9TUtfeXKU6d1CybK+AskF7BOxaoHgObRfQEkt8uPCUvgDMMwnQwewBmGYQJKp1ShhIbreZO5dD2dtFZqI1bWpGgjlhOhCm2kcvMXVSjjHQD0WCcNn47LhVNk09W6nAHztGpkxwVWUK1X9PL4yHg5Vfdq8PEDpSYxVSQqcFndSVo1FdcnH0DoYLl8IfLNKu9lu4QRUJjhBFq7aFmpy7vefNPNMBEbfir72ox/reJ9A0D9KfL4yLl6ibrXDZfdmPCVjpn9yWj5bM1tCU32H/4dAKD03PhrHJIxFJp0+5tUTew+Tm/JZwaRWjdFBt1KR9C8ujm67anef9+pUiuS8070O1G6JN9OP3nkc6xCYRiG6Ux0SAl8x4XaQFL4rHcDiTJsFBqedn2WyFVRZlhHt5V6ynDm1WjmB8oNbftRg+08v9wikyHUs6edjmzXkYJV0KeWg7RxsewuufFw7Tgd6taJ6t8Zuy9dIY06iUhiaochAOi6MDWjUFtq7tPS7PBr9HNXQb2cAnr5iVtQtj0/kW3v+qq/7faCk4vktov0b7TXH7z9RhMxLrtRe5ssv/Qm57IjE63duz7UM5Z1r8ggViXneA9glYh7YCrsP0WPSWqG1vwzXfaSF65jCZxhGKYzwQM4wzBMQGl3FYq5E0l4Ta0v5ajNbQG9wa0yVgFA1UVaPVB2nZwWua6KdAlCZa6uSodRUfmWRqrXJHyt6de6u1TGSd7TT//vdlrllyp+xddWRh7A2dDjRKrBrJJh7xTd//mvy/5ffbdWM5Rd7/8zzuTmu21JZBVxOtk4Wz+Dvl9oY2vtNDmuVVykd+lJxmBtssZS0aYasz8ZFTH7gTMMw3QyeABnGIYJKGnf0MENv9QmJkptYmJOm6hVT2GUL23BK9EWZjFBb3209Yd6M4q+y+RjM6eOBd9on1unCWV2kfQuCddtdKyzmpLGmo42Hie3FuttqFCq5ssZVcUMZzWBWqJccKH2le1qLQ2OtbWG8jhwCsiUCE5qk/rr9XRXWKJDrGX+SiWh1BGJkIjaZPfZ0rsj1aXOTvU01Sbm1nDhTZtTKkthqk3iecs4qXdMDoh/vmNn1HEz4Fzx7fL+bmqTUJ/e+p5NW+OcmRwtZ0iPlsF36vaaXkoVF8n+NN+5QXfLc1WwOsA59nYsutVTzGNNl+oxZccIrZY2PZoUOd+1ei7TDZbAGYZhAkq7GzHbA7fdc5RPptcVc8niFoLUDRUkqtd8Z0NI1iFWCNrl0SFoVRAnAGjNCen8T76KOtdEGWuTMdSGPtC+4yur5Iyk4lLtD0xH6s1mxdLMbDargpT5FaDMxPMuTR5QxjpT4kxmU2PlTw5on3K34E29P9ZG/61Hb486brL6XjmjLbs2WvLMHlpip5uO1e+CkyEvmSBmXg2rsdq761xZ95zmiJ2X+1ZmQ9nGgo2YDMMwnQwewBmGYQJKRlUoRLQFwG4ATRkrNP30QedqD9D52sTt6fh0tjb53Z4SIUTftpkZHcABgIiWOelygkpnaw/Q+drE7en4dLY2Zao9rEJhGIYJKDyAMwzDBJT2GMCfaIcy00lnaw/Q+drE7en4dLY2ZaQ9GdeBMwzDMP7AKhSGYZiAwgM4wzBMQMnoAE5Ek4loFRHVENHsTJbtB0RUTEQfENFKIvqGiK6y8nsR0V+JqNr67Ol2r44EEYWI6AsiesP6PpSIPrXa8zIR5bjdoyNBRIVEtICIvrX66qgg9xERXW29b8uJ6EUiygtSHxHRfCJqJKLlRp5jf5DkQWuM+JqIDo995/YjRpvutt65r4noVSIqNI7Nsdq0iohO9aseGRvAiSgE4GEApwEYBeA8IhqVqfJ9IgzgWiHESADjAfzKasNsAJVCiHIAldb3IHEVgJXG97sAzLPasx3Axe1Sq+R5AMDbQoiDAIyGbFsg+4iIBgO4EsBYIcQhAEIApiFYffQ0gMlt8mL1x2kAyq2/mQAezVAdE+VpRLfprwAOEUIcCqAKwBwAsMaIaQAOtq55xBoPUyaTEvg4ADVCiDVCiH0AXgIwJYPlp4wQokEI8bmV3gU5MAyGbMcz1mnPAJjaPjVMHCIqAnAGgCet7wTgRAALrFOC1p4eAI4D8BQACCH2CSF2IMB9BBn2OZ+IsiEjATcgQH0khPgbgG1tsmP1xxQAzwrJYgCFRDQwMzX1jlObhBDvCiFUFK3FAIqs9BQALwkhWoQQawHUQI6HKZPJAXwwADPkXp2VF0iIqBTAYQA+BdBfCNEAyEEeQL/2q1nC3A/g1wBUkOLeAHYYL2LQ+mkYgC0A/mCphZ4kom4IaB8JITYCuAfAesiBeyeAzxDsPgJi90dnGSdmAHjLSqetTZkcwJ2ioQfSh5GICgD8GcAsIURze9cnWYjoTACNQojPzGyHU4PUT9kADgfwqBDiMMjYO4FQlzhh6YanABgKYBCAbpBqhrYEqY/iEfT3D0Q0F1Ld+oLKcjjNlzZlcgCvA1BsfC8C4C3QbweCiLpADt4vCCEWWtmb1TTP+myMdX0H42gAZxFRLaRK60RIibzQmq4DweunOgB1Qgi1xc4CyAE9qH10EoC1QogtQoj9ABYCmIBg9xEQuz8CPU4Q0XQAZwI4X+hFNmlrUyYH8KUAyi3reQ6kUn9RBstPGUs//BSAlUKI+4xDiwBMt9LTAbye6bolgxBijhCiSAhRCtkf7wshzgfwAYCzrdMC0x4AEEJsArCBiEZYWZMArEBA+whSdTKeiLpa759qT2D7yCJWfywCcKHljTIewE6launoENFkADcAOEsIYe7VtgjANCLKJaKhkAbaxHdEcUIIkbE/AKdDWmdXA5ibybJ9qv8xkFOfrwF8af2dDqk3rgRQbX32au+6JtG2iQDesNLDrBesBsCfAOS2d/0SbMsYAMusfnoNQM8g9xGAWwB8C2A5gOcA5AapjwC8CKm/3w8pjV4cqz8g1Q0PW2PEvyC9b9q9DR7bVAOp61Zjw2PG+XOtNq0CcJpf9eCl9AzDMAGFV2IyDMMEFB7AGYZhAgoP4AzDMAGFB3CGYZiAwgM4wzBMQOEBnGEYJqDwAM4wDBNQ/g10cschR5Ez2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(decoded_imgs[0].reshape((19,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,1), activation='relu', input_shape=input_shape, data_format='channels_last', padding='same'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Conv2D(64, (3,1), activation='relu', padding='same'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Conv2D(128, (3,1), activation='relu', padding='same'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Conv2D(256, (3,1), activation='relu', padding='same'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), padding='same', name='pooling'))\n",
    "\n",
    "#model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\n",
    "#model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\n",
    "#model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\n",
    "#model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\n",
    "#model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\n",
    "#model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, (3,1), activation='relu', padding='same'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same', name='pooling'))\n",
    "\n",
    "model.add(Conv2D(256, (3,1), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Flatten(name='flatten'))\n",
    "\n",
    "model.add(Dense(30000, activation='relu'))\n",
    "\n",
    "model.add(Dense(15000, activation='relu'))\n",
    "\n",
    "model.add(Dense(7000, activation='relu'))\n",
    "\n",
    "model.add(Dense(3500, activation='relu'))\n",
    "\n",
    "model.add(Dense(1500, activation='relu'))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1500, activation='relu'))\n",
    "\n",
    "model.add(Dense(3500, activation='relu'))\n",
    "\n",
    "model.add(Dense(7000, activation='relu'))\n",
    "\n",
    "model.add(Dense(15000, activation='relu'))\n",
    "\n",
    "model.add(Dense(30000, activation='relu'))\n",
    "\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(model.get_layer('flatten').output_shape[1], activation='relu'))\n",
    "model.add(Reshape((model.get_layer('pooling').output_shape[1:])))\n",
    "\n",
    "model.add(Conv2D(256, (3,1), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D(size=(2,2)))\n",
    "#model.add(UpSampling2D(size=(1,1)))\n",
    "model.add(Conv2D(256, (3,1), activation='relu',padding='same'))\n",
    "model.add(UpSampling2D(size=(2,2)))\n",
    "model.add(Conv2D(128, (3,1), activation='relu',padding='same'))\n",
    "model.add(Conv2D(64, (3,1), activation='relu',padding='same'))\n",
    "#model.add(UpSampling2D(size=(2,2)))\n",
    "model.add(Conv2D(32, (3,1), activation='relu',padding='same'))\n",
    "#model.add(UpSampling2D(size=(2,2)))\n",
    "model.add(Conv2D(1, (3,1), activation='sigmoid',padding='same'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('flatten').output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8, 256)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('pooling').output_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_250 (Conv2D)          (None, 32, 128, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_251 (Conv2D)          (None, 32, 128, 64)       6208      \n",
      "_________________________________________________________________\n",
      "conv2d_252 (Conv2D)          (None, 32, 128, 128)      24704     \n",
      "_________________________________________________________________\n",
      "conv2d_253 (Conv2D)          (None, 32, 128, 256)      98560     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 16, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_254 (Conv2D)          (None, 16, 64, 256)       196864    \n",
      "_________________________________________________________________\n",
      "pooling (MaxPooling2D)       (None, 8, 32, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_255 (Conv2D)          (None, 8, 32, 256)        196864    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 30000)             1966110000\n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 15000)             450015000 \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 7000)              105007000 \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 3500)              24503500  \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 1500)              5251500   \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 512)               768512    \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 1500)              769500    \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 3500)              5253500   \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 7000)              24507000  \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 15000)             105015000 \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 30000)             450030000 \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 65536)             1966145536\n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 8, 32, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_256 (Conv2D)          (None, 8, 32, 256)        196864    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling (None, 16, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_257 (Conv2D)          (None, 16, 64, 256)       196864    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_20 (UpSampling (None, 32, 128, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_258 (Conv2D)          (None, 32, 128, 128)      98432     \n",
      "_________________________________________________________________\n",
      "conv2d_259 (Conv2D)          (None, 32, 128, 64)       24640     \n",
      "_________________________________________________________________\n",
      "conv2d_260 (Conv2D)          (None, 32, 128, 32)       6176      \n",
      "_________________________________________________________________\n",
      "conv2d_261 (Conv2D)          (None, 32, 128, 1)        97        \n",
      "=================================================================\n",
      "Total params: 5,104,422,449\n",
      "Trainable params: 5,104,422,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(4,1), activation='relu', input_shape=input_shape, data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Conv2D(64, (4,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Conv2D(128, (3,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 16, 128, 32)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 64, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 5, 64, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 32, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 1, 32, 128)        24704     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 16, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 296,682\n",
      "Trainable params: 296,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
